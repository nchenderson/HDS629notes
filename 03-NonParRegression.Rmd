# Nonparametric Regression with Longitudinal Data {#nonpar-regression}


## Notation

* For **longitudinal data**, we will again use the following **notation**: 
    + Individual $i$ has observations for both the outcome and the covariates at times $t_{i1}, \ldots, t_{in_{i}}$
   
    + $Y_{ij}$ is the outcome for individual $i$ at time $t_{ij}$.
    
    + $\mathbf{x}_{ij}$ is the vector of covariates at time $t_{ij}$.
    
    + The $i^{th}$ individual has $n_{i}$ observations: $Y_{i1}, \ldots, Y_{in_{i}}$.
    
    + There will be $m$ individuals in the study (so $1 \leq i \leq m$).


* A general regression model relating $Y_{ij}$ and $\mathbf{x}_{ij}$ is the following:
\begin{equation}
Y_{ij} = \mu( \mathbf{x}_{ij} ) + \varepsilon_{ij}  \nonumber
\end{equation}

* Here, $\mu(\mathbf{x}_{ij}) = E(Y_{ij}| \mathbf{x}_{ij})$  is the "mean function".

* In **nonparametric approaches** to estimating $\mu(\cdot)$, we will try to estimate $\mu(\mathbf{x})$ without
making any strong assumptions about the form of $\mu( \mathbf{x} )$.

* Basically, in a nonparametric approach, there is not a fixed set of parameters describing 
the mean function that does not change as the sample size grows.


## Kernel Smoothing

### Description of Kernel Regression

* With **kernel regression**, we estimate the mean function $\mu(\mathbf{x})$ at $\mathbf{x}$
by taking a weighted **"local average"** of the $Y_{ij}$ around $\mathbf{x}$.

* Specifically, the **kernel regression estimate** of $\mu(\cdot)$ at a point $\mathbf{x}$ can be expressed as
\begin{equation}
\hat{\mu}( \mathbf{x} ) = \sum_{i=1}^{m}\sum_{j=1}^{n_{i}} w_{ij}(\mathbf{x})Y_{ij} 
\end{equation}

* The **"weights"** at the point $\mathbf{x}$ are given by
\begin{equation}
w_{ij}(\mathbf{x}) = \frac{ K\Big( \frac{\mathbf{x} - \mathbf{x}_{ij}}{ h_{n} }\Big) }{ \sum_{i=1}^{m}\sum_{j=1}^{n_{i}} K\Big( \frac{\mathbf{x} - \mathbf{x}_{ij}}{ h_{n} }\Big)  }
(\#eq:nw-weights)
\end{equation}

* When using the weights \@ref(eq:nw-weights), $\hat{\mu}(\mathbf{x})$ is known as the **Nadaraya-Watson** esitmator.

---

* The function $K(\cdot)$ in \@ref(eq:nw-weights) is referred to as the **"kernel function"**.

* The **kernel function** $K(\cdot)$ is: 
   + A smooth nonnegative function 
   + Symmetric around $0$ 
   + Has a mode at $0$ and decays the further you go away from $0$

* A common choice of $K(\cdot)$ is the **Gaussian kernel**
\begin{equation}
K(\mathbf{u}) = \exp\Big\{ - \frac{||\mathbf{u}||^{2}}{2} \Big\}
\end{equation}

```{r, echo=FALSE}
uu <- seq(-5, 5, length.out=1000)
plot(uu, dnorm(uu), type="n", xlab="u", ylab="K(u)", 
     main="Gaussian Kernel for a scalar covariate", cex.lab=1.2, las=1)
lines(uu, dnorm(uu), lwd=2)
```

---

* Observations where $\mathbf{x}_{ij}$ is **"close"** to $\mathbf{x}$ will be given 
a larger weight $w_{ij}(\mathbf{x})$ because $||\mathbf{x} - \mathbf{x}_{ij}||^{2}$ will be small.

* Similarly, observations where $\mathbf{x}_{ij}$ is **"far away"** from $\mathbf{x}$ will be given 
a smaller weight $w_{ij}(\mathbf{x})$ because $||\mathbf{x} - \mathbf{x}_{ij}||^{2}$ will be small.

--- 

* The term $h_{n} > 0$ is referred to as the **bandwidth**.

* The **bandwidth** determines how many observations have 
a strong impact on the value of $\hat{\mu}( \mathbf{x} )$.

* If the bandwidth $h_{n}$ is **small**, observations close to $\mathbf{x}$ will largely
determine the value of $\hat{\mu}(\mathbf{x})$.

* If the bandwidth $h_{n}$ is **large**, the value of $\hat{\mu}(\mathbf{x})$ will be more heavily influenced 
by a larger number of observations.

---

* Kernel regression estimates with a **smaller bandwidth** will be more "wiggly" and **non-smooth**.

* Kernel regression estimates with a **larger bandwidth** will be more **smooth**.

### Kernel Regression in the sleepstudy data

* Again, let's look at the **sleepstudy** data from the **lme4** package.

* The **sleepstudy** data had 18 participants with **reaction time**
measured across 10 days.

```{r, echo=TRUE}
library(lme4)
data(sleepstudy)
head(sleepstudy)
```

```{r, echo=FALSE, message=FALSE}
plot(sleepstudy$Days, sleepstudy$Reaction, las=1, ylab="Reaction Time", xlab="Days",
     main="Sleepstudy Data: All Subjects", type="n")
points(sleepstudy$Days, sleepstudy$Reaction, pch=16, cex=0.8)
```

---

* We can estimate the **marginal mean function** for the **sleepstudy** data by using a **GEE**. 

* We will assume that reaction time is a **linear function** of time on study: 
    + That is, we will assume that $\mu(t) = \beta_{0} + \beta_{1} t$.
```{r, echo=TRUE}
library(geepack)
## Use AR(1) correlation structure
sleep.gee <- geeglm(Reaction ~ Days, data=sleepstudy, id=Subject, corstr="ar1") 
```

* To get the value of the estimated **regression function**, we can use the first
$10$ fitted values (because the fitted values for each subject are the same as the overall mean function)
```{r, echo=TRUE}
## Estimated mean function at each time point
gee.regfn <- sleep.gee$fitted.values[1:10,1] 

### Now plot the estimated mean function
plot(sleepstudy$Days, sleepstudy$Reaction, las=1, ylab="Reaction Time", xlab="Days",
     main="Sleepstudy: GEE estimate of Mean Function", type="n")
points(sleepstudy$Days, sleepstudy$Reaction, pch=16, cex=0.8)
lines(0:9, gee.regfn, lwd=2, col="red")
```

---

* To find a kernel regression estimate of the mean function, you can use the **ksmooth** function in **R**.

* One thing to note is that **ksmooth** only works for a scalar covariate.

* Using a **bandwidth** of $0.5$ and a **Gaussian kernel**, we can find the kernel regression estimate of the mean function 
with the following **R** code:
```{r, echo=TRUE}
sleep.kernel <- ksmooth(sleepstudy$Days, sleepstudy$Reaction, kernel="normal",
                        bandwidth = 0.5)
```

* This will return a list with an "x vector" and a "y vector".

* The `x` vector will be the vector of points at which the regression function
is estimated. The `y` vector will be a vector containing the estimated values of the regression function.

---

* Let's **plot** the estimated mean function to see what it looks like:
```{r, echo=TRUE}
plot(sleepstudy$Days, sleepstudy$Reaction, las=1, ylab="Reaction Time", xlab="Days",
     main="Sleepstudy: Kernel Regression with Bandwidth = 0.5", type="n")
points(sleepstudy$Days, sleepstudy$Reaction, pch=16, cex=0.8)
lines(sleep.kernel$x, sleep.kernel$y, lwd=2, col="red")
```

* This **bandwidth** looks too small. There are clear "near jumps" in between 
some of the days.   

* We can try a **bandwidth** of $1$ to see if we can smooth this out a bit.
```{r, echo=TRUE}
sleep.kernel.bw1 <- ksmooth(sleepstudy$Days, sleepstudy$Reaction, kernel="normal",
                        bandwidth = 1)

plot(sleepstudy$Days, sleepstudy$Reaction, las=1, ylab="Reaction Time", xlab="Days",
     main="Sleepstudy: Kernel Regression with Bandwidth = 1", type="n")
points(sleepstudy$Days, sleepstudy$Reaction, pch=16, cex=0.8)
lines(sleep.kernel.bw1$x, sleep.kernel.bw1$y, lwd=2, col="red")
```


### Bandwidth Selection


## Regression Splines

* Using **regression splines** is another common nonparametric approach for estimating a 
mean function.

* The most common type of spline used in the context of nonparametric regression is the **cubic spline**. 

* **Definition**: A **cubic spline** with knots $u_{1} < u_{2} < \ldots < u_{q}$ is a function $f(x)$ such that
    + $f(x)$ is a cubic function over each of the intervals $(-\infty, u_{1}], [u_{1}, u_{2}], \ldots, [u_{q-1}, u_{q}], [u_{q}, \infty)$.
    + $f(x)$, $f'(x)$, and $f''(x)$ are all continuous functions.

---

* A commonly used **basis** for the set of cubic splines with knots $u_{1} < u_{2} < \ldots < u_{q}$ is the **B-spline** basis.

* This means that if $\varphi_{1, B}(x), \ldots, \varphi_{q+4, B}(x)$ are the basis functions 










---
