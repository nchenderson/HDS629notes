# Sparse Regression for Longitudinal Data {#glmm-lasso}


---

## Sparse regression methods

* Sparse regression methods typically involve estimating the regression 
coefficients by minimizing a **penalized** least-squares criterion.

* The most well-known sparse regression method is the **lasso**.

* With the **lasso**, the regression coefficients $\boldsymbol{\beta}$ are 
found by minimizing the following penalized least-squares criterion:
\begin{equation}
Q_{\lambda}(\boldsymbol{\beta}) = \sum_{i=1}^{n}(y_{i} - \mathbf{x}_{i}^{T}\boldsymbol{\beta})^{2} + \lambda \sum_{j=1}^{p}|\beta_{j}|
\end{equation}

---

* An advantage of the **lasso** is that many of the individual estimated regression coefficients $\hat{\beta}_{j}$ will equal zero **exactly**.

* You can think of the lasso as performing **variable selection**
    + The regression coefficient estimates which are non-zero will be the "selected" variables.
    
* A nice feature of the **lasso** is that it performs **simultaneous** variable
selection and **regression coefficient estimation**.
    + You do not need to first select a model and then estimate the coefficients from this selected model.
    
    + The selection and estimation is done at the same time.

---

* $\lambda \geq 0$ in the $L_{1}$ penalty function $\lambda \sum_{j=1}^{p}|\beta_{j}|$ is referred to as the **"tuning parameter"**.

* If $\lambda$ is **large enough**, **all** of the estimated regression coefficients will be equal to zero.

* If $\lambda = 0$, then the estimated regression coefficients will be the same as
the usual least-squares estimates.

* For **intermediate** values of $\lambda$, some regression coefficients will be **"selected"** with
the remaining regression coefficient estimates being set to zero.

* The value of the tuning parameter $\lambda$ is most ofte chosen through **cross-validation**.
    + However, $\lambda$ is sometimes chosen by looking at an estimate of the "degrees of freedom" associated $\lambda$.

* **Lasso paths**: You can plot the values of the regression coefficients for different values of $\lambda$ to get a sense of which variables are selected first.

---

* In addition to performing variable selection, the lasso also **shrinks** the regression coefficient estimates **towards zero**. 

*  This can improve predictive performance when the regression coefficient estimates
have **high variance**.
    + This can occur, for example, if the matrix $\mathbf{X}^{T}\mathbf{X}$ is poorly conditioned.

* Another advantage of the lasso and other penalized regression methods is 
that they can be used when the number of variables is **greater** than the 
number of observations.

* Although the lasso is often suggested as a tool for high-dimensional problems (i.e., lots of covariates),
the lasso is still a good tool for moderate-sized number of covariates (e.g., 10-20).
    + The lasso can still improve predictive performance in such cases, and the lasso enables 
      simultaneous variable selection and estimation.

## The Lasso with longitudinal data

**Recall our notation for longitudinal data with random effects**:

* $Y_{ij}$ - outcome for individual $i$ at time $t_{ij}$.

* $\mathbf{x}_{ij}$ - vector of covariates for individual $i$ at time $t_{ij}$.

* $\mathbf{z}_{ij}$ - vector determining form of random effects for individual $i$ at time $t_{ij}$

---

* With **penalized regression for longitudinal data**, the linear mixed model still assumes that
\begin{eqnarray}
Y_{ij} &=& 
\beta_{0} + \mathbf{x}_{ij}^{T}\boldsymbol{\beta} + b_{ij} + e_{ij} \nonumber \\
&=& \beta_{0} + \mathbf{x}_{ij}^{T}\boldsymbol{\beta} + \mathbf{z}_{ij}^{T}\mathbf{u}_{i} + e_{ij}
(\#eq:lmm-general)
\end{eqnarray}
    + $\boldsymbol{\beta}$ - vector of fixed effects
    + $\mathbf{u}_{i}$ - vector of random effects
    + $\mathbf{u}_{i} \sim \textrm{Normal}(0, \boldsymbol{\Sigma}_{\boldsymbol{\theta}})$.
    + $e_{ij} \sim \textrm{Normal}(0, \sigma^{2})$.

---

* If $\mathbf{Y}_{i} = (Y_{i1}, ..., Y_{in_{i}})$ is the vector of observations
from the $i^{th}$ person.

* The vectors $\mathbf{Y}_{1}, \ldots, \mathbf{Y}_{m}$ **are independent** although
the observations within each vector are not independent.

* The distribution of $\mathbf{Y}_{i}$ is 
$\mathbf{Y}_{i} \sim \textrm{Normal}\left( \mathbf{X}_{i}\boldsymbol{\beta}, \mathbf{V}_{i}(\boldsymbol{\theta}, \sigma^{2}) \right)$.
    + $\mathbf{X}_{i}$ is the $n_{i} \times p$ design matrix for individual $i$.

* The covariance matrix of $\mathbf{Y}_{i}$ is
\begin{equation}
\mathbf{V}_{i}(\boldsymbol{\theta}, \sigma^{2}) = \textrm{Cov}(\mathbf{Y}_{i}) = \mathbf{Z}_{i}\boldsymbol{\Sigma}_{\boldsymbol{\theta}}\mathbf{Z}_{i}^{T} + \sigma^{2}\mathbf{I}_{n_{i}}
\end{equation}
    + $\mathbf{Z}_{i}$ is the $n_{i} \times q$ random effects design matrix for individual $i$.

---

* Let $\mathbf{Y}$ be the vector of responses stacked in "long form":
    + $\mathbf{Y} = (Y_{11}, Y_{12}, ...., Y_{mn_{m}})$
    
* Under the assumed linear mixed model \@ref(eq:lmm-general), we have
\begin{equation}
\mathbf{Y} \sim \textrm{Normal}(\mathbf{X}\boldsymbol{\beta}, \mathbf{V})
\end{equation}

* The covariance matrix $\mathbf{V}$ will be **"block diagonal"** diagonal matrix with
the blocks being $\mathbf{V}_{i}(\theta, \sigma^{2})$
\begin{equation}
\mathbf{V} = 
\begin{bmatrix}
\mathbf{V}_{1}(\boldsymbol{\theta}, \sigma^{2}) & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{0} & \mathbf{V}_{2}(\boldsymbol{\theta}, \sigma^{2}) & \mathbf{0} & \ldots & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{V}_{3}(\boldsymbol{\theta}, \sigma^{2})  & \ldots & \mathbf{0} \\
\vdots & & & \ddots & \vdots \\
\mathbf{0} & \mathbf{0} & \mathbf{0} & \ldots & \mathbf{V}_{m}(\boldsymbol{\theta}, \sigma^{2})
\end{bmatrix}
\end{equation}
---

* With the **LMM-Lasso** (@schelldorfer2011), you estimate the vector of fixed effects $\boldsymbol{\beta}$
and the parameters in $\mathbf{V}$ by minimizing the following penalized negative log-likelihood:
\begin{eqnarray}
&& Q_{\lambda}(\boldsymbol{\beta}, \boldsymbol{\theta}, \sigma^{2}) = \frac{1}{2}\log\det(\mathbf{V}) + \frac{1}{2}(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^{T}\mathbf{V}^{-1}
(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}) + \lambda\sum_{j=1}^{p} |\beta_{j}| \nonumber \\
&=& \frac{1}{2}\sum_{i=1}^{m} \log\det\left( \mathbf{V}_{i}(\boldsymbol{\theta}, \sigma^{2}) \right) + \frac{1}{2}\sum_{i=1}^{m} (\mathbf{Y}_{i} - \mathbf{X}_{i}\boldsymbol{\beta})^{T}\mathbf{V}_{i}^{-1}(\boldsymbol{\theta}, \sigma^{2})
(\mathbf{Y}_{i} - \mathbf{X}_{i}\boldsymbol{\beta}) + \lambda\sum_{j=1}^{p} |\beta_{j}| \nonumber \\
\end{eqnarray}

* In @schelldorfer2011, suggest using a **Bayesian information criterion** (BIC) to choose the tuning
parameter $\lambda$.

* This is defined as 
\begin{equation}
\textrm{BIC}_{\lambda} = -2 \times \textrm{log-likelihood} + \log(n) \times df_{\lambda} 
\end{equation}
    + $df_{\lambda}$ is equal to the number of non-zero regression coefficients when using $\lambda$ **plus** the 
    number of paramaters in the matrix $\mathbf{V}_{i}(\boldsymbol{\theta}, \sigma^{2})$.
    
## Lasso for LMMs and GLMMs in R

* One **R** package which fits linear mixed models and generalized linear mixed models
with the Lasso penalty is the **glmmLasso** package.

* There are also methods and R implementations for penalized regression with GEEs. See, 
for example, the paper: @wang2012
    + I won't cover that today.


### Soccer Data
* To briefly show how this works, we can use the **soccer** data from the **glmmLasso** package.

* This is actually not a longitudinal dataset, but it does have repeated measures. 

* This dataset has **54 observations** with **23 unique teams**.

* Each row in this dataset corresponds to data taken from a single team in a single season.
```{r, echo=TRUE}
library(glmmLasso)
data("soccer")
dim(soccer)  ## 54 observations and 16 variables
head(soccer)
length(unique(soccer$team)) ## 23 unique teams
```

---

* The variable `team` represents the soccer team. 
    + Each team has 2 or 3 seasons of data.

* The variable `points` represents the total number of points scored over the course of the season.

* There are a number of other variables that may explain some of the variation in 
points scored: `ball.possession`, `tackles`, etc.

---

* We will use `points` and 10 of the other variables as the fixed-effects covariates.

* It is common in practice to **center and scale** the covariates before running the lasso:
```{r, echo=TRUE}
soccer[,c(4,5,9:16)] <- scale(soccer[,c(4,5,9:16)], center=TRUE, scale=TRUE)
soccer <- data.frame(soccer)
```

---

* To fit an lmm-lasso with $\lambda = 100$ and a random intercept for each team, we can use the following code

```{r, echo=TRUE}
lm.lambda100 <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
                                   + ball.possession + tackles 
                                   + ave.attend + sold.out, rnd = list(team=~1), 
                                  lambda=100, data = soccer)
```

* Note that the random effects model (i.e., the model for $b_{ij}$) is specified through the `rnd` argument.
    + `team = ~1` means that $b_{ij} = u_{i}$ for each $i,j$.
    
    


* To look at the summary of the parameter estimates, use `summary` 
```{r, echo=TRUE}
summary(lm.lambda100)
```

* All coefficient estimates are non-zero except for the "average unfariness score per match" variable

---

* If we set $\lambda = 500$, all of the coefficient estimates will be zero:
```{r, echo=TRUE}
lm.lambda500 <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
                                   + ball.possession + tackles 
                                   + ave.attend + sold.out, rnd = list(team=~1), 
                                   lambda=500, data = soccer)

summary(lm.lambda500)
```



---

* We can find the value of BIC by looking at the `$bic` component of lm.lambda100
```{r, echo=TRUE}
lm.lambda100$bic
```

### Choosing the tuning parameter for the soccer data

* Because $\lambda = 500$ implies that all of the coefficient estimates are zero, we know
that the "best" value of $\lambda$ should be somewhere between $0$ and $500$.

* Let's compute the **BIC** across a grid of $\lambda$ values from $0$ to $500$ and plot the 
result
```{r, echo=TRUE}
lam.seq <- seq(0, 500, by=5)
BIC.values <- rep(0, length(lam.seq))
for(k in 1:length(lam.seq)) {
lm.tmp <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
                          + ball.possession + tackles 
                          + ave.attend + sold.out, rnd = list(team=~1), 
                          lambda=lam.seq[k], data = soccer)
BIC.values[k] <- lm.tmp$bic
}
plot(lam.seq, BIC.values, xlab=expression(lambda), ylab="BIC", main="BIC for soccer data")
```

* It looks like the lowest **BIC** value is in between 0 and 50. 
    + Let's plot the BIC values for a denser grid of $\lambda$ values between 0 and 50

```{r, echo=TRUE}   
lam.seq <- seq(0, 50, by=1)
BIC.values <- rep(0, length(lam.seq))
for(k in 1:length(lam.seq)) {
  lm.tmp <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
                      + ball.possession + tackles 
                      + ave.attend + sold.out, rnd = list(team=~1), 
                      lambda=lam.seq[k], data = soccer)
  BIC.values[k] <- lm.tmp$bic
}
plot(lam.seq, BIC.values, xlab=expression(lambda), ylab="BIC", main="BIC for soccer data")
lines(lam.seq, BIC.values)
```

* The best value of $\lambda$ according to the BIC criterion is $16$:
```{r, echo=TRUE}
lam.seq[which.min(BIC.values)]
```

---

* Let's look at the regression coefficient estimates using $\lambda = 16$
```{r, echo=TRUE}
lm.lambda16 <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
                                   + ball.possession + tackles 
                                   + ave.attend + sold.out, rnd = list(team=~1), 
                                   lambda=16, data = soccer)

summary(lm.lambda16)
```

## Cross-Validation for Longitudinal Data

* **Cross-validation** without any longitudinal or repeated-measures structure is pretty straightforward.

* For longitudinal data, the type of cross-validation can depend on the **prediction goals/context**. 

* In many cases, it makes sense to hold out **random individuals** (or groups) in each **test set**.

* In other words, each **training set** would look like the following:
\begin{equation}
\mathcal{T}_{r} = \{ \textrm{ all } (Y_{ij}, \mathbf{x}_{ij}), \textrm{such that } i \in \mathcal{S} \} 
\end{equation}
where $\mathcal{S}$ is a random subset of indeces.

---

* In cases where you are thinking of using your model for **forecasting**, it may make sense 
to use an **alternative strategy** for cross-validation.

* In this case, you may want to construct the test sets so that they only contain observations
at **"future" time points** when compared with the training set. 

---

* Let's try doing **5-fold** cross-validation with the `soccer` data.

* To do this, it's easier to just create a **team id** variable first
```{r, echo=TRUE}
team.labels<-data.frame(team=unique(soccer$team),team.id=as.numeric(unique(soccer$team)))
soccer <- merge(soccer, team.labels, by="team")
head(soccer)
```

* Now create each of the 5 **test sets**.
```{r, echo=TRUE}
set.seed(2352)
## first create the indices for the test sets
nfolds <- 5
test.groups <- sample(1:nfolds, size=23, replace=TRUE)
test.groups
## test.groups == k means that the observation will be in the kth test set
## For such a small dataset, you may want to randomly generate the
## test sets so that they all have the same size.
```

---

* Now, compute cross-validation estimates of the **mean-squared error** over a grid of $\lambda$ values

```{r, echo=TRUE}
lam.seq <- seq(0, 200, by=5)
MSE <- matrix(0, nfolds, length(lam.seq))
for(j in 1:length(lam.seq)) {
    
    for(k in 1:nfolds) {
       soccer.test <- soccer[test.groups==k,]
       soccer.train <- soccer[test.groups!=k,]
       
       tmp.lm <- glmmLasso(points ~ transfer.spendings + ave.unfair.score 
                 + ball.possession + tackles 
                 + ave.attend + sold.out, rnd = list(team=~1),
                 lambda=lam.seq[j], data = soccer.train)
       
       predicted.values <- predict(tmp.lm, newdata=soccer.test)
       MSE[k,j] <- mean((predicted.values - soccer.test$points)^2)
    }
}
plot(lam.seq, colMeans(MSE), xlab=expression(lambda), ylab="MSE", main="5-fold
     cross-validation for the soccer data")
lines(lam.seq, colMeans(MSE))
```

* According to the cross-validation estimates of prediction error, the 
best value of $\lambda$ is roughly $80$. 

## Penalized Generalized Estimating Equations

* Without any penalization, a generalized estimating equation (GEE) approach to estimating $\boldsymbol{\beta}$ 
works by choosing $\boldsymbol{\beta}$ to solve the following system of equations
\begin{equation}
S_{\alpha}(\boldsymbol{\beta}) = \sum_{i=1}^{m} \mathbf{D}_{i}^{T}\mathbf{V}_{i}^{-1}\left(\mathbf{Y}_{i} - \boldsymbol{\mu}_{i}(\boldsymbol{\beta}) \right) = \mathbf{0}
\end{equation}
   + $\boldsymbol{\mu}_{i}(\boldsymbol{\beta}) = g^{-1}(\mathbf{X}_{i}\boldsymbol{\beta})$: this is a $n_{i} \times 1$ vector
   
   + $\mathbf{D}_{i} = \partial \boldsymbol{\mu}_{i}/\partial \boldsymbol{\beta}$: this is a $n_{i} \times p$ matrix.

* $\mathbf{V}_{i}$ is the "working" covariance matrix of $\mathbf{Y}_{i}$ which can depend on the parameter $\alpha$.

---

* For penalized GEE, we are going to solve the equation $U_{\alpha,\lambda}(\boldsymbol{\beta}) = \mathbf{0}$, 
where $U_{\alpha, \lambda}(\boldsymbol{\beta})$ is defined as
\begin{equation}
U_{\alpha, \lambda}(\boldsymbol{\beta}) = S_{\alpha}(\boldsymbol{\beta}) - \sum_{j=1}^{p} q_{\lambda}(|\beta_{j}|)\textrm{sign}(\beta_{j})
\end{equation}

* Here, $q_{\lambda}()$ is some choice of "penalty" function and $\textrm{sign}(\beta_{j}) = 1$ if $\beta_{j} > 0$
and $\textrm{sign}(\beta_{j}) = -1$ if $\beta_{j} < 0$. 

* The reason for considering $\textrm{sign}(\beta_{j})$ is that we are no longer trying to minimize $U_{\alpha, \lambda}(\boldsymbol{\beta})$, but rather trying to solve $U_{\alpha,\lambda}(\boldsymbol{\beta}) = \mathbf{0}$.
   + You can think of this as setting the derivative of a quasi-penalized log-likelihood to zero and solving it.

---

* There are a number of possible choices for $q_{\lambda}()$.

* Penalized GEE as implemented by the **PGEE** package uses the derivative of the "SCAD" penalty. 

* For $t > 0$, the derivative of the SCAD penalty is defined as
\begin{equation}
q_{\lambda}(t) = \begin{cases}
t & \text{ for } t < \lambda \\
\frac{a\lambda - t}{(a - 1)\lambda} & \text{ for } \lambda \leq t < a\lambda \\
0 & \text{ for } t > a\lambda
\end{cases}
\end{equation}

### The PGEE package

* Just to show the basics of how the PGEE package works, we can look at the **yeastG1** dataset from the PGEE package

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(PGEE)
data(yeastG1)

## look at first 6 rows and first 5 columns
yeastG1[1:6, 1:5]
```

* The response of interest is the continuous measurement `y`.

* There are 96 covariates (besides time). I think these are all just different transcription factors.

* These 96 covariates are not time-varying.

---

* Suppose we want to fit the following marginal **mean** model
\begin{equation}
E(Y_{ij}|\mathbf{x}_{ijk}) = \gamma_{0} + \gamma_{1}t_{ij} + \sum_{k=1}^{96}\beta_{j}x_{ijk}
\end{equation}
   + Note that $x_{ijk}$ does not change across values of $j$.

* To fit the above model with an **AR(1)** correlation structure and $\lambda = 0.1$, you can use the following **R** code
```{r, echo=TRUE, eval=FALSE}
m0 <- PGEE(y ~. -id, id=id, corstr="AR-1", lambda=0.1, data=yeastG1)
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
m0 <- PGEE(y ~. -id, id=id, corstr="AR-1", lambda=0.1, data=yeastG1)
```

* Let's look at the values of the first 5 estimated regression coefficients:
```{r, echo=TRUE}
m0$coefficients[1:5]
```

* The `PGEE` function does not automatically return **exactly zero** regression coefficients,
but you can set those coefficients whose absolute value is less than some small threshold equal to zero.
```{r, echo=TRUE}
length(m0$coefficients)
## 71 out of 98 coefficients are "zero"
sum(abs(m0$coefficients) < 1e-4)
```

---

* The `PGEE` package does have a function to select the best value of $\lambda$ through cross-validation.
     + However, you do have to provide a range of lambda values for the function to search over.

* By trial and error, you can find 
    + a small value $\lambda_{min}$ where most of the coefficients are nonzero.
    + a large value $\lambda_{max}$ where most of the coefficients are zero.
    + then, perform cross-validation over the range $(\lambda_{min}, \lambda_{max})$. 

* Setting $\lambda_{min} = 0.01$ and $\lambda_{max} = 0.03$ seems reasonable.
    + This gives a range of 12-92 for the number of zero coefficients

```{r, results='hide', message=FALSE, warning=FALSE}
mlow <- PGEE(y ~. -id, id=id, corstr="AR-1", lambda=0.01, data=yeastG1)
mhigh <- PGEE(y ~. -id, id=id, corstr="AR-1", lambda=0.3, data=yeastG1)

sum(abs(mlow$coefficients) < 1e-4) ## only 12 out of 98 are zero
sum(abs(mhigh$coefficients) < 1e-4) ## now, 92 out of 98 are zero
```

---

* Now, let's use the `CVfit` function from the `PGEE` package to get the best value of $\lambda$ over
the range $(\lambda_{min}, \lambda_{max})$.
    + Use 5-fold cross-validation using the `fold` argument in `CVfit`.
    + Use a lambda sequence of length 10 from 0.01 to 0.3

```{r, results='hide', message=FALSE, warning=FALSE}
cv.yeast <- CVfit(y ~. -id, id=id, lambda=seq(0.01, 0.3, length.out=10), 
                  fold = 5, data=yeastG1)
```

* The cross-validation done by `CVfit` does automatically assume an **"independent"** working correlation structure.

* The `lam.opt` component of `cv.yeast` gives the optimal value of lambda.
```{r, echo=TRUE}
cv.yeast$lam.opt
```

* Now, we can just use `PGEE` with the optimal value of lambda.
```{r, results='hide', message=FALSE, warning=FALSE}
mfinal <- PGEE(y ~. -id, id=id, corstr="AR-1", lambda=cv.yeast$lam.opt, data=yeastG1)
```

* From the `mfinal` object returned by `PGEE`, we can look at the selected **nonzero** coefficients
```{r, echo=TRUE}
mfinal$coefficients[abs(mfinal$coefficients) > 1e-4]
```

* We can also look at the estimated working correlation matrix
```{r, echo=TRUE}
round(mfinal$working.correlation, 3)
```

* `PGEE` also returns most of the other types of components that functions
  like `lm`, `glm`, `geeglm` return: e.g., `fitted.values`, `residuals`, etc.



## GLMM-Lasso with Binary Outcomes

* You can use `glmmLasso` with **binary outcomes** by adding the `family=binomial()` argument.

* As a quick example, let's look at the `ohio` data from the `geepack` package.
```{r, echo=TRUE}
library(geepack)
data(ohio)
head(ohio)
```

* For the `glmmLasso` function, you do need to make sure the "id variable" is a factor.
```{r, echo=TRUE}
ohio$id <- factor(ohio$id) 
```

* Let's now fit a penalized generalized linear mixed model with $\lambda = 10$:
```{r, echo=TRUE}
ohio.fit10 <- glmmLasso(smoke ~ resp + age, family=binomial(), rnd = list(id=~1),
                      lambda=10, data = ohio)
```

* It looks like the **wheeze status** variable was selected while the **age** variable was not.
```{r, echo=TRUE}
summary(ohio.fit10)
```

---
