[["index.html", "Notes for Case Studies in Health Big Data Preface", " Notes for Case Studies in Health Big Data Nicholas Henderson 2024-02-15 Preface This contains some of the course notes for Biostatistics 629. "],["mixed-models.html", "Chapter 1 Mixed Models for Longitudinal Data Analysis 1.1 Methods for Analyzing Longitudinal Data 1.2 Mixed Models for Continuous Outcomes 1.3 Advantages of using random effects 1.4 Generalized linear mixed models (GLMMs) 1.5 Fitting Linear Mixed Models (LMMs) and Generalized Linear Mixed models (GLMMs) in R 1.6 Exercises", " Chapter 1 Mixed Models for Longitudinal Data Analysis 1.1 Methods for Analyzing Longitudinal Data Longitudinal data refers to data that: Has multiple individuals. Each individual has multiple observations over time. We will denote the outcomes of interest with \\(Y_{ij}\\). \\(Y_{ij}\\) - outcome for individual \\(i\\) at time \\(t_{ij}\\). The \\(i^{th}\\) individual has \\(n_{i}\\) observations: \\(Y_{i1}, \\ldots, Y_{in_{i}}\\). There will be \\(m\\) individuals in the study (so \\(1 \\leq i \\leq m\\)). \\(\\mathbf{x}_{ij} = (x_{ij1}, \\ldots, x_{ijp})\\) is the vector of covariates for individual \\(i\\) at time \\(t_{ij}\\). Note that the vector \\(\\mathbf{x}_{ij}\\) will often include the time point \\(t_{ij}\\) as a covariate. Note: For your project, you may have to do some data processing of the data to get it into the form \\((Y_{ij}, \\mathbf{x}_{ij})\\), for \\(j = 1,\\ldots,n_{i}\\). For example, although the data that you analyze for the project look at certain variables over time, they could be recorded “asynchronously” in some way. This means meaning that the outcomes \\(Y_{ij}\\) and the covariates \\(\\mathbf{x}_{ij}\\) are not exactly matched in time. For example, on a given day, you may observe \\(Y_{ij}\\) at times 9:00, 10:00, and 11:00, but observe the covariate of interest at 9:17, 9:53, and 11:08. Or, your outcome \\(Y_{ij}\\) may only be observed once per week, while the covariates of interest are measured multiple times per week. We will discuss some ways of organizing asynchronous longitudinal data later in the class. Figure 1.1: Plots of individual-specific trajectories from the sleepstudy data with the trajectories of 4 individuals highlighted. The above figure shows an example of outcomes from a longitudinal study (the sleepstudy data in the lme4 package). In the sleepstudy data: The observation time points of observation \\(t_{ij}\\) are the same for each individual \\(i\\). So, we can say \\(t_{ij} = t_{j}\\) for all \\(i\\). The outcome \\(Y_{ij}\\) is the reaction time for the \\(i^{th}\\) individual at time point \\(t_{j}\\). The 10 time points are \\((t_{1}, \\ldots, t_{10}) = (0, 1, \\ldots, 9)\\). There are \\(m = 18\\) individuals in the study. Each individual is observed at \\(10\\) time points. So, \\(n_{i} = 10\\) for every \\(i\\). Most of the well-known regression-based methods for analyzing longitudinal data can be classified (see Diggle et al. (2013)) into one of the three following categories: Random effects/mixed models, Marginal models, Transition models Random effects/Mixed Models “Random effects” are added to the regression model describing the outcomes for each individual. These “random regression coefficients” are viewed as a sample from some distribution. Marginal models Regression coefficients have a “population average” interpretation. Only mean of \\(Y_{ij}\\) and correlation structure of \\((Y_{i1}, \\ldots, Y_{in_{i}})\\) are modeled. Generalized estimating equations (GEEs) are often used for estimating model parameters. Transition models Choose a probability model for the distribution of \\(Y_{ij}\\) given the value of the outcome at the previous time point \\(Y_{ij-1}\\). 1.2 Mixed Models for Continuous Outcomes If each \\(Y_{ij}\\) is a continuous outcome and we were to build a regression model without any random effects, we might assume something like: \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + e_{ij} \\tag{1.1} \\end{equation}\\] \\(\\mathbf{x}_{ij} = (x_{ij1}, \\ldots, x_{ijp})\\) is the vector of covariates for individual \\(i\\) at time \\(t_{ij}\\). The vector \\(\\mathbf{x}_{ij}\\) could contain individual information such as smoking status or age. \\(\\mathbf{x}_{ij}\\) could also contain some of the actual time points: \\(t_{ij}, t_{ij-1}, ...\\) or transformations of these time points. The regression model (1.1) assumes the same mean function \\(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta}\\) holds for all individuals that have the same value of \\(\\mathbf{x}_{ij}\\). It is often reasonable to assume that the regression coefficients vary across individuals. This can often better account for heterogeneity across individuals. The figure below shows 3 different regression lines from the sleepstudy data. Each regression line was estimated using only data from one individual. Figure 1.2: Separately estimated regression lines for 3 subjects in the sleepstudy data. Figure 1.1 suggests there is some heterogeneity in the relationship between study day and response time across individuals. The response time of Subject 309 changes very little over time. For Subject 308, there is a more clear positive association between response time and day of study. For the sleepstudy data, a linear regression for reaction time vs. study day which assumes that Expected response time is a linear function of study day, All individuals have the same regression coefficients, would have the form: \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1} t_{j} + e_{ij} \\end{equation}\\] If we allowed each individual to have his/her own intercept and slope, we could instead consider the following model \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1} t_{j} + u_{i0} + u_{i1}t_{j} + e_{ij} \\tag{1.2} \\end{equation}\\] \\(\\beta_{0} + u_{i0}\\) - intercept for individual \\(i\\). \\(\\beta_{1} + u_{i1}\\) - slope for individual \\(i\\). If we assume \\((u_{i0}, u_{i1})\\) are sampled from some distribution, \\(u_{i0}\\) and \\(u_{i1}\\) are referred to as random effects. Typically, it is assumed that \\((u_{i0}, u_{i1})\\) are sampled from a multivariate normal distribution with mean zero: \\[\\begin{equation} (u_{i0}, u_{i1}) \\sim \\textrm{Normal}( \\mathbf{0}, \\boldsymbol{\\Sigma}_{\\tau} ) \\end{equation}\\] Model (1.2) is called a mixed model because it contains both fixed effects \\((\\beta_{0}, \\beta_{1})\\) and random effects \\((u_{i0}, u_{i1})\\). More generally, a linear mixed model (LMM) for longitudinal data will have the form: \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} + e_{ij} \\tag{1.3} \\end{equation}\\] \\(\\boldsymbol{\\beta}\\) - vector of fixed effects \\(\\mathbf{u}_{i}\\) - vector of random effects If we stack the responses into a long vector \\(\\mathbf{Y}\\) and random effects into a long vector \\(\\mathbf{u}\\) \\(\\mathbf{Y} = (Y_{11}, Y_{12}, ...., Y_{mn_{m}})\\) - this vector has length \\(\\sum_{k=1}^{m} n_{k}\\) \\(\\mathbf{u} = (u_{10}, u_{11}, ...., u_{mq})\\) - this vector has length \\(m \\times (q + 1)\\). Then, we can write the general form (1.3) of the LMM as \\[\\begin{equation} \\mathbf{Y} = \\mathbf{X}\\tilde{\\boldsymbol{\\beta}} + \\mathbf{Z}\\mathbf{u} + \\mathbf{e} \\end{equation}\\] \\(i^{th}\\) row of \\(\\mathbf{X}\\) is \\((1, \\mathbf{x}_{ij}^{T})\\). \\(i^{th}\\) row of \\(\\mathbf{Z}\\) is \\(\\mathbf{z}_{ij}^{T}\\). \\(\\tilde{\\boldsymbol{\\beta}} = (\\beta_{0}, \\boldsymbol{\\beta})\\). Constructing an LMM can be thought of as choosing the desired “\\(\\mathbf{X}\\)” and “\\(\\mathbf{Z}\\)” matrices. 1.3 Advantages of using random effects 1.3.1 Within-subject correlation Using an LMM automatically accounts for the “within-subject” correlation. That is, the correlation between two observations from the same individual. This correlation arises because observations on the same individual “share” common random effects. The correlation between the \\(j^{th}\\) and \\(k^{th}\\) observation from individual \\(i\\) is \\[\\begin{equation} \\textrm{Corr}(Y_{ij}, Y_{ik}) = \\frac{ \\mathbf{z}_{ij}^{T}\\boldsymbol{\\Sigma}_{\\tau}\\mathbf{z}_{ik} }{ \\sqrt{\\sigma^{2} + \\mathbf{z}_{ij}^{T}\\boldsymbol{\\Sigma}_{\\tau}\\mathbf{z}_{ij}}\\sqrt{\\sigma^{2} + \\mathbf{z}_{ik}^{T}\\boldsymbol{\\Sigma}_{\\tau}\\mathbf{z}_{ik}}} \\end{equation}\\] When using only a random intercept, the correlation between \\(Y_{ij}\\) and \\(Y_{ik}\\) is \\[\\begin{equation} \\textrm{Corr}(Y_{ij}, Y_{ik}) = \\frac{ \\sigma_{u}^{2} }{ \\sigma^{2} + \\sigma_{u}^{2} } \\end{equation}\\] In this case, \\(\\mathbf{z}_{ij} = 1\\) and \\(u_{i} \\sim \\textrm{Normal}(0, \\sigma_{u}^{2})\\) \\(\\sigma^{2}\\) is the variance of the residual term \\(e_{ij}\\) For longitudinal data, one criticism of the random intercept model is that the within-subject correlation does not vary across time. 1.3.2 Inference about Heterogeneity - Variance of Random Effects One of the goals of the data analysis may be to characterize the heterogeneity in the relationship between the outcome and some of the covariates across individuals. Looking at the estimates of the variance of the random effects is one way of addressing this goal. An estimate of \\(\\textrm{Var}( u_{ih} )\\) “substantially greater than zero” is an indication that there is variability in the regression coefficient corresponding to \\(u_{ih}\\) across individuals. For example, with the random intercept and slope model for the sleepstudy data \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1}t_{j} + u_{i0} + u_{i1}t_{j} + e_{ij} \\end{equation}\\] If \\(\\textrm{Var}( u_{i1} )\\) is “large”, this implies that the response to additional days of sleep deprivation varies considerably across individuals. The response time of some individuals is not impacted much by additional days of little sleep. Some individuals respond strongly to additional days of little sleep. 1.3.3 Best Linear Unbiased Prediction You may want to estimate or “predict” the mean function/trajectory of a given individual. This means you want to estimate/predict the following quantity: \\[\\begin{equation} \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} \\end{equation}\\] The “Best Linear Unbiased Predictor” (BLUP) of this is \\[\\begin{equation} \\textrm{BLUP}(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i}) = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}E(\\mathbf{u}_{i}|Y_{i1}, \\ldots, Y_{in_{i}}) \\end{equation}\\] I would think of the values of \\(\\textrm{BLUP}(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i})\\) (for different values of \\(j\\)) as an estimate of the “true trajectory” (i.e., the true mean) of the \\(i^{th}\\) individual. The observed longitudinal outcomes from individual \\(i\\) are a “noisy estimate” of that individual’s true trajectory. The BLUPs are more stable “shrinkage” estimates of the trajectory of individual \\(i\\). These are called shrinkage estimates because often shrinks the estimate that would be obtained using only data from individual \\(i\\) towards the “overall” estimate \\(\\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta}\\). For example, if we had the intercept-only model \\(Y_{ij} = \\beta_{0} + u_{i} + e_{ij}\\), the value of the BLUPs is \\[\\begin{equation} \\textrm{BLUP}(\\beta_{0} + u_{i}) = \\frac{n_{i}\\sigma_{u}^{2}}{\\sigma^{2} + n_{i}\\sigma_{u}^{2} }\\bar{Y}_{i.} + \\Big(1 - \\frac{n_{i}\\sigma_{u}^{2}}{\\sigma^{2} + n_{i}\\sigma_{u}^{2} }\\Big)\\bar{Y}_{..} \\end{equation}\\] \\(\\bar{Y}_{i.}\\) is the sample mean from individual-\\(i\\) data \\(\\bar{Y}_{i.}\\) would be the estimate of the intercept if we only looked at data from the \\(i^{th}\\) individual. \\(\\bar{Y}_{..}\\) - overall mean \\(\\bar{Y}_{..}\\) would be the estimate of the intercept if we ignored variation in intercepts across individuals. You can also think of \\(\\textrm{BLUP}(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i})\\) as a prediction of what the observed trajectory for individual \\(i\\) would be if that individual were in a future study under the same conditions. Say \\(Y_{i1}&#39;, \\ldots, Y_{in_{i}}&#39;\\) are the observations for individual \\(i\\) in a future study. The outcomes in the future study are determined by \\[\\begin{equation} Y_{ij}&#39; = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} + e_{ij}&#39; \\end{equation}\\] The expectation of \\(Y_{ij}&#39;\\) given the observed data in our longitudinal study is \\[\\begin{eqnarray} E(Y_{ij}&#39;|Y_{i1}, \\ldots, Y_{in_{i}}) &amp;=&amp; \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}E(\\mathbf{u}_{i}|Y_{i1}, \\ldots, Y_{in_{i}}) \\nonumber \\\\ &amp;=&amp; \\textrm{BLUP}(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i}) \\nonumber \\end{eqnarray}\\] 1.4 Generalized linear mixed models (GLMMs) Generalized linear models (GLMs) are used to handle data (often “non-continuous”) that can’t be reasonably modeled with a Gaussian distribution. The most common scenarios where you would use GLMs in practice are binary, count, and multinomial outcomes. With a generalized linear mixed model (GLMM), you assume that a GLM holds conditional on the value of the random effects. 1.4.1 GLMMs with Binary Outcomes Under the GLM framework, the usual approach for handling binary outcomes is logistic regression. The assumptions underlying logistic regression are: The outcomes are independent Each outcome follows a Bernoulli distribution. The log-odds parameter is assumed to be a linear combination of the covariates. With the GLMM version of logistic regression, we will make almost the same assumptions as the regular GLM version of logistic regression. The main difference is that each assumption in the GLMM will be conditional on the values of the random effects. To be specific, for longitudinal binary outcomes \\(Y_{ij}\\), the GLMM version of logistic regression assumes the following: Conditional on the vector of random effects \\(\\mathbf{u}_{i}\\) \\[\\begin{equation} Y_{i1}, \\ldots, Y_{in_{i}}|\\mathbf{u}_{i} \\textrm{ are independent } \\end{equation}\\] Conditional on \\(\\mathbf{u}_{i}\\), each \\(Y_{ij}\\) has a Bernoulli distribution \\[\\begin{equation} Y_{ij}|\\mathbf{u}_{i} \\sim \\textrm{Bernoulli}\\big\\{ p_{ij}(\\mathbf{u}_{i}) \\big\\} \\end{equation}\\] so that \\(p_{ij}( \\mathbf{u}_{i} ) = P(Y_{ij} = 1| \\mathbf{u}_{i})\\). The “conditional” log-odds term \\(\\log\\{ p_{ij}(\\mathbf{u}_{i})/[1 - p_{ij}(\\mathbf{u}_{i})] \\}\\) is a linear combination of the covariates and the random effects vector \\(\\mathbf{u}_{i}\\): \\[\\begin{equation} \\textrm{logit}\\{ p_{ij}(\\mathbf{u}_{i}) \\} = \\log\\Big( \\frac{ p_{ij}(\\mathbf{u}_{i})}{ 1 - p_{ij}(\\mathbf{u}_{i}) } \\Big) = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} \\end{equation}\\] As with a linear mixed model, we assume that the random-effects vector \\(\\mathbf{u}_{i}\\) has a multivariate normal distribution with mean zero and covariance matrix \\(\\boldsymbol{\\Sigma}_{\\tau}\\) \\[\\begin{equation} \\mathbf{u}_{i} \\sim \\textrm{Normal}( \\mathbf{0}, \\boldsymbol{\\Sigma}_{\\tau}) \\end{equation}\\] 1.4.2 GLMMs with Count Outcomes For count outcomes, responses are typically assumed to follow a Poisson distribution and sometimes a negative binomial distribution - conditional on the values of the random effects. For the Poisson model, we assume \\(Y_{ij}|\\mathbf{u}_{i} \\sim \\textrm{Poisson}\\{ \\mu_{ij}( \\mathbf{u}_{i} ) \\}\\), \\[\\begin{equation} E(Y_{ij}| \\mathbf{u}_{i}) = \\mu_{ij}(\\mathbf{u}_{i}) \\qquad \\textrm{Var}( Y_{ij}| \\mathbf{u}_{i} ) = \\mu_{ij}(\\mathbf{u}_{i}) \\end{equation}\\] One common problem with the Poisson distribution is overdispersion (i.e., variance is greater than the mean). The variance of the Poisson equals the mean. While the marginal variance will not equal the mean in a GLMM, requiring the conditional means and variances to be equal could lead to a poor fit. For the negative binomial model, we assume \\(Y_{ij}|\\mathbf{u}_{i} \\sim \\textrm{NB}\\{ \\mu_{ij}( \\mathbf{u}_{i}) , \\phi \\}\\), \\[\\begin{equation} E(Y_{ij}| \\mathbf{u}_{i}) = \\mu_{ij}(\\mathbf{u}_{i}) \\qquad \\textrm{Var}( Y_{ij}| \\mathbf{u}_{i} ) = \\mu_{ij}(\\mathbf{u}_{i}) + \\phi\\mu_{ij}^{2}(\\mathbf{u}_{i}) \\end{equation}\\] \\(\\phi\\) is often referred to as the overdispersion parameter. With a GLMM model for count data, it is typical to model the log of the conditional mean \\(\\mu_{ij}(\\mathbf{u}_{i})\\) with a linear regression: \\[\\begin{equation} \\log\\{ \\mu_{ij}(\\mathbf{u}_{i}) \\} = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} \\end{equation}\\] Again, for the Poisson GLMM, the usual assumption for the random effects is that \\[\\begin{equation} \\mathbf{u}_{i} \\sim \\textrm{Normal}( \\mathbf{0}, \\boldsymbol{\\Sigma}_{\\tau}) \\end{equation}\\] 1.5 Fitting Linear Mixed Models (LMMs) and Generalized Linear Mixed models (GLMMs) in R The lme4 package is probably the most general package for fitting LMMs and GLMMs. library(lme4) With Python, you can use the mixedlm function from the statmodels module to fit linear mixed models. 1.5.1 Fitting LMMs with the sleepstudy data To start off, let’s use the sleepstudy longitudinal data in lme4 and look at the data from the first two individuals in this data. data(sleepstudy) dim(sleepstudy) # 18 individuals, each with 10 observations ## [1] 180 3 sleepstudy[1:20,] # Data from the subjects with ids: 308 and 309 ## Reaction Days Subject ## 1 249.5600 0 308 ## 2 258.7047 1 308 ## 3 250.8006 2 308 ## 4 321.4398 3 308 ## 5 356.8519 4 308 ## 6 414.6901 5 308 ## 7 382.2038 6 308 ## 8 290.1486 7 308 ## 9 430.5853 8 308 ## 10 466.3535 9 308 ## 11 222.7339 0 309 ## 12 205.2658 1 309 ## 13 202.9778 2 309 ## 14 204.7070 3 309 ## 15 207.7161 4 309 ## 16 215.9618 5 309 ## 17 213.6303 6 309 ## 18 217.7272 7 309 ## 19 224.2957 8 309 ## 20 237.3142 9 309 The sleepstudy data is an example of longitudinal data stored in long format (as opposed to “wide” format). In long format, each row of the dataset corresponds to an observation from one individual at one time point. The lmer function in lme4 fits linear mixed models. This has many of the same features as the lm function in R. To fit an LMM with lmer, the main thing to do is to specify the “X” part of the model (i.e., the fixed effects) and the “Z” part of the model (i.e., the random effects). The “X” part of the model is done using the exact same “formula notation” used in the lm function. The “Z” part of the model is done using the following type of syntax: (formula | group_var) group_var is the “grouping variable” used for the random effects For longitudinal data, this would be the variable which identifies each individual. For example, this might be an identifier variable which stores a separate id for each person. 1.5.1.1 LMM with a single, random intercept for each subject Let’s fit an LMM where there is a fixed slope for time and only a random intercept for each Subject \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1}t_{j} + u_{i} + e_{ij} \\tag{1.4} \\end{equation}\\] For the “X” part of this model, we use Reaction ~ Days. This gives us a fixed intercept and a fixed slope for the Days variable. For the “Z” part of this model, we just add (1|Subject). This says that there is only a random intercept within the grouping variable Subject. Putting these two together, we can fit the LMM (1.4) using the following code: lmm.sleep.intercept &lt;- lmer(Reaction ~ Days + (1|Subject), data = sleepstudy) You can always use the model.matrix method on the fitted lmer object to check that the “X” and “Z” matrices correspond to the model you want. Let’s look at the first 5 rows of the “X” matrix from lmm.sleep.intercept x.mat &lt;- model.matrix(lmm.sleep.intercept) ## This design matrix should have an intercept column ## and a column which stores the &quot;Days&quot; variable x.mat[1:5,] ## (Intercept) Days ## 1 1 0 ## 2 1 1 ## 3 1 2 ## 4 1 3 ## 5 1 4 Let’s look at the first 20 rows of the “Z” matrix from lmm.intercept ## Use argument type = &quot;random&quot; to get random-effects design matrix z.mat &lt;- model.matrix(lmm.sleep.intercept, type=&quot;random&quot;) z.mat[1:20,] # The . values in zmat correspond to zeros ## 20 x 18 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 18 column names &#39;308&#39;, &#39;309&#39;, &#39;310&#39; ... ]] ## ## 1 1 . . . . . . . . . . . . . . . . . ## 2 1 . . . . . . . . . . . . . . . . . ## 3 1 . . . . . . . . . . . . . . . . . ## 4 1 . . . . . . . . . . . . . . . . . ## 5 1 . . . . . . . . . . . . . . . . . ## 6 1 . . . . . . . . . . . . . . . . . ## 7 1 . . . . . . . . . . . . . . . . . ## 8 1 . . . . . . . . . . . . . . . . . ## 9 1 . . . . . . . . . . . . . . . . . ## 10 1 . . . . . . . . . . . . . . . . . ## 11 . 1 . . . . . . . . . . . . . . . . ## 12 . 1 . . . . . . . . . . . . . . . . ## 13 . 1 . . . . . . . . . . . . . . . . ## 14 . 1 . . . . . . . . . . . . . . . . ## 15 . 1 . . . . . . . . . . . . . . . . ## 16 . 1 . . . . . . . . . . . . . . . . ## 17 . 1 . . . . . . . . . . . . . . . . ## 18 . 1 . . . . . . . . . . . . . . . . ## 19 . 1 . . . . . . . . . . . . . . . . ## 20 . 1 . . . . . . . . . . . . . . . . The . values in z.mat are just zeros. Notice that each Subject has its own “intercept” column. This what we want - each Subject has its own intercept. Let’s look at the estimated parameters from the LMM with random intercepts using summary summary(lmm.sleep.intercept) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Reaction ~ Days + (1 | Subject) ## Data: sleepstudy ## ## REML criterion at convergence: 1786.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.2257 -0.5529 0.0109 0.5188 4.2506 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Subject (Intercept) 1378.2 37.12 ## Residual 960.5 30.99 ## Number of obs: 180, groups: Subject, 18 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 251.4051 9.7467 25.79 ## Days 10.4673 0.8042 13.02 ## ## Correlation of Fixed Effects: ## (Intr) ## Days -0.371 The estimated fixed-effects intercept is \\(\\hat{\\beta}_{0} = 251.4\\), and the estimated fixed-effects slope is \\(\\hat{\\beta}_{1} = 10.5\\). The estimated variance of the random intercept is \\(\\hat{\\tau}^{2} = 1378.2\\) (standard deviation is \\(\\hat{\\tau} = 37.1\\)). i.e., it is estimated that \\(u_{i} \\sim \\textrm{Normal}(0, 1378.2)\\). 1.5.1.2 LMM with both a random intercept and slope for each subject Now, let’s fit an LMM where there is a fixed slope for time and both a random intercept and slope for each Subject \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1}t_{j} + u_{i0} + u_{i1}t_{j} + e_{ij} \\tag{1.5} \\end{equation}\\] This is done with lmer using the following code: lmm.sleep.slope &lt;- lmer(Reaction ~ Days + (Days|Subject), data = sleepstudy) Again, let’s check the “X” and “Z” matrices from lmm.sleep.slope to double-check that everything makes sense x.mat2 &lt;- model.matrix(lmm.sleep.slope) ## This design matrix should be the same as that from lmm.sleep.intercept x.mat2[1:5,] ## (Intercept) Days ## 1 1 0 ## 2 1 1 ## 3 1 2 ## 4 1 3 ## 5 1 4 First 20 rows of the “Z” matrix from lmm.sleep.slope: ## Use argument type = &quot;random&quot; to get random-effects design matrix z.mat2 &lt;- model.matrix(lmm.sleep.slope, type=&quot;random&quot;) z.mat2[1:20,] # The . values in zmat2 correspond to zeros ## 20 x 36 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 36 column names &#39;308&#39;, &#39;308&#39;, &#39;309&#39; ... ]] ## ## 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 1 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 1 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 1 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 1 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 7 1 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 8 1 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 9 1 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 10 1 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 11 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 12 . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 13 . . 1 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 14 . . 1 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 15 . . 1 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 16 . . 1 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 17 . . 1 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 18 . . 1 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 19 . . 1 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 20 . . 1 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Note that the two columns for each Subject in z.mat2 are of the form \\((1, t_{j})\\), which is what we want. Let’s look at the estimated parameters from lmm.sleep.slope summary(lmm.sleep.slope) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Reaction ~ Days + (Days | Subject) ## Data: sleepstudy ## ## REML criterion at convergence: 1743.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.9536 -0.4634 0.0231 0.4634 5.1793 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 612.10 24.741 ## Days 35.07 5.922 0.07 ## Residual 654.94 25.592 ## Number of obs: 180, groups: Subject, 18 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 251.405 6.825 36.838 ## Days 10.467 1.546 6.771 ## ## Correlation of Fixed Effects: ## (Intr) ## Days -0.138 The estimated fixed-effects coefficients are \\(\\hat{\\beta}_{0} = 251.4\\), and \\(\\hat{\\beta}_{1} = 10.5\\) respectively. The estimated standard deviation and correlation of the random effects are Estimated standard deviation of \\(u_{i0}\\) is \\(24.7\\). Estimated standard deviation of \\(u_{i1}\\) is \\(5.9\\). Estimated correlation between \\(u_{i0}\\) and \\(u_{i1}\\) is \\(0.07\\). Rather than always printing out the entire summary, you can directly extract the estimates of the fixed effects with coef: coef( summary(lmm.sleep.slope) ) ## Estimate Std. Error t value ## (Intercept) 251.40510 6.824597 36.838090 ## Days 10.46729 1.545790 6.771481 To directly extract the estimates of the variance (or standard deviation) of the random effects, you can use: VarCorr( lmm.sleep.slope ) ## Groups Name Std.Dev. Corr ## Subject (Intercept) 24.7407 ## Days 5.9221 0.066 ## Residual 25.5918 Interpreting the estimated variance of the random effects One way to think about the magnitude of the variance components is to look at the 5th and 95th percentiles of the random effects distribution. For example, if you only have a random intercept term, then roughly \\(90\\%\\) of individuals will have an intercept that falls in the interval \\([\\hat{\\beta}_{0} - 1.64\\hat{\\sigma}_{u0}, \\hat{\\beta}_{0} + 1.64\\hat{\\sigma}_{u0}]\\). If you have a random slope term, then roughly \\(90\\%\\) of individuals will have an intercept that falls in the interval \\([\\hat{\\beta}_{1} - 1.64\\hat{\\sigma}_{u1}, \\hat{\\beta}_{1} + 1.64\\hat{\\sigma}_{u1}]\\). Another idea for helping to interpret the magnitude of the random effects is to plot many random trajectories for specific choices of the covariate vector \\(\\mathbf{x}_{i}\\) (if the \\(\\mathbf{x}_{i}\\) vary across individuals). For example, in the sleepstudy data, you could plot \\(\\hat{\\beta}_{0} + u_{i0} + \\hat{\\beta}_{1}t_{j} + u_{i1}t_{j}\\) where the pairs \\((u_{i0}, u_{i1})\\) are generated from the estimated joint Normal distribution. Sigma.hat &lt;- VarCorr( lmm.sleep.slope )$Subject # This is the random-effects # covariance matrix ndraws &lt;- 100 Sigma.hat.sqrt &lt;- chol(Sigma.hat) beta.hat &lt;- coef( summary(lmm.sleep.slope) )[,1] # estimated fixed effects print(beta.hat) ## (Intercept) Days ## 251.40510 10.46729 plot(sleepstudy$Days, sleepstudy$Reaction, type=&quot;n&quot;, xlab=&quot;Days&quot;, ylab=&quot;Response&quot;, las=1, main=&quot;sleepstudy: Variation in subject-specific trajectories&quot;) for(k in 1:ndraws) { uvec.draw &lt;- Sigma.hat.sqrt%*%rnorm(2) # draw random (ui0, ui1) pair trajectory &lt;- beta.hat[1] + uvec.draw[1] + (0:9)*(beta.hat[2] + uvec.draw[2]) lines(0:9, trajectory) } Figure 1.3: Random trajectories for sleepstudy data using the estimated intercept and slope random-effects variances. 1.5.2 Model Comparison of LMMs using anova The anova function can directly compare nested linear mixed models that are fit by lmer. That is, where one larger model contains all of the components of the smaller model. To use anova, you should include the argument REML = FALSE so lmer uses maximum likelihood estimation. As an example, let’s compare the following two models on the sleelpstudy data Model M0: Only has an intercept as a fixed effect and a random intercept for each person. Model M1: Has an intercept and slope as the fixed effects and a random intercept for each person. Let’s first fit these models using REML = FALSE: M0 &lt;- lmer(Reaction ~ 1 + (1|Subject), data = sleepstudy, REML=FALSE) M1 &lt;- lmer(Reaction ~ Days + (1|Subject), data = sleepstudy, REML=FALSE) Running anova(M0, M1) will perform a chi-square test comparing these two models. anova(M0, M1) also reports an AIC and BIC measure. anova(M0, M1) ## Data: sleepstudy ## Models: ## M0: Reaction ~ 1 + (1 | Subject) ## M1: Reaction ~ Days + (1 | Subject) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## M0 3 1916.5 1926.1 -955.27 1910.5 ## M1 4 1802.1 1814.8 -897.04 1794.1 116.46 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can see that the chi-square statistic is very large. There is strong evidence in favor of model M1 over M0. Now, suppose we want to compare the following two models Model M1: Has an intercept and slope as the fixed effects and a random intercept for each person. Model M2: Has an intercept and slope as the fixed effects and a random intercept and slope for each person. To compare M1 with M2, let’s fit M2 with REML = FALSE: M2 &lt;- lmer(Reaction ~ Days + (Days|Subject), data = sleepstudy, REML=FALSE) Now, use anova(M1, M2) to compare these models directly anova(M1, M2) ## Data: sleepstudy ## Models: ## M1: Reaction ~ Days + (1 | Subject) ## M2: Reaction ~ Days + (Days | Subject) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## M1 4 1802.1 1814.8 -897.04 1794.1 ## M2 6 1763.9 1783.1 -875.97 1751.9 42.139 2 7.072e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Based on the chi-square statistic and associated p-value, there is strong evidence favoring M2 over M1. 1.5.3 Extracting BLUPs in lme4 To get the “BLUPs” the intercepts and slopes \\(\\textrm{BLUP}(u_{i0})\\) and \\(\\textrm{BLUP}(u_{i1})\\), use ranef blups.slope &lt;- ranef( lmm.sleep.slope ) To plot these, use dotplot (you will need to load the lattice package first) library(lattice) dotplot(blups.slope) ## $Subject ## This plots subjects sorted by individual-specific estimates of intercepts To extract the “predicted” random effects into a DataFrame use ranef.df &lt;- as.data.frame( blups.slope) head(ranef.df) ## grpvar term grp condval condsd ## 1 Subject (Intercept) 308 2.258551 12.07086 ## 2 Subject (Intercept) 309 -40.398738 12.07086 ## 3 Subject (Intercept) 310 -38.960409 12.07086 ## 4 Subject (Intercept) 330 23.690620 12.07086 ## 5 Subject (Intercept) 331 22.260313 12.07086 ## 6 Subject (Intercept) 332 9.039568 12.07086 This returns a data frame of the BLUPs for each random effect along with a “standard error” for each BLUP. What we discussed earlier in Section 1.3, were the BLUPs for \\(\\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}\\mathbf{u}_{i}\\) not just the individual components of \\(\\mathbf{u}_{i}\\). For this random intercept and slope model, this is \\(\\textrm{BLUP}(\\beta_{0} + \\beta_{1}t_{j} + u_{i0} + u_{i1}t_{j})\\) These are obtained by using the fitted method blup.full &lt;- fitted( lmm.sleep.slope ) # Should be a vector of length 180 If we plot \\(\\textrm{BLUP}(\\beta_{0} + \\beta_{1}t_{j} + u_{i0} + u_{i1}t_{j})\\) as a function of time for all individuals, it will look like the following: library(ggplot2) # First add the BLUPs to the sleepstudy data as a separate variable sleepstudy$blups &lt;- blup.full # Now plot BLUPs vs. study data for each subject ggplot(sleepstudy, aes(x=Days, y=blups, group=Subject)) + geom_line(aes(color=Subject)) + labs(title = &quot;BLUPs in Random Intercept + Random Slope Model&quot;, y = &quot;Reaction Time&quot;) 1.5.4 Fitting Binary GLMMs using the Ohio data To use the ohio data, we will first load the geepack R package: library(geepack) This dataset has 2148 observations from 537 individuals data(ohio) head(ohio, 12) # look at first 12 rows of ohio ## resp id age smoke ## 1 0 0 -2 0 ## 2 0 0 -1 0 ## 3 0 0 0 0 ## 4 0 0 1 0 ## 5 0 1 -2 0 ## 6 0 1 -1 0 ## 7 0 1 0 0 ## 8 0 1 1 0 ## 9 0 2 -2 0 ## 10 0 2 -1 0 ## 11 0 2 0 0 ## 12 0 2 1 0 The outcome of interest in ohio is “wheezing status”: 1 - yes, 0 - no. The resp variable contains wheezing status. The id variable contains the unique identifier for each individual. The age in the ohio dataset is the time variable. The age variable is recorded as: (age in years - 9). Each individual starts the study at 7 years of age. The smoke variable is an indicator of maternal smoking at the starting year of the study. In lme4, fitting a GLMM with binary responses can be done with the glmer function. The glmer function has the following syntax: glmer(formula, data, family) The formula argument uses the same syntax as lmer When handling binary outcomes, you need to specify the family argument as: family = binomial. Just exploring this data by looking at the raw proportions, it appears that probability of wheezing decreases as age increases (within each level of smoking) maternal smoking increases the probability of wheezing at each age library(dplyr) prop_summary_ohio &lt;- ohio %&gt;% group_by(smoke, age) %&gt;% summarize( prop_wheeze = mean(resp) ) prop_summary_ohio ## # A tibble: 8 × 3 ## # Groups: smoke [2] ## smoke age prop_wheeze ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 -2 0.16 ## 2 0 -1 0.149 ## 3 0 0 0.143 ## 4 0 1 0.106 ## 5 1 -2 0.166 ## 6 1 -1 0.209 ## 7 1 0 0.187 ## 8 1 1 0.139 So, we are probably going to want to include both age and smoke in our model. 1.5.4.1 A Random Intercept Model Let’s use a GLMM to explore the relationship between wheezing status and the: age of the child maternal smoking status A GLMM for wheezing status which has age and smoking status as fixed effects and random individual-specific intercepts can be expressed as \\[\\begin{equation} \\textrm{logit}\\{ p_{ij}(u_{i}) \\} = \\beta_{0} + \\beta_{age}\\textrm{age}_{ij} + \\beta_{smk}\\textrm{smoke}_{i} + u_{i} \\tag{1.6} \\end{equation}\\] Model (1.6) can be fit with the following code # id is the grouping variable ohio.intercept &lt;- glmer(resp ~ age + smoke + (1 | id), data = ohio, family = binomial) coef(summary(ohio.intercept)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.3739539 0.27497502 -12.270038 1.311914e-34 ## age -0.1767645 0.06796698 -2.600741 9.302258e-03 ## smoke 0.4147806 0.28704052 1.445024 1.484510e-01 VarCorr(ohio.intercept) ## Groups Name Std.Dev. ## id (Intercept) 2.3432 For a binary GLMM, the estimated standard deviation for the random intercept can be a little hard to interpret, though this value seems rather large to me. One way to report this is to look at the variation in \\(p_{ij}(u_{i})\\) for different values of age and smoking status. For this purpose, you could report the interval \\(\\textrm{expit}\\big(\\hat{\\beta}_{0} + \\hat{\\beta}_{age}\\times \\textrm{age} + \\hat{\\beta}_{smk}\\times \\textrm{smoke} \\pm 1.64\\hat{\\sigma}_{u0} \\big)\\), where \\(\\textrm{expit}(x) = 1/(1 + e^{-x})\\) One way to interpret the variation visually is to randomly generate many values of \\(p_{ij}( u_{i} )\\) using the estimated distribution of \\(u_{i}\\) to simulate the values of \\(u_{i}\\). This can help us to get a sense of how much variability there is in wheezing probability across individuals. To do this, I simulated values of \\(p_{ij}( u_{i} )\\) for each combination of age/smoking status and plotted the results in 8 densities in 4 panels. It would probably be better to use some sort of bounded density estimator for these plots. beta.hat &lt;- coef(summary(ohio.intercept))[,1] n &lt;- 1000 pneg2.smoke &lt;- plogis(rnorm(n,sd=2.34) + beta.hat[1] - 2*beta.hat[2] + beta.hat[3]) #age -2 with smoke pneg2 &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1] - 2*beta.hat[2]) #age -2 w/o smoke pneg1.smoke &lt;- plogis(rnorm(n,sd=2.34) + beta.hat[1] - 1*beta.hat[2] + beta.hat[3]) #age -1 with smoke pneg1 &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1] - 1*beta.hat[2]) # age -1 w/o smoke p0.smoke &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1] + beta.hat[3]) # age 0 with smoke p0 &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1]) # age 0 w/o smoke p1.smoke &lt;- plogis(rnorm(n,sd=2.34) + beta.hat[1] + beta.hat[2] + beta.hat[3]) # age 1 with smoke p1 &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1] + beta.hat[2]) # age 1 w/o smoke par(mfrow=c(2,2), mar=c(4.1, 4.1, .5, .5)) plot(density(pneg2), lwd=2, xlab = &quot;Probability of Wheezing at Age 7&quot;, main=&quot;&quot;, col=&quot;red&quot;) d &lt;- density(pneg2.smoke) lines(d$x, d$y, lwd=2) legend(&quot;topright&quot;, legend = c(&quot;Smoke&quot;, &quot;No Smoke&quot;), col=c(&quot;black&quot;, &quot;red&quot;), bty=&#39;n&#39;, lwd=2) plot(density(pneg1), lwd=2, xlab = &quot;Probability of Wheezing at Age 8&quot;, main=&quot;&quot;, col=&quot;red&quot;) d &lt;- density(pneg1.smoke) lines(d$x, d$y, lwd=2) legend(&quot;topright&quot;, legend = c(&quot;Smoke&quot;, &quot;No Smoke&quot;), col=c(&quot;black&quot;, &quot;red&quot;), bty=&#39;n&#39;, lwd=2) plot(density(p0), lwd=2, xlab = &quot;Probability of Wheezing at Age 9&quot;, main=&quot;&quot;, col=&quot;red&quot;) d &lt;- density(p0.smoke) lines(d$x, d$y, lwd=2) legend(&quot;topright&quot;, legend = c(&quot;Smoke&quot;, &quot;No Smoke&quot;), col=c(&quot;black&quot;, &quot;red&quot;), bty=&#39;n&#39;, lwd=2) plot(density(p1), lwd=2, xlab = &quot;Probability of Wheezing at Age 10&quot;, main=&quot;&quot;, col=&quot;red&quot;) d &lt;- density(p1.smoke) lines(d$x, d$y, lwd=2) legend(&quot;topright&quot;, legend = c(&quot;Smoke&quot;, &quot;No Smoke&quot;), col=c(&quot;black&quot;, &quot;red&quot;), bty=&#39;n&#39;, lwd=2) Figure 1.4: Distribution of Wheezing probability across individuals for different values of age and smoking status 1.6 Exercises For these exercises, you will use the actg_trial dataset. This dataset was obtained from https://content.sph.harvard.edu/fitzmaur/ala2e/cd4-data.txt. I also placed this dataset in a file named actg_trial.csv inside the Data folder on Canvas. When you load the dataset into R, it should look like the following head( actg_trial, 10) ## SubjectID Treatment Age Sex Week CD4 ## 1 1 2 36.4271 1 0.0000 3.135494 ## 2 1 2 36.4271 1 7.5714 3.044522 ## 3 1 2 36.4271 1 15.5714 2.772589 ## 4 1 2 36.4271 1 23.5714 2.833213 ## 5 1 2 36.4271 1 32.5714 3.218876 ## 6 1 2 36.4271 1 40.0000 3.044522 ## 7 2 4 47.8467 1 0.0000 3.068053 ## 8 2 4 47.8467 1 8.0000 3.891820 ## 9 2 4 47.8467 1 16.0000 3.970292 ## 10 2 4 47.8467 1 23.0000 3.610918 This longitudinal dataset has 5036 observations with the following 6 variables: SubjectID - subject identifier Treatment - treatment received (4 possible treatments) Age - age in years at baseline Sex - 1=M, 0=F Week - time in weeks from baseline CD4 - this is the natural logarithm of the CD4 count + 1 1.6.1 Questions How many individuals are in the trial? What are the smallest and largest values of \\(n_{i}\\) in this data set? Ignoring the longitudinal correlation, try fitting the linear model \\[\\begin{equation} Y_{ij} = \\beta_{1} + \\gamma_{1}t_{ij} + \\sum_{j=2}^{4}\\beta_{j}I(Trtment = j) + \\sum_{j=2}^{4}\\gamma_{j}t_{ij}I(Trtment = j) + e_{ij} \\tag{1.7} \\end{equation}\\] where \\(Y_{ij}\\) is the value of CD4 of subject \\(i\\) at time \\(t_{ij}\\) and where \\(t_{i1}, ..., t_{in_{i}}\\) are the values of the Week variable for person \\(i\\). For model (1.7), what is the interpretation of \\(\\beta_{1}\\), \\(\\beta_{2}\\), \\(\\beta_{3}\\), \\(\\beta_{4}\\)? What are the interpretations of \\(\\beta_{2} - \\beta_{1}\\), \\(\\beta_{3} - \\beta_{1}\\), and \\(\\beta_{4} - \\beta_{1}\\)? If this is a randomized trial, what do you expect the values of \\(\\beta_{2} - \\beta_{1}\\), \\(\\beta_{3} - \\beta_{1}\\), \\(\\beta_{4} - \\beta_{1}\\) will be? In model (1.7), what is the interpretation of \\(\\gamma_{1}\\)? In model (1.7), what term represents the change in CD4 from time 0 to 24 weeks for those assigned to treatment 2? What is the standard error for this term? Now account for longitudinal correlation by fitting a mixed model to the actg_trial dataset. Try fitting the following mixed model \\[\\begin{equation} Y_{ij} = u_{i} + \\beta_{1} + \\gamma_{1}t_{ij} + \\sum_{j=2}^{4}\\beta_{j}I(Trtment = j) + \\sum_{j=2}^{4}\\gamma_{j}t_{ij}I(Trtment = j) + e_{ij}, \\end{equation}\\] where \\(u_{i}\\) is a subject-specific random intercept term? For this model, what is the dimension of the “Z matrix” returned by lmer? How do the estimates \\(\\beta_{1}\\), \\(\\beta_{2}\\), \\(\\beta_{3}\\), \\(\\beta_{4}\\) compare with the model which did not have any random effects? Now, try fitting a mixed effects model with random intercepts and slopes using lmer: \\[\\begin{equation} Y_{ij} = u_{i0} + u_{i1}t_{ij} + \\beta_{1} + \\gamma_{1}t_{ij} + \\sum_{j=2}^{4}\\beta_{j}I(Trtment = j) + \\sum_{j=2}^{4}\\gamma_{j}t_{ij}I(Trtment = j) + e_{ij} \\end{equation}\\] Use the anova function to perform a likelihood ratio test of the random intercept vs. the random intercept + slope model. References "],["missing-data.html", "Chapter 2 Missing Data and Multiple Imputation 2.1 Missing Data in R and “Direct Approaches” for Handling Missing Data 2.2 Multiple Imputation 2.3 What is MICE doing? 2.4 Longitudinal Data 2.5 Different Missing Data Mechanisms", " Chapter 2 Missing Data and Multiple Imputation The book “Flexible Imputation of Missing Data” is a resource you also might find useful. It is available online at: https://stefvanbuuren.name/fimd/ 2.1 Missing Data in R and “Direct Approaches” for Handling Missing Data In many real-world datasets, it is very common to have missing values. In R, missing values are stored as NA, meaning “Not Available”. As an example, let’s look at the airquality dataframe available in base R data(airquality) head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 You can see that the 5th observation for the Ozone variable is missing, and the 5th and 6th observations for the Solar.R variable are missing. You can use is.na to find which data entries have missing values. If is.na returns TRUE, the entry is missing. If is.na returns FALSE, the entry is not missing head( is.na(airquality) ) ## Ozone Solar.R Wind Temp Month Day ## [1,] FALSE FALSE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE FALSE ## [3,] FALSE FALSE FALSE FALSE FALSE FALSE ## [4,] FALSE FALSE FALSE FALSE FALSE FALSE ## [5,] TRUE TRUE FALSE FALSE FALSE FALSE ## [6,] FALSE TRUE FALSE FALSE FALSE FALSE Computing the sum of is.na(airquality) tells us how many missing values there are in this dataset. sum( is.na(airquality) ) ## 44 missing entries in total ## [1] 44 dim( airquality ) ## Dataset has a total of 153 x 6 = 918 entries ## [1] 153 6 Doing apply(is.na(airquality), 2, sum) gives us the number of missing values for each variable. apply( is.na(airquality), 2, sum) ## Ozone Solar.R Wind Temp Month Day ## 37 7 0 0 0 0 2.1.1 Complete Case Analysis (Listwise Deletion) Suppose we wanted to run a regression with Ozone as the response and Solar.R, Wind, and Temp as the covariates. If we do this in R, we will get the following result: air.lm1 &lt;- lm( Ozone ~ Solar.R + Wind + Temp, data = airquality ) summary( air.lm1 ) ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.485 -14.219 -3.551 10.097 95.619 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -64.34208 23.05472 -2.791 0.00623 ** ## Solar.R 0.05982 0.02319 2.580 0.01124 * ## Wind -3.33359 0.65441 -5.094 1.52e-06 *** ## Temp 1.65209 0.25353 6.516 2.42e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21.18 on 107 degrees of freedom ## (42 observations deleted due to missingness) ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.5948 ## F-statistic: 54.83 on 3 and 107 DF, p-value: &lt; 2.2e-16 How is R handling all the missing values in airquality? As a default, lm performs a complete case analysis (sometimes referred to as listwise deletion). (This is the default unless you changed the default na.action setting with options(...)) A complete case analysis for regression will delete a rows from the dataset if any of the variables used from that row have a missing value. In this example, a row will be dropped if either the value of Ozone, Solar.R, Wind, or Temp is missing. After these observations have been deleted, the usual regression parameters are estimated from the remaining “complete” dataset. To remove all rows where there is at least one missing value, you can use na.omit: complete.air &lt;- na.omit( airquality ) ## Check that there are no missing values sum( is.na(complete.air) ) # This should be zero ## [1] 0 Let’s now fit a linear regression using the complete dataset complete.air air.lm2 &lt;- lm( Ozone ~ Solar.R + Wind + Temp, data = complete.air ) Because R does a complete-case analysis as default, the estimated regression coefficients should be the same when using the “incomplete dataset” airquality as when using the “complete dataset” complete.air ## The estimated regression coefficients should be the same round(air.lm1$coefficients, 3) ## (Intercept) Solar.R Wind Temp ## -64.342 0.060 -3.334 1.652 round(air.lm2$coefficients, 3) ## (Intercept) Solar.R Wind Temp ## -64.342 0.060 -3.334 1.652 2.1.2 Other “Direct” Methods In general, doing a complete-case analysis is not advisable. A complete-case analysis should really only be used if you are confident that the data are missing completely at random (MCAR). Roughly speaking, MCAR means that the probability of having a missing value is not related to missing or observed values of the data. Section 2.5 gives a more formal definition of MCAR. A few other direct ways of handling missing data that you may have used or seen used in practice include: Mean imputation. Missing values are replaced by the value of the mean of that variable. Regression imputation. Missing values are replaced by a regression prediction from the values of the other variables. Unless the data are missing completely at random (MCAR), each of these methods will produce biased estimates of the parameters of interest and generate incorrect standard errors. 2.2 Multiple Imputation 2.2.1 Short Overview of Multiple Imputation To summarize, multiple imputation consists of the following steps: Create \\(K\\) different “complete datasets” where each dataset contains no missing data. For each of these \\(K\\) complete datasets, compute the estimates of interest. “Pool” these separate estimates to get final estimates and standard errors. The nice thing about multiple imputation is that you can always use your original analysis approach. That is, you can just apply your original analysis method to each of the \\(K\\) complete datasets. The complicated part in multiple imputation is generating the \\(K\\) complete datasets in a “valid” or “statistically principled” way. Fortunately, there are a number of R packages that implement different approaches for creating the \\(K\\) imputed datasets. 2.2.2 Multiple imputation with mice I will primarily focus on the mice package. mice stands for “Multivariate Imputation by Chained Equations” The mice function within the mice package is the primary function for performing multiple imputation. To use mice, just use mice(df) where df is the name of the dataframe. (Set print = FALSE if you don’t want it to print out the number of the iteration). Choose a value of seed so that the results are reproducible. Note that the default number of complete datasets returned is 5. This can be changed with the m argument. A value of m set to 5 or 10 is a typically recommendation for something that works well in practice. Let’s try running the mice function with the airquality dataset. library(mice) imputed.airs &lt;- mice(airquality, print=FALSE, seed=101) The object returned by mice will have a component called imp which is a list. Each component of imp is a dataframe corresponding to a single variable in the original dataframe This dataframe will contain the imputed values for the missing values of that variable. For example, imputed.airs$imp will be a list with each component of the list being one of the variables from airquality names( imputed.airs$imp ) ## [1] &quot;Ozone&quot; &quot;Solar.R&quot; &quot;Wind&quot; &quot;Temp&quot; &quot;Month&quot; &quot;Day&quot; The Ozone component of imputed.airs$imp will be a dataframe with 37 rows and 5 columns. This is because the Ozone variable had 37 missing values, and there are 5 multiple imputations (the default number in mice) dim(imputed.airs$imp$Ozone) ## [1] 37 5 ## Imputed missing ozone values across the five multiple imputations head(imputed.airs$imp$Ozone) ## 1 2 3 4 5 ## 5 6 8 18 6 37 ## 10 12 18 27 18 30 ## 25 8 14 6 18 18 ## 26 13 1 13 37 13 ## 27 19 18 4 18 34 ## 32 40 47 45 23 18 The row names in imputed.airs$imp$Ozone correspond to the index of the observation in the original airquality dataframe. For example, the 5th observation of the Ozone variable has 6 in the 1st imputation, 8 in the 2nd imputation, 18 in the 3rd imputation, etc. …. Similarly, the Solar.R component of imputed.airs$imp will be a data frame This is because the Solar.R variable had 7 missing values, and there are 5 multiple imputations. dim(imputed.airs$imp$Solar.R) ## [1] 7 5 ## Imputed missing ozone values across the five multiple imputations head(imputed.airs$imp$Solar.R) ## 1 2 3 4 5 ## 5 131 285 274 92 139 ## 6 127 248 175 167 175 ## 11 71 71 238 115 284 ## 27 238 8 49 223 238 ## 96 258 203 229 223 291 ## 97 313 259 274 83 272 For example, the 5th observation of the Solar.R variable has 131 in the 1st imputation, 285 in the 2nd imputation, 274 in the 3rd imputation, etc. …. 2.2.2.1 with(), pool(), complete() You could use the components of imputed.airs$imp to directly fit 5 separate regression on the multiply imputed datasets and then average the results. However, this is much easier if you just use the with function from mice air.multi.imputelm &lt;- with(imputed.airs, lm( Ozone ~ Solar.R + Wind + Temp)) This will produce 5 different sets of estimates of the regression coefficients: summary(air.multi.imputelm) ## # A tibble: 20 × 6 ## term estimate std.error statistic p.value nobs ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 (Intercept) -90.3 19.1 -4.72 5.44e- 6 153 ## 2 Solar.R 0.0642 0.0199 3.22 1.56e- 3 153 ## 3 Wind -2.31 0.550 -4.20 4.61e- 5 153 ## 4 Temp 1.83 0.212 8.64 8.13e-15 153 ## 5 (Intercept) -49.4 19.0 -2.60 1.02e- 2 153 ## 6 Solar.R 0.0548 0.0196 2.79 5.92e- 3 153 ## 7 Wind -3.43 0.545 -6.28 3.44e- 9 153 ## 8 Temp 1.46 0.211 6.92 1.27e-10 153 ## 9 (Intercept) -56.7 19.0 -2.98 3.36e- 3 153 ## 10 Solar.R 0.0646 0.0199 3.25 1.42e- 3 153 ## 11 Wind -3.22 0.546 -5.90 2.38e- 8 153 ## 12 Temp 1.50 0.211 7.09 4.97e-11 153 ## 13 (Intercept) -58.4 18.9 -3.09 2.41e- 3 153 ## 14 Solar.R 0.0687 0.0199 3.46 7.08e- 4 153 ## 15 Wind -3.27 0.544 -6.01 1.35e- 8 153 ## 16 Temp 1.57 0.210 7.50 5.33e-12 153 ## 17 (Intercept) -58.9 22.2 -2.66 8.69e- 3 153 ## 18 Solar.R 0.0404 0.0231 1.75 8.27e- 2 153 ## 19 Wind -3.31 0.636 -5.21 6.11e- 7 153 ## 20 Temp 1.64 0.245 6.68 4.37e-10 153 To get the “pooled estimates and standard errors” from these 5 different sets of regression coefficients use the pool function from mice: summary( pool(air.multi.imputelm) ) ## term estimate std.error statistic df p.value ## 1 (Intercept) -62.73141323 26.25695768 -2.389135 16.63162 2.903387e-02 ## 2 Solar.R 0.05855766 0.02400718 2.439173 36.55628 1.969933e-02 ## 3 Wind -3.10764673 0.75290758 -4.127527 16.76039 7.227036e-04 ## 4 Temp 1.60026617 0.27155700 5.892929 23.89788 4.511087e-06 The pooled “final estimates” of the regression coefficients are just the means of the estimated regression coefficients from the 5 multiply imputed datasets. The pooled standard error for the \\(j^{th}\\) regression coefficient is given by \\[\\begin{equation} (\\textrm{pooled } SE_{j})^{2} = \\frac{1}{K}\\sum_{k=1}^{K} SE_{jk}^{2} + \\frac{K+1}{K(K-1)}\\sum_{k=1}^{K}(\\hat{\\beta}_{jk} - \\bar{\\hat{\\beta}}_{j.})^{2}, \\end{equation}\\] where \\(SE_{jk}\\) is the standard error for the \\(j^{th}\\) regression coefficient from the \\(k^{th}\\) complete dataset, and \\(\\hat{\\beta}_{jk}\\) is the estimate of \\(\\beta_{j}\\) from the \\(k^{th}\\) complete dataset. It is sometimes useful to actually extract each of the completed datasets. This is true, for example, in longitudinal data where you may want to go back and forth between “wide” and “long” formats. To extract each of the completed datasets, you can use the complete function from mice. The following code will return the 5 complete datasets from imputed.airs completed.airs &lt;- mice::complete(imputed.airs, action=&quot;long&quot;) action = \"long\" means that it will return the 5 complete datasets as one dataframe with the individual datasets “stacked on top of each other.” completed.airs will be a dataframe that has 5 times as many rows as the airquality data frame The variable .imp is an indicator of which of the 5 imputations that row corresponds to. head(completed.airs) ## .imp .id Ozone Solar.R Wind Temp Month Day ## 1 1 1 41 190 7.4 67 5 1 ## 2 1 2 36 118 8.0 72 5 2 ## 3 1 3 12 149 12.6 74 5 3 ## 4 1 4 18 313 11.5 62 5 4 ## 5 1 5 6 131 14.3 56 5 5 ## 6 1 6 28 127 14.9 66 5 6 dim(completed.airs) ## [1] 765 8 dim(airquality) ## [1] 153 6 Using complete.airs, we can compute the multiple imputation-based estimates of the regression coefficients “by hand” This should give us the same results as when using with BetaMat &lt;- matrix(NA, nrow=5, ncol=4) for(k in 1:5) { ## Find beta.hat from kth imputed dataset BetaMat[k,] &lt;- lm(Ozone ~ Solar.R + Wind + Temp, data=completed.airs[completed.airs$.imp==k,])$coefficients } round(colMeans(BetaMat), 3) # compare with the results from using the &quot;with&quot; function ## [1] -62.731 0.059 -3.108 1.600 2.2.3 Categorical Variables in MICE You can impute values of missing categorical variables directly with the mice function The only thing to remember is that any categorical variable should be stored in your data frame as a factor. As an example, let’s define the data frame testdf as testdf &lt;- data.frame(wt=c(103.2, 57.6, 33.4, 87.2, NA, NA, 98.5, 77.3), age=c(NA, &quot;old&quot;, &quot;middle_age&quot;, &quot;young&quot;, NA, &quot;old&quot;, &quot;young&quot;, &quot;middle_age&quot;)) This data frame does not store age as a factor str(testdf) ## &#39;data.frame&#39;: 8 obs. of 2 variables: ## $ wt : num 103.2 57.6 33.4 87.2 NA ... ## $ age: chr NA &quot;old&quot; &quot;middle_age&quot; &quot;young&quot; ... mice will not run if we try to use testdf as the input data frame to mice However, it will work if we just change the variable age to a factor. So, if we define the data frame testdf_fac as testdf_fac &lt;- testdf testdf_fac$age &lt;- as.factor(testdf_fac$age) str(testdf_fac) ## &#39;data.frame&#39;: 8 obs. of 2 variables: ## $ wt : num 103.2 57.6 33.4 87.2 NA ... ## $ age: Factor w/ 3 levels &quot;middle_age&quot;,&quot;old&quot;,..: NA 2 1 3 NA 2 3 1 Then, we should be able to use mice with testdf_fac imptest &lt;- mice(testdf_fac) ## ## iter imp variable ## 1 1 wt age ## 1 2 wt age ## 1 3 wt age ## 1 4 wt age ## 1 5 wt age ## 2 1 wt age ## 2 2 wt age ## 2 3 wt age ## 2 4 wt age ## 2 5 wt age ## 3 1 wt age ## 3 2 wt age ## 3 3 wt age ## 3 4 wt age ## 3 5 wt age ## 4 1 wt age ## 4 2 wt age ## 4 3 wt age ## 4 4 wt age ## 4 5 wt age ## 5 1 wt age ## 5 2 wt age ## 5 3 wt age ## 5 4 wt age ## 5 5 wt age Look at imputed values of age: imptest$imp$age ## 1 2 3 4 5 ## 1 young young old young young ## 5 middle_age middle_age middle_age young old Also, remember that if you are reading a .csv file into R, including the argument stringsAsFactors=TRUE in read.csv will automatically make all the string variables in the .csv file factors in the data frame that is read into R. 2.3 What is MICE doing? Suppose we have data from \\(q\\) variables \\(Z_{i1}, \\ldots, Z_{iq}\\). Let \\(\\mathbf{Z}_{mis}\\) denote the entire collection of missing observations and \\(\\mathbf{Z}_{obs}\\) the entire collection of observed values, and let \\(\\mathbf{Z} = (\\mathbf{Z}_{obs}, \\mathbf{Z}_{mis})\\). Let \\(\\mathbf{R}\\) be an indicator of missingness. That is, \\(R_{ij} = 1\\) if \\(Z_{ij}\\) was missing and \\(R_{ij} = 0\\) if \\(Z_{ij}\\) was observed. The basic idea behind multiple imputation is to, in some way, generate samples \\(\\mathbf{Z}_{mis}^{(1)}, \\ldots, \\mathbf{Z}_{mis}^{(K)}\\) from a flexible probability model \\(p(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs})\\) \\(p(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs})\\) represents the conditional distribution of \\(\\mathbf{Z}_{mis}\\) given the observed \\(\\mathbf{Z}_{obs}\\). The missing at random (MAR) assumption implies we only need to work with the conditional distribution \\(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs}\\). The MAR assumption implies the distribution of \\(\\mathbf{Z}_{mis}\\) given \\(\\mathbf{Z}_{obs}, \\mathbf{R}\\) is the same as the distribution of \\(\\mathbf{Z}_{mis}\\) given \\(\\mathbf{Z}_{obs}\\). A valid estimate \\(\\hat{\\theta}\\) of the parameter of interest \\(\\theta\\) can often be thought of as a posterior mean: \\(\\hat{\\theta} = E(\\theta|\\mathbf{Z}_{obs}, \\mathbf{R})\\) is the expectation of \\(\\theta\\) given the observed data values \\(\\mathbf{Z}_{obs}\\) and missingness indicators. Then, \\(\\hat{\\theta}\\) can be expressed as: \\[\\begin{eqnarray} \\hat{\\theta} &amp;=&amp; E( \\theta |\\mathbf{Z}_{obs}, \\mathbf{R} ) = \\int E\\Big\\{ \\theta \\Big| \\mathbf{Z}_{obs}, \\mathbf{R} \\mathbf{Z}_{mis} \\Big\\} p(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs}, \\mathbf{R}) d\\mathbf{Z}_{mis} \\nonumber \\\\ &amp;=&amp; \\int E\\Big\\{ \\theta \\Big| \\mathbf{Z}_{obs}, \\mathbf{R}, \\mathbf{Z}_{mis} \\Big\\} p(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs} ) d\\mathbf{Z}_{mis} \\nonumber \\\\ &amp;\\approx&amp; \\frac{1}{K} \\sum_{k=1}^{K} E\\Big\\{ \\theta \\Big| \\mathbf{Z}_{obs}, \\mathbf{R}, \\mathbf{Z}_{mis}^{(k)} \\Big\\} \\end{eqnarray}\\] For a more non-Bayesian interpretation, you can often think of an estimate \\(\\hat{\\theta}_{full}\\) with no missing data (such as maximum likelihood) as the solution of an estimating equation \\[\\begin{equation} U(\\theta; \\mathbf{Z}_{mis}, \\mathbf{Z}_{obs}) = 0 \\end{equation}\\] With multiple imputation, \\(\\hat{\\theta}\\) is approximately finding a solution of the following estimating equation \\[\\begin{equation} \\frac{1}{K} \\sum_{k=1}^{K} U(\\theta; \\mathbf{Z}_{mis}^{(k)}, \\mathbf{Z}_{obs}) \\approx E\\Big\\{ U(\\theta; \\mathbf{Z}_{mis}, \\mathbf{Z}_{obs}) | \\mathbf{Z}_{obs}, \\mathbf{R} \\Big\\} = 0 \\end{equation}\\] There are two main approaches for setting up a model for the conditional distribution of \\(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs}\\). One approach is to directly specify a full joint model for \\(\\mathbf{Z} = (\\mathbf{Z}_{mis}, \\mathbf{Z}_{obs})\\) For example, you could assume that \\(\\mathbf{Z}\\) follows a multivariate a normal distribution. This approach is not so straightforward when you have variables of mixed type: some continuous,some binary, some categorical, etc. … Let the vector \\(\\mathbf{Z}_{j} = (Z_{1j}, \\ldots, Z_{nj})\\) denote all the “data” (whether observed or unobserved) from variable \\(j\\). The fully conditional specification (FCS) approach specifies the distribution of each variable \\(\\mathbf{Z}_{j}\\) conditional on the remaining variables \\(\\mathbf{Z}_{-j}\\). The FCS approach is the one used by mice. With the FCS approach, we assume models for \\(q\\) different conditional distributions \\[\\begin{eqnarray} p(\\mathbf{Z}_{1}&amp;|&amp;\\mathbf{Z}_{-1}, \\boldsymbol{\\eta}_{1}) \\nonumber \\\\ p(\\mathbf{Z}_{2}&amp;|&amp;\\mathbf{Z}_{-2}, \\boldsymbol{\\eta}_{2}) \\nonumber \\\\ &amp;\\vdots&amp; \\nonumber \\\\ p(\\mathbf{Z}_{q}&amp;|&amp;\\mathbf{Z}_{-q}, \\boldsymbol{\\eta}_{q}) \\end{eqnarray}\\] With mice, the parameters \\(\\eta_{j}\\) and the missing values for each variable \\(\\mathbf{Z}_{j,mis}\\) are updated one-at-a-time via a kind of Gibbs sampler. All of the missing values can be imputed in one cycle of the Gibbs sampler. Multiple cycles are repeated to get multiple completed datasets. The default model for a continuous variable \\(\\mathbf{Z}_{j}\\) is to use predictive mean matching. The default model for a binary variable \\(\\mathbf{Z}_{j}\\) is logistic regression. Look at the defaultMethod argument of mice and Buuren and Groothuis-Oudshoorn (2010) for more details about how to change these default models. 2.4 Longitudinal Data A direct way to do multiple imputation with longitudinal data is to use mice on the dataset stored in wide format. Remember that in wide format, each row corresponds to a different individual. Applying multiple imputation to the wide-format dataset can account for the fact that observations across individuals will be correlated. Let’s look at the ohio data from the geepack package again library(geepack) data(ohio) head(ohio) ## resp id age smoke ## 1 0 0 -2 0 ## 2 0 0 -1 0 ## 3 0 0 0 0 ## 4 0 0 1 0 ## 5 0 1 -2 0 ## 6 0 1 -1 0 The ohio dataset is in long format. We need to first convert this into wide format. With the tidyr package, you can convert from long to wide using spread: (Use gather to go from wide to long) library( tidyr ) ohio.wide &lt;- spread(ohio, key=age, value=resp) ## Change variable names to so that ages go from 7 to 10 names(ohio.wide) &lt;- c(&quot;id&quot;, &quot;smoke&quot;, &quot;age7&quot;, &quot;age8&quot;, &quot;age9&quot;, &quot;age10&quot;) head(ohio.wide) ## id smoke age7 age8 age9 age10 ## 1 0 0 0 0 0 0 ## 2 1 0 0 0 0 0 ## 3 2 0 0 0 0 0 ## 4 3 0 0 0 0 0 ## 5 4 0 0 0 0 0 ## 6 5 0 0 0 0 0 The variable age7 now represents the value of resp at age 7, age8 represents the value of resp at age 8, etc… reshape from base R can also be used to go from long to wide # Example of using reshape #ohio.wide2 &lt;- reshape(ohio, v.names=&quot;resp&quot;, idvar=&quot;id&quot;, timevar=&quot;age&quot;, direction=&quot;wide&quot;) The ohio dataset does not have any missing values. Let’s introduce missing values for the variable resp values by assuming that the probability of being missing is positively related to smoking status. Let \\(R_{ij}\\) be an indicator of missingness of resp for individual \\(i\\) at the \\(j^{th}\\) follow-up time. When randomly generating missing values, we will assume that: \\[\\begin{equation} P( R_{ij} = 1| \\textrm{smoke}_{i}) = \\begin{cases} 0.05 &amp; \\textrm{ if } \\textrm{smoke}_{i} = 0 \\\\ 0.3 &amp; \\textrm{ if } \\textrm{smoke}_{i} = 1 \\end{cases} \\tag{2.1} \\end{equation}\\] To generate missing values according to assumption (2.1), we can use the following R code: We will call the new data frame ohio.wide.miss ohio.wide.miss &lt;- ohio.wide m &lt;- nrow(ohio.wide.miss) ## number of individuals in study for(k in 1:m) { resp.values &lt;- ohio.wide[k, 3:6] # values of resp for individual k if(ohio.wide[k,2] == 1) { # if smoke = 1 Rij &lt;- sample(0:1, size=4, replace=TRUE, prob=c(0.7, 0.3)) } else { # if smoke = 0 Rij &lt;- sample(0:1, size=4, replace=TRUE, prob=c(0.95, 0.05)) } resp.values[Rij==1] &lt;- NA # insert NA values where Rij = 1 ohio.wide.miss[k, 3:6] &lt;- resp.values } head(ohio.wide.miss, 10) ## id smoke age7 age8 age9 age10 ## 1 0 0 0 0 NA 0 ## 2 1 0 0 0 0 0 ## 3 2 0 0 0 0 0 ## 4 3 0 0 0 0 0 ## 5 4 0 0 0 0 0 ## 6 5 0 0 0 0 0 ## 7 6 0 0 0 0 0 ## 8 7 0 0 NA 0 0 ## 9 8 0 0 0 0 0 ## 10 9 0 0 0 0 0 ohio.wide.miss now has 257 missing entries sum( is.na(ohio.wide.miss)) ## [1] 296 Before using multiple imputation with ohio.wide.miss, let’s look at the regression coefficient estimates that would be obtained with a complete case analysis. To use glmer on the missing-data version of ohio, we need to first convert ohio.wide.miss back into long form: ohio.miss &lt;- gather(ohio.wide.miss, age, resp, age7:age10) ohio.miss$age[ohio.miss$age == &quot;age7&quot;] &lt;- -2 ohio.miss$age[ohio.miss$age == &quot;age8&quot;] &lt;- -1 ohio.miss$age[ohio.miss$age == &quot;age9&quot;] &lt;- 0 ohio.miss$age[ohio.miss$age == &quot;age10&quot;] &lt;- 1 ohio.miss &lt;- ohio.miss[order(ohio.miss$id),] ## sort everything according to id ohio.miss$age &lt;- as.numeric(ohio.miss$age) head(ohio.miss) ## id smoke age resp ## 1 0 0 -2 0 ## 538 0 0 -1 0 ## 1075 0 0 0 NA ## 1612 0 0 1 0 ## 2 1 0 -2 0 ## 539 1 0 -1 0 Let’s use a random intercept model as we did in our earlier discussion of generalized linear mixed models: ## Complete case analysis library(lme4) ohio.cca &lt;- glmer(resp ~ age + smoke + (1 | id), data = ohio.miss, family = binomial) # Now look at estimated regression coefficients for complete case analysis: round(coef(summary(ohio.cca)), 4) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.8009 0.4346 -8.7459 0.0000 ## age -0.1580 0.0783 -2.0165 0.0437 ## smoke 0.2333 0.3402 0.6857 0.4929 Now, let’s use mice to create 10 “completed versions” of ohio.wide.miss imputed.ohio &lt;- mice(ohio.wide.miss, m=10, print=FALSE, seed=101) For the case of longitudinal data, we probably want to actually extract each complete dataset. (This is because many of the analysis methods such as lmer assume the data is in long form). This can be done with the following code: completed.ohio &lt;- mice::complete(imputed.ohio, &quot;long&quot;) head(completed.ohio) ## .imp .id id smoke age7 age8 age9 age10 ## 1 1 1 0 0 0 0 0 0 ## 2 1 2 1 0 0 0 0 0 ## 3 1 3 2 0 0 0 0 0 ## 4 1 4 3 0 0 0 0 0 ## 5 1 5 4 0 0 0 0 0 ## 6 1 6 5 0 0 0 0 0 completed.ohio will be a dataframe that has 10 times as many rows as the original ohio.wide data frame dim(ohio.wide) ## [1] 537 6 dim(completed.ohio) ## [1] 5370 8 The variable .imp in completed.ohio is an indicator of which of the 10 “imputed datasets” this is from: table( completed.ohio$.imp ) # Tabulate impute indicators ## ## 1 2 3 4 5 6 7 8 9 10 ## 537 537 537 537 537 537 537 537 537 537 For each of the 10 complete datasets, we need to convert the wide dataset into long form before using glmer: ## Multiple imputation-based estimates of regression coefficients ## for the missing version of the ohio data. BetaMat &lt;- matrix(NA, nrow=10, ncol=3) for(k in 1:10) { tmp.ohio &lt;- completed.ohio[completed.ohio$.imp==k,-c(1,2)] tmp.ohio.long &lt;- gather(tmp.ohio, age, resp, age7:age10) tmp.ohio.long$age[tmp.ohio.long$age == &quot;age7&quot;] &lt;- -2 tmp.ohio.long$age[tmp.ohio.long$age == &quot;age8&quot;] &lt;- -1 tmp.ohio.long$age[tmp.ohio.long$age == &quot;age9&quot;] &lt;- 0 tmp.ohio.long$age[tmp.ohio.long$age == &quot;age10&quot;] &lt;- 1 tmp.ohio.long$age &lt;- as.numeric(tmp.ohio.long$age) ohio.tmpfit &lt;- glmer(resp ~ age + smoke + (1 | id), data = tmp.ohio.long, family = binomial) BetaMat[k,] &lt;- coef(summary(ohio.tmpfit))[,1] } The multiple imputation-based estimates of the regression coefficients for the missing version of ohio are: round(colMeans(BetaMat), 4) ## [1] -3.5711 -0.1066 0.2865 Compare the above regression coefficients with those from the complete-case analysis. 2.5 Different Missing Data Mechanisms For this section, we will consider the setup where we have \\(n\\) “observations” and \\(q\\) “variables”: denoted by \\(Z_{i1}, \\ldots, Z_{iq}\\), for \\(i = 1, \\ldots, n\\). Let \\(\\mathbf{Z}_{mis}\\) denote the collection of missing observations and \\(\\mathbf{Z}_{obs}\\) the collection of observed values, and let \\(\\mathbf{Z} = (\\mathbf{Z}_{obs}, \\mathbf{Z}_{mis})\\). The variables \\(R_{ij}\\) are defined as \\[\\begin{equation} R_{ij} = \\begin{cases} 1 &amp; \\textrm{ if } Z_{ij} \\textrm{ is missing } \\\\ 0 &amp; \\textrm{ if } Z_{ij} \\textrm{ is observed } \\end{cases} \\end{equation}\\] 2.5.1 Missing Completely at Random (MCAR) The missingness mechanism is said to be MCAR if \\[\\begin{equation} P(R_{ij} = 1|\\mathbf{Z}_{obs}, \\mathbf{Z}_{mis}) = P(R_{ij}=1) \\end{equation}\\] 2.5.2 Missing at Random (MAR) The missingness mechanism is said to be MAR if: \\[\\begin{equation} P(R_{ij} = 1|\\mathbf{Z}_{obs}, \\mathbf{Z}_{mis}) = P(R_{ij}=1|\\mathbf{Z}_{obs}) \\end{equation}\\] If missingness is follows either MAR or MCAR, direct use of multiple imputation is a valid approach. 2.5.3 Missing not at Random (MNAR) If the missingness mechanism is classified as missing not at random (MNAR), the probability \\(P(R_{ij} = 1|\\mathbf{Z}_{obs}, \\mathbf{Z}_{mis})\\) cannot be factorized into a simpler form. If the missingness is MNAR, direct use of multiple imputation may be invalid and modeling of the missingness mechanism using subject-matter knowledge may be needed. Use of multiple imputation with a sensitivity analysis is one approach to consider. References "],["nonpar-regression.html", "Chapter 3 Nonparametric Regression with Longitudinal Data 3.1 Notation 3.2 Kernel Smoothing 3.3 Regression Splines", " Chapter 3 Nonparametric Regression with Longitudinal Data 3.1 Notation For longitudinal data, we will again use the following notation: Individual \\(i\\) has observations for both the outcome and the covariates at times \\(t_{i1}, \\ldots, t_{in_{i}}\\) \\(Y_{ij}\\) is the outcome for individual \\(i\\) at time \\(t_{ij}\\). \\(\\mathbf{x}_{ij}\\) is the vector of covariates at time \\(t_{ij}\\). The \\(i^{th}\\) individual has \\(n_{i}\\) observations: \\(Y_{i1}, \\ldots, Y_{in_{i}}\\). There will be \\(m\\) individuals in the study (so \\(1 \\leq i \\leq m\\)). A general regression model relating \\(Y_{ij}\\) and \\(\\mathbf{x}_{ij}\\) is the following: \\[\\begin{equation} Y_{ij} = \\mu( \\mathbf{x}_{ij} ) + \\varepsilon_{ij} \\nonumber \\end{equation}\\] Here, \\(\\mu(\\mathbf{x}_{ij}) = E(Y_{ij}| \\mathbf{x}_{ij})\\) is the “mean function”. In nonparametric approaches to estimating \\(\\mu(\\cdot)\\), we will try to estimate \\(\\mu(\\mathbf{x})\\) without making any strong assumptions about the form of \\(\\mu( \\mathbf{x} )\\). Basically, in a nonparametric approach, there is not a fixed set of parameters describing the mean function that does not change as the sample size grows. 3.2 Kernel Smoothing 3.2.1 Description of Kernel Regression With kernel regression, we estimate the mean function \\(\\mu(\\mathbf{x})\\) at \\(\\mathbf{x}\\) by taking a weighted “local average” of the \\(Y_{ij}\\) around \\(\\mathbf{x}\\). Specifically, the kernel regression estimate of \\(\\mu(\\cdot)\\) at a point \\(\\mathbf{x}\\) can be expressed as \\[\\begin{equation} \\hat{\\mu}( \\mathbf{x} ) = \\sum_{i=1}^{m}\\sum_{j=1}^{n_{i}} w_{ij}(\\mathbf{x})Y_{ij} \\end{equation}\\] The “weights” at the point \\(\\mathbf{x}\\) are given by \\[\\begin{equation} w_{ij}(\\mathbf{x}) = \\frac{ K\\Big( \\frac{\\mathbf{x} - \\mathbf{x}_{ij}}{ h_{n} }\\Big) }{ \\sum_{i=1}^{m}\\sum_{j=1}^{n_{i}} K\\Big( \\frac{\\mathbf{x} - \\mathbf{x}_{ij}}{ h_{n} }\\Big) } \\tag{3.1} \\end{equation}\\] When using the weights (3.1), \\(\\hat{\\mu}(\\mathbf{x})\\) is known as the Nadaraya-Watson esitmator. The function \\(K(\\cdot)\\) in (3.1) is referred to as the “kernel function”. The kernel function \\(K(\\cdot)\\) is: A smooth nonnegative function Symmetric around \\(0\\) Has a mode at \\(0\\) and decays the further you go away from \\(0\\) A common choice of \\(K(\\cdot)\\) is the Gaussian kernel \\[\\begin{equation} K(\\mathbf{u}) = \\exp\\Big\\{ - \\frac{||\\mathbf{u}||^{2}}{2} \\Big\\} \\end{equation}\\] Observations where \\(\\mathbf{x}_{ij}\\) is “close” to \\(\\mathbf{x}\\) will be given a larger weight \\(w_{ij}(\\mathbf{x})\\) because \\(||\\mathbf{x} - \\mathbf{x}_{ij}||^{2}\\) will be small. Similarly, observations where \\(\\mathbf{x}_{ij}\\) is “far away” from \\(\\mathbf{x}\\) will be given a smaller weight \\(w_{ij}(\\mathbf{x})\\) because \\(||\\mathbf{x} - \\mathbf{x}_{ij}||^{2}\\) will be small. The term \\(h_{n} &gt; 0\\) is referred to as the bandwidth. The bandwidth determines how many observations have a strong impact on the value of \\(\\hat{\\mu}( \\mathbf{x} )\\). If the bandwidth \\(h_{n}\\) is small, observations close to \\(\\mathbf{x}\\) will largely determine the value of \\(\\hat{\\mu}(\\mathbf{x})\\). If the bandwidth \\(h_{n}\\) is large, the value of \\(\\hat{\\mu}(\\mathbf{x})\\) will be more heavily influenced by a larger number of observations. Kernel regression estimates with a smaller bandwidth will be more “wiggly” and non-smooth. Kernel regression estimates with a larger bandwidth will be more smooth. 3.2.2 Kernel Regression in the sleepstudy data Again, let’s look at the sleepstudy data from the lme4 package. The sleepstudy data had 18 participants with reaction time measured across 10 days. library(lme4) ## Loading required package: Matrix data(sleepstudy) head(sleepstudy) ## Reaction Days Subject ## 1 249.5600 0 308 ## 2 258.7047 1 308 ## 3 250.8006 2 308 ## 4 321.4398 3 308 ## 5 356.8519 4 308 ## 6 414.6901 5 308 We can estimate the marginal mean function for the sleepstudy data by using a GEE. We will assume that reaction time is a linear function of time on study: That is, we will assume that \\(\\mu(t) = \\beta_{0} + \\beta_{1} t\\). library(geepack) ## Use AR(1) correlation structure sleep.gee &lt;- geeglm(Reaction ~ Days, data=sleepstudy, id=Subject, corstr=&quot;ar1&quot;) To get the value of the estimated regression function, we can use the first \\(10\\) fitted values (because the fitted values for each subject are the same as the overall mean function) ## Estimated mean function at each time point gee.regfn &lt;- sleep.gee$fitted.values[1:10,1] ### Now plot the estimated mean function plot(sleepstudy$Days, sleepstudy$Reaction, las=1, ylab=&quot;Reaction Time&quot;, xlab=&quot;Days&quot;, main=&quot;Sleepstudy: GEE estimate of Mean Function&quot;, type=&quot;n&quot;) points(sleepstudy$Days, sleepstudy$Reaction, pch=16, cex=0.8) lines(0:9, gee.regfn, lwd=2, col=&quot;red&quot;) To find a kernel regression estimate of the mean function, you can use the ksmooth function in R. One thing to note is that ksmooth only works for a scalar covariate. Using a bandwidth of \\(0.5\\) and a Gaussian kernel, we can find the kernel regression estimate of the mean function with the following R code: sleep.kernel &lt;- ksmooth(sleepstudy$Days, sleepstudy$Reaction, kernel=&quot;normal&quot;, bandwidth = 0.5) This will return a list with an “x vector” and a “y vector”. The x vector will be the vector of points at which the regression function is estimated. The y vector will be a vector containing the estimated values of the regression function. Let’s plot the estimated mean function to see what it looks like: plot(sleepstudy$Days, sleepstudy$Reaction, las=1, ylab=&quot;Reaction Time&quot;, xlab=&quot;Days&quot;, main=&quot;Sleepstudy: Kernel Regression with Bandwidth = 0.5&quot;, type=&quot;n&quot;) points(sleepstudy$Days, sleepstudy$Reaction, pch=16, cex=0.8) lines(sleep.kernel$x, sleep.kernel$y, lwd=2, col=&quot;red&quot;) This bandwidth looks too small. There are clear “near jumps” in between some of the days. We can try a bandwidth of \\(1\\) to see if we can smooth this out a bit. sleep.kernel.bw1 &lt;- ksmooth(sleepstudy$Days, sleepstudy$Reaction, kernel=&quot;normal&quot;, bandwidth = 1) plot(sleepstudy$Days, sleepstudy$Reaction, las=1, ylab=&quot;Reaction Time&quot;, xlab=&quot;Days&quot;, main=&quot;Sleepstudy: Kernel Regression with Bandwidth = 1&quot;, type=&quot;n&quot;) points(sleepstudy$Days, sleepstudy$Reaction, pch=16, cex=0.8) lines(sleep.kernel.bw1$x, sleep.kernel.bw1$y, lwd=2, col=&quot;red&quot;) 3.2.3 Bandwidth Selection The bandwidth can be chosen to get a level of smoothness that looks good visually. For example, when observations are only collected daily like in the sleepstudy you will probably want to choose a bandwidth so that the estimated mean function does not have obvious bumps in between days. To choose the bandwidth \\(h_{n} &gt; 0\\) using a formal criterion, a common approach is to use leave-one-out cross-validation. In the context of longitudinal data, it is usually suggested that you leave one subject out at a time rather than one observation at a time (Rice and Silverman (1991)). The reason for this is that the subject-level leave-one-out cross-validation score is a good estimate of the mean-squared prediction error of regardless of what the correlation structure is for the within-subject outcomes. This is not the case when using observation-level leave-one-out cross-validation. The subject-level leave-one-out cross-validation score for a given bandwidth choice is defined as \\[\\begin{equation} \\textrm{LOOCV}(h_{n}) = \\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}} \\{ Y_{ij} - \\hat{\\mu}_{h_{n}}^{(-i)}(\\mathbf{x}_{ij}) \\}^{2} \\end{equation}\\] Here, \\(\\hat{\\mu}_{h_{n}}^{(-i)}(\\mathbf{x}_{ij})\\) is the mean function estimate when using bandwidth \\(h_{n}\\) and when ignoring the data from subject \\(i\\). 3.2.4 Another Example: The Bone Data As another example, we can use the “bone” dataset. This is a longitudinal dataset with typically 2 or 3 observations per individual. The outcome variable of interest is the relative spinal bone mineral density. This is actual the difference in mineral density taken on two consecutive visits divided by the average mineral density on those visits. bonedat &lt;- read.table(&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/bone.data&quot;, header=TRUE) head(bonedat) ## idnum age gender spnbmd ## 1 1 11.70 male 0.018080670 ## 2 1 12.70 male 0.060109290 ## 3 1 13.75 male 0.005857545 ## 4 2 13.25 male 0.010263930 ## 5 2 14.30 male 0.210526300 ## 6 2 15.30 male 0.040843210 For this data, the interest would be to model the mean function for bone mineral density (the variable spnbmd) as a function of age We can compute the leave-one-out cross-validation score for the bone data for different values of \\(h_{n}\\) (here \\(0.1 \\leq h_{n} \\leq 1\\)) with the following code: nh &lt;- 200 hh &lt;- seq(.1, 1, length.out=nh) LOOCV &lt;- rep(0, nh) subj.list &lt;- unique(bonedat$idnum) nsubj &lt;- length(subj.list) for(k in 1:nh) { ss &lt;- 0 for(i in 1:nsubj) { ind &lt;- bonedat$idnum==subj.list[i] yy &lt;- bonedat$spnbmd[-ind] xx &lt;- bonedat$age[-ind] tmp &lt;- ksmooth(xx, yy, kernel=&quot;normal&quot;, bandwidth = hh[k], x.points=bonedat$age[ind]) mu.hat &lt;- tmp$y ss &lt;- ss + sum((bonedat$spnbmd[ind] - mu.hat)^2) } LOOCV[k] &lt;- ss } hh[which.min(LOOCV)] ## best seems to be 0.1 ## [1] 0.1 In this case, the best bandwidth was \\(0.1\\) according to the subject-level leave-one-out cross-validation criterion. The kernel regression estimate of the mean function with the bandwidth of \\(0.1\\) is plotted below: bone.kernel &lt;- ksmooth(bonedat$age, bonedat$spnbmd, kernel=&quot;normal&quot;, bandwidth = 0.1, x.points=seq(9.4, 25, length.out=100)) plot(bonedat$age, bonedat$spnbmd, las=1, ylab=&quot;spnbmd&quot;, xlab=&quot;age&quot;, main=&quot;Bone Data: Kernel Regression with Bandwidth = 0.1&quot;, type=&quot;n&quot;) points(bonedat$age, bonedat$spnbmd, pch=16, cex=0.8) lines(bone.kernel$x, bone.kernel$y, lwd=2, col=&quot;red&quot;) Using a bandwidth of \\(1\\) gives a smoother mean function estimate. The performance of kernel regression methods can degrade quickly as we move to higher dimensions. The convergence rate of the estimated regression function to the true regression function slows substantially as we the dimension of the covariates \\(\\mathbf{x}_{ij}\\). “Curse of dimensionality” - need very large datasets to have a sufficient number of observations near a given point \\(\\mathbf{x}\\). Another approach when using multiple covariates is to use generalized additive models. With generalized additive models, the mean function is expressed as the sum of several univariate nonparametric functions: \\[\\begin{equation} \\mu(\\mathbf{x}) = \\beta_{0} + \\mu(x_{1}) + \\mu(x_{2}) + \\ldots + \\mu(x_{p}) \\end{equation}\\] 3.3 Regression Splines 3.3.1 Overview Using regression splines is a common nonparametric approach for estimating a mean function. The most common type of spline used in the context of nonparametric regression is the cubic spline. Definition: A cubic spline with knots \\(u_{1} &lt; u_{2} &lt; \\ldots &lt; u_{q}\\) is a function \\(f(x)\\) such that \\(f(x)\\) is a cubic function over each of the intervals \\((-\\infty, u_{1}], [u_{1}, u_{2}], \\ldots, [u_{q-1}, u_{q}], [u_{q}, \\infty)\\). \\(f(x)\\), \\(f&#39;(x)\\), and \\(f&#39;&#39;(x)\\) are all continuous functions. A commonly used set of basis functions for the set of cubic splines with knots \\(u_{1} &lt; u_{2} &lt; \\ldots &lt; u_{q}\\) is the B-spline basis functions. This means that if \\(\\varphi_{1, B}(x), \\ldots, \\varphi_{q+4, B}(x)\\) are the B-spline basis functions for the set of cubic splines with knots \\(u_{1} &lt; u_{2} &lt; \\ldots &lt; u_{q}\\), we can represent any cubic spline estimate of the mean function as \\[\\begin{equation} \\hat{\\mu}(x) = \\sum_{j=1}^{q+4} \\hat{\\beta}_{j}\\varphi_{j, B}(x) \\end{equation}\\] The nice thing about using regression splines is that they can estimated in the same way as in a “typical” regression setting. For regression splines, the columns of the design matrix contain the values of \\(\\varphi_{j,B}(x_{i})\\). 3.3.2 Regression Splines with Longitudinal Data in R Regression splines can be fitted in R by using the splines package library(splines) The bs function in splines generates the B-spline “design” matrix bs(x, df, knots, degree) x - vector of covariates values. This can also just be the name of a variable when bs is used inside a function such as geeglm. df - the “degrees of freedom”. For a cubic spline this is actually \\(q + 3\\) rather than \\(q + 4\\). If you just enter df, the bs function will pick the knots for you. knots - the vector of knots. If you don’t want to pick the knots, you can just enter a number for the df. degree - the degree of the piecewise polynomial. Typically, degree=3 which would be a cubic spline. You can directly use regression splines within the “GEE framework”. In this case, you can model the marginal mean (or part of the marginal mean function) with a spline. Here, we are assuming that \\(E(Y_{ij}|t_{ij}) = f_{0}(t_{ij})\\) with: \\(Y_{ij}\\) is the spnbmd value of individual \\(i\\) at age \\(t_{ij}\\). \\(t_{ij}\\) is the \\(j^{th}\\) age value of individual \\(i\\) The function \\(f_{0}(t)\\) will be estimated with a spline. To fit this with an AR1 correlation structure, you would use the following code: gee.bone0 &lt;- geeglm(spnbmd ~ bs(age, df=6), id=idnum, data=bonedat, corstr = &quot;ar1&quot;) The argument df = 6 means that the number of columns in the design matrix is 7 (due to the intercept), and the number of knots is is determined by the equation \\(q + 4 = 7\\) (so \\(q = 3\\)). You can actually explicity define the set of knots using the knots argument if you would like. We can look at the estimates of the regression coefficients by using summary. summary( gee.bone0 ) ## ## Call: ## geeglm(formula = spnbmd ~ bs(age, df = 6), data = bonedat, id = idnum, ## corstr = &quot;ar1&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) 0.05124 0.01580 10.518 0.00118 ** ## bs(age, df = 6)1 -0.01790 0.03214 0.310 0.57751 ## bs(age, df = 6)2 0.06437 0.01647 15.281 9.27e-05 *** ## bs(age, df = 6)3 -0.03130 0.02028 2.383 0.12263 ## bs(age, df = 6)4 -0.04220 0.01668 6.398 0.01142 * ## bs(age, df = 6)5 -0.06118 0.01951 9.837 0.00171 ** ## bs(age, df = 6)6 -0.03974 0.01687 5.549 0.01849 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = ar1 ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 0.001623 0.000135 ## Link = identity ## ## Estimated Correlation Parameters: ## Estimate Std.err ## alpha 0.2872 0.06967 ## Number of clusters: 261 Maximum cluster size: 3 There’s not really any interpretation to the individual regression coefficients estimates in a spline model You would typically be more interested in plotting the fitted values as a function of age. The gee.bone0 object has a component called “fitted values” which contains the values of \\(\\hat{f}_{0}(t_{ij})\\) for each \\(t_{ij}\\). ## First 5 observations and corresponding fitted values bonedat$age[1:5] ## [1] 11.70 12.70 13.75 13.25 14.30 gee.bone0$fitted.values[1:5] ## [1] 0.07000 0.08114 0.07288 0.07918 0.06279 To plot the estimated mean function, we can just draw lines through the fitted values: plot(bonedat$age, bonedat$spnbmd, xlab=&quot;age&quot;, ylab=&quot;spnbmd&quot;, main=&quot;Regression Spline Estimate for Bone Data&quot;, las=1) lines(bonedat$age[order(bonedat$age)], gee.bone0$fitted.values[order(bonedat$age)], lwd=2) ## plot spline estimate ## Use order(bonedat$age) so that the observations are ## sorted by age. 3.3.3 Looking at a Continuous and a Binary Covariate For example, with the bone data, suppose we want to fit separate curves for the male and female groups. We could express the mean function \\(\\mu(\\cdot)\\) as \\[\\begin{equation} \\mu(t_{ij}) = f_{0}(t_{ij}) + A_{ij}f_{1}(t_{ij}) \\tag{3.2} \\end{equation}\\] \\(f_{0}(\\cdot)\\) and \\(f_{1}(\\cdot)\\) would be modeled with regression splines. \\(t_{ij}\\) is the value of age for observation \\((i,j)\\) \\(A_{ij} = 1\\) if the \\((i,j)\\) observation corresponds to a male individual and \\(A_{ij} = 0\\) if the \\((i,j)\\) observation corresponds to a female individual. To fit model (3.2) using the geepack package, you can use the following code library(splines) gee.bone01 &lt;- geeglm(spnbmd ~ bs(age, df=6) + gender*bs(age,df=6), id=idnum, data=bonedat, corstr=&quot;ar1&quot;) We can plot the estimated mean functions by first extracting the fitted values for both the male and female groups: male.fitted &lt;- gee.bone01$fitted.values[bonedat$gender==&quot;male&quot;] male.age &lt;- bonedat$age[bonedat$gender==&quot;male&quot;] female.fitted &lt;- gee.bone01$fitted.values[bonedat$gender==&quot;female&quot;] female.age &lt;- bonedat$age[bonedat$gender==&quot;female&quot;] Now, plot the fitted curves for both groups plot(bonedat$age, bonedat$spnbmd, lwd=1, xlab=&quot;age&quot;, ylab=&quot;spnbmd&quot;, main=&quot;Regression Splines for the Bone Data&quot;, las=1) points(bonedat$age[bonedat$gender==&quot;male&quot;], bonedat$spnbmd[bonedat$gender==&quot;male&quot;], cex=0.8) points(bonedat$age[bonedat$gender==&quot;female&quot;], bonedat$spnbmd[bonedat$gender==&quot;female&quot;], pch=16, cex=0.8) lines(male.age[order(male.age)], male.fitted[order(male.age)], col=&quot;red&quot;, lwd=2) lines(female.age[order(female.age)], female.fitted[order(female.age)], col=&quot;blue&quot;, lwd=2) legend(&quot;topright&quot;, legend=c(&quot;Male&quot;, &quot;Female&quot;), col=c(&quot;red&quot;, &quot;blue&quot;), lwd=3, bty=&#39;n&#39;) We could also fit a model where there is a simple “male” effect that does not change over time. \\[\\begin{equation} \\mu(t_{ij}) = f_{0}(t_{ij}) + \\beta_{1}A_{ij} \\end{equation}\\] The mean function for male individuals would be \\(f_{0}(t) + \\beta_{1}\\). The mean function for females would be \\(f_{0}(t)\\). This could be fit with the following code: gee.bone1 &lt;- geeglm(spnbmd ~ bs(age, df=6) + gender, id=idnum, data=bonedat, corstr=&quot;ar1&quot;) If we plot the estimated mean functions from gee.bone1, it looks like the following: I think the model \\(\\mu(t_{ij}) = f_{0}(t_{ij}) + \\beta_{1}A_{ij}\\) is just not a good one. Forcing the mean function to have this form hides the differences between males/females. 3.3.4 Model Comparison We just fit the following three models \\[\\begin{eqnarray} \\mu(t_{ij}) &amp;=&amp; f_{0}(t_{ij}) \\\\ \\mu(t_{ij}) &amp;=&amp; f_{0}(t_{ij}) + \\beta_{1}A_{ij} \\\\ \\mu(t_{ij}) &amp;=&amp; f_{0}(t_{ij}) + A_{ij}f_{1}(t_{ij}) \\end{eqnarray}\\] These model fits were saved as gee.bone0, gee.bone1, and gee.bone01. We can formally compare the models using the anova method in R To compare, \\(\\mu(t_{ij}) = f_{0}(t_{ij})\\) vs. \\(\\mu(t_{ij}) = f_{0}(t_{ij}) + \\beta_{1}A_{ij}\\) do the following: anova( gee.bone0, gee.bone1) ## Analysis of &#39;Wald statistic&#39; Table ## ## Model 1 spnbmd ~ bs(age, df = 6) + gender ## Model 2 spnbmd ~ bs(age, df = 6) ## Df X2 P(&gt;|Chi|) ## 1 1 0.164 0.69 This p-value is quite large (0.69). This is saying there is not strong evidence favoring model \\(\\mu(t_{ij}) = f_{0}(t_{ij}) + \\beta_{1}A_{ij}\\) over the more simple model \\(\\mu(t_{ij}) = f_{0}(t_{ij})\\). In other words, a model with a single “male effect” is not better than a nonparametric model that does not take male/female into consideration. Now, let’s compare the models \\(\\mu(t_{ij}) = f_{0}(t_{ij})\\) vs. \\(\\mu(t_{ij}) = f_{0}(t_{ij}) + A_{ij}f_{1}(t_{ij})\\) anova( gee.bone0, gee.bone01) ## Analysis of &#39;Wald statistic&#39; Table ## ## Model 1 spnbmd ~ bs(age, df = 6) + gender * bs(age, df = 6) ## Model 2 spnbmd ~ bs(age, df = 6) ## Df X2 P(&gt;|Chi|) ## 1 7 62.1 5.9e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here, we see a very small p-value. This is strong evidence in favor of the model \\(\\mu(t_{ij}) = f_{0}(t_{ij}) + A_{ij}f_{1}(t_{ij})\\) over the model \\(\\mu(t_{ij}) = f_{0}(t_{ij})\\). 3.3.5 ACTG trial example actg_trial &lt;- read.csv(&quot;~/Documents/HDS629/actg_trial.csv&quot;) When you load the dataset into R, it should look like the following head( actg_trial, 10) ## SubjectID Treatment Age Sex Week CD4 ## 1 1 2 36.43 1 0.000 3.135 ## 2 1 2 36.43 1 7.571 3.045 ## 3 1 2 36.43 1 15.571 2.773 ## 4 1 2 36.43 1 23.571 2.833 ## 5 1 2 36.43 1 32.571 3.219 ## 6 1 2 36.43 1 40.000 3.045 ## 7 2 4 47.85 1 0.000 3.068 ## 8 2 4 47.85 1 8.000 3.892 ## 9 2 4 47.85 1 16.000 3.970 ## 10 2 4 47.85 1 23.000 3.611 This longitudinal dataset has 5036 observations with the following 6 variables: SubjectID - subject identifier Treatment - treatment received (4 possible treatments) Age - age in years at baseline Sex - 1=M, 0=F Week - time in weeks from baseline CD4 - this is the natural logarithm of the CD4 count + 1 Note that Treatment should be a factor variable actg_trial$Treatment &lt;- factor(actg_trial$Treatment) Let’s plot of CD4 vs. week for just individuals in Treatment 1. This plot includes a lowess smoothing line. Not any clear evidence that change in CD4 over time is not linear in the treatment 1 group. Trt1Dat &lt;- subset(actg_trial, Treatment==1) We can compare the following two models \\[\\begin{eqnarray} \\mu(t_{ij}) &amp;=&amp; \\beta_{0} + \\beta_{1}t_{ij} \\\\ \\mu(t_{ij}) &amp;=&amp; f_{0}(t_{ij}) \\end{eqnarray}\\] where \\(f_{0}\\) will be modeled with a spline function. We can fit these two models with geeglm using the following code: actg_trt1_linear &lt;- geeglm(CD4 ~ Week, id=SubjectID, data=Trt1Dat, corstr=&quot;ar1&quot;) actg_trt1_spline &lt;- geeglm(CD4 ~ bs(Week, df=4), id=SubjectID, data=Trt1Dat, corstr=&quot;ar1&quot;) Now, do a formal comparison: anova(actg_trt1_linear, actg_trt1_spline) ## Analysis of &#39;Wald statistic&#39; Table ## ## Model 1 CD4 ~ bs(Week, df = 4) ## Model 2 CD4 ~ Week ## Df X2 P(&gt;|Chi|) ## 1 3 0.975 0.81 No evidence to favor the nonparametric model over the linear model. For this type of data where, the time point of observations fall into clear “groups”, another direct nonparametric approach is just to have an indicator for each one of the time groups (e.g., weeks 0 - 5, weeks 5-12, weeks 12-20, etc.) Now, suppose we are interested in looking at differences in response to treatments in some way. Consider the following 3 models (where \\(A_{i}\\) is treatment assigned at baseline): \\[\\begin{eqnarray} \\mu(t_{ij}, a) &amp;=&amp; \\beta_{0} + \\beta_{1}t_{ij} \\\\ \\mu(t_{ij}, a) &amp;=&amp; \\beta_{0} + \\beta_{1}t_{ij} + I(A_{i}=a, a \\geq 2)\\beta_{a} \\\\ \\mu(t_{ij}, a) &amp;=&amp; \\beta_{0} + \\beta_{1}t_{ij} + I(A_{i}=a, a \\geq 2)\\beta_{a} + I(A_{i}=a, a \\geq 2)\\gamma_{a}t_{ij} \\\\ \\end{eqnarray}\\] Now, the mean function depends on time and treatment What is the interpretation of each of these models? Which model does not really make sense since this is a randomized trial? actg_mod1 &lt;- geeglm(CD4 ~ Week, id=SubjectID, data=actg_trial, corstr=&quot;ar1&quot;) actg_mod2 &lt;- geeglm(CD4 ~ Week + Treatment, id=SubjectID, data=actg_trial, corstr=&quot;ar1&quot;) actg_mod3 &lt;- geeglm(CD4 ~ Week*Treatment, id=SubjectID, data=actg_trial, corstr=&quot;ar1&quot;) Compare model 3 vs model 2 anova(actg_mod2, actg_mod3) ## Analysis of &#39;Wald statistic&#39; Table ## ## Model 1 CD4 ~ Week * Treatment ## Model 2 CD4 ~ Week + Treatment ## Df X2 P(&gt;|Chi|) ## 1 3 49.7 9.4e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Compare model 2 vs model 1 anova(actg_mod2, actg_mod1) ## Analysis of &#39;Wald statistic&#39; Table ## ## Model 1 CD4 ~ Week + Treatment ## Model 2 CD4 ~ Week ## Df X2 P(&gt;|Chi|) ## 1 3 6.53 0.089 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How would we fit the model \\[\\begin{equation} \\mu(t_{ij}, a) = \\beta_{0} + \\beta_{1}t_{ij} + I(A_{i}=a, a \\geq 2)\\gamma_{a}t_{ij} \\end{equation}\\] Use: actg_mod4 &lt;- geeglm(CD4 ~ Week + Week:Treatment, id=SubjectID, data=actg_trial, corstr=&quot;ar1&quot;) References "],["glmm-lasso.html", "Chapter 4 Sparse Regression for Longitudinal Data 4.1 Sparse regression methods 4.2 The Lasso with longitudinal data 4.3 Lasso for LMMs and GLMMs in R 4.4 Cross-Validation for Longitudinal Data 4.5 Penalized Generalized Estimating Equations 4.6 GLMM-Lasso with Binary Outcomes", " Chapter 4 Sparse Regression for Longitudinal Data 4.1 Sparse regression methods Sparse regression methods typically involve estimating the regression coefficients by minimizing a penalized least-squares criterion. The most well-known sparse regression method is the lasso. With the lasso, the regression coefficients \\(\\boldsymbol{\\beta}\\) are found by minimizing the following penalized least-squares criterion: \\[\\begin{equation} Q_{\\lambda}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n}(y_{i} - \\mathbf{x}_{i}^{T}\\boldsymbol{\\beta})^{2} + \\lambda \\sum_{j=1}^{p}|\\beta_{j}| \\end{equation}\\] An advantage of the lasso is that many of the individual estimated regression coefficients \\(\\hat{\\beta}_{j}\\) will equal zero exactly. You can think of the lasso as performing variable selection The regression coefficient estimates which are non-zero will be the “selected” variables. A nice feature of the lasso is that it performs simultaneous variable selection and regression coefficient estimation. You do not need to first select a model and then estimate the coefficients from this selected model. The selection and estimation is done at the same time. \\(\\lambda \\geq 0\\) in the \\(L_{1}\\) penalty function \\(\\lambda \\sum_{j=1}^{p}|\\beta_{j}|\\) is referred to as the “tuning parameter”. If \\(\\lambda\\) is large enough, all of the estimated regression coefficients will be equal to zero. If \\(\\lambda = 0\\), then the estimated regression coefficients will be the same as the usual least-squares estimates. For intermediate values of \\(\\lambda\\), some regression coefficients will be “selected” with the remaining regression coefficient estimates being set to zero. The value of the tuning parameter \\(\\lambda\\) is most ofte chosen through cross-validation. However, \\(\\lambda\\) is sometimes chosen by looking at an estimate of the “degrees of freedom” associated \\(\\lambda\\). Lasso paths: You can plot the values of the regression coefficients for different values of \\(\\lambda\\) to get a sense of which variables are selected first. In addition to performing variable selection, the lasso also shrinks the regression coefficient estimates towards zero. This can improve predictive performance when the regression coefficient estimates have high variance. This can occur, for example, if the matrix \\(\\mathbf{X}^{T}\\mathbf{X}\\) is poorly conditioned. Another advantage of the lasso and other penalized regression methods is that they can be used when the number of variables is greater than the number of observations. Although the lasso is often suggested as a tool for high-dimensional problems (i.e., lots of covariates), the lasso is still a good tool for moderate-sized number of covariates (e.g., 10-20). The lasso can still improve predictive performance in such cases, and the lasso enables simultaneous variable selection and estimation. 4.2 The Lasso with longitudinal data Recall our notation for longitudinal data with random effects: \\(Y_{ij}\\) - outcome for individual \\(i\\) at time \\(t_{ij}\\). \\(\\mathbf{x}_{ij}\\) - vector of covariates for individual \\(i\\) at time \\(t_{ij}\\). \\(\\mathbf{z}_{ij}\\) - vector determining form of random effects for individual \\(i\\) at time \\(t_{ij}\\) With penalized regression for longitudinal data, the linear mixed model still assumes that \\[\\begin{eqnarray} Y_{ij} &amp;=&amp; \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + b_{ij} + e_{ij} \\nonumber \\\\ &amp;=&amp; \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} + e_{ij} \\tag{4.1} \\end{eqnarray}\\] \\(\\boldsymbol{\\beta}\\) - vector of fixed effects \\(\\mathbf{u}_{i}\\) - vector of random effects \\(\\mathbf{u}_{i} \\sim \\textrm{Normal}(0, \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}})\\). \\(e_{ij} \\sim \\textrm{Normal}(0, \\sigma^{2})\\). If \\(\\mathbf{Y}_{i} = (Y_{i1}, ..., Y_{in_{i}})\\) is the vector of observations from the \\(i^{th}\\) person. The vectors \\(\\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{m}\\) are independent although the observations within each vector are not independent. The distribution of \\(\\mathbf{Y}_{i}\\) is \\(\\mathbf{Y}_{i} \\sim \\textrm{Normal}\\left( \\mathbf{X}_{i}\\boldsymbol{\\beta}, \\mathbf{V}_{i}(\\boldsymbol{\\theta}, \\sigma^{2}) \\right)\\). \\(\\mathbf{X}_{i}\\) is the \\(n_{i} \\times p\\) design matrix for individual \\(i\\). The covariance matrix of \\(\\mathbf{Y}_{i}\\) is \\[\\begin{equation} \\mathbf{V}_{i}(\\boldsymbol{\\theta}, \\sigma^{2}) = \\textrm{Cov}(\\mathbf{Y}_{i}) = \\mathbf{Z}_{i}\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}\\mathbf{Z}_{i}^{T} + \\sigma^{2}\\mathbf{I}_{n_{i}} \\end{equation}\\] \\(\\mathbf{Z}_{i}\\) is the \\(n_{i} \\times q\\) random effects design matrix for individual \\(i\\). Let \\(\\mathbf{Y}\\) be the vector of responses stacked in “long form”: \\(\\mathbf{Y} = (Y_{11}, Y_{12}, ...., Y_{mn_{m}})\\) Under the assumed linear mixed model (4.1), we have \\[\\begin{equation} \\mathbf{Y} \\sim \\textrm{Normal}(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{V}) \\end{equation}\\] The covariance matrix \\(\\mathbf{V}\\) will be “block diagonal” diagonal matrix with the blocks being \\(\\mathbf{V}_{i}(\\theta, \\sigma^{2})\\) \\[\\begin{equation} \\mathbf{V} = \\begin{bmatrix} \\mathbf{V}_{1}(\\boldsymbol{\\theta}, \\sigma^{2}) &amp; \\mathbf{0} &amp; \\mathbf{0} &amp; \\ldots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{V}_{2}(\\boldsymbol{\\theta}, \\sigma^{2}) &amp; \\mathbf{0} &amp; \\ldots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{V}_{3}(\\boldsymbol{\\theta}, \\sigma^{2}) &amp; \\ldots &amp; \\mathbf{0} \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\mathbf{0} &amp; \\ldots &amp; \\mathbf{V}_{m}(\\boldsymbol{\\theta}, \\sigma^{2}) \\end{bmatrix} \\end{equation}\\] — With the LMM-Lasso (Schelldorfer, Bühlmann, and De Geer (2011)), you estimate the vector of fixed effects \\(\\boldsymbol{\\beta}\\) and the parameters in \\(\\mathbf{V}\\) by minimizing the following penalized negative log-likelihood: \\[\\begin{eqnarray} &amp;&amp; Q_{\\lambda}(\\boldsymbol{\\beta}, \\boldsymbol{\\theta}, \\sigma^{2}) = \\frac{1}{2}\\log\\det(\\mathbf{V}) + \\frac{1}{2}(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})^{T}\\mathbf{V}^{-1} (\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}) + \\lambda\\sum_{j=1}^{p} |\\beta_{j}| \\nonumber \\\\ &amp;=&amp; \\frac{1}{2}\\sum_{i=1}^{m} \\log\\det\\left( \\mathbf{V}_{i}(\\boldsymbol{\\theta}, \\sigma^{2}) \\right) + \\frac{1}{2}\\sum_{i=1}^{m} (\\mathbf{Y}_{i} - \\mathbf{X}_{i}\\boldsymbol{\\beta})^{T}\\mathbf{V}_{i}^{-1}(\\boldsymbol{\\theta}, \\sigma^{2}) (\\mathbf{Y}_{i} - \\mathbf{X}_{i}\\boldsymbol{\\beta}) + \\lambda\\sum_{j=1}^{p} |\\beta_{j}| \\nonumber \\\\ \\end{eqnarray}\\] In Schelldorfer, Bühlmann, and De Geer (2011), suggest using a Bayesian information criterion (BIC) to choose the tuning parameter \\(\\lambda\\). This is defined as \\[\\begin{equation} \\textrm{BIC}_{\\lambda} = -2 \\times \\textrm{log-likelihood} + \\log(n) \\times df_{\\lambda} \\end{equation}\\] \\(df_{\\lambda}\\) is equal to the number of non-zero regression coefficients when using \\(\\lambda\\) plus the number of paramaters in the matrix \\(\\mathbf{V}_{i}(\\boldsymbol{\\theta}, \\sigma^{2})\\). 4.3 Lasso for LMMs and GLMMs in R One R package which fits linear mixed models and generalized linear mixed models with the Lasso penalty is the glmmLasso package. There are also methods and R implementations for penalized regression with GEEs. See, for example, the paper: Wang, Zhou, and Qu (2012) I won’t cover that today. 4.3.1 Soccer Data To briefly show how this works, we can use the soccer data from the glmmLasso package. This is actually not a longitudinal dataset, but it does have repeated measures. This dataset has 54 observations with 23 unique teams. Each row in this dataset corresponds to data taken from a single team in a single season. library(glmmLasso) data(&quot;soccer&quot;) dim(soccer) ## 54 observations and 16 variables ## [1] 54 16 head(soccer) ## pos team points transfer.spendings transfer.receits yellow.card ## 338 12 1. FC Koeln 39 5150000 750000 70 ## 357 13 1. FC Koeln 38 11500000 900000 70 ## 324 16 1. FC Nuernberg 31 5400000 6350000 61 ## 360 16 1. FC Nuernberg 31 450000 1900000 53 ## 353 9 1. FSV Mainz 05 47 3000000 200000 58 ## 333 7 1899 Hoffenheim 55 11950000 125000 70 ## yellow.red.card red.card unfair.score ave.unfair.score ball.possession ## 338 2 2 86 2.53 49.02 ## 357 1 3 88 2.59 48.23 ## 324 3 0 70 2.06 52.02 ## 360 2 0 59 1.74 50.41 ## 353 1 3 76 2.24 47.76 ## 333 3 3 94 2.76 49.64 ## tackles capacity total.attend ave.attend sold.out ## 338 49.17 50076 853767 50222 11 ## 357 48.94 50076 822102 48359 6 ## 324 51.26 48548 742739 43691 2 ## 360 49.61 48548 719705 42336 5 ## 353 49.44 20300 342350 20138 11 ## 333 50.38 30164 477414 28083 17 length(unique(soccer$team)) ## 23 unique teams ## [1] 23 The variable team represents the soccer team. Each team has 2 or 3 seasons of data. The variable points represents the total number of points scored over the course of the season. There are a number of other variables that may explain some of the variation in points scored: ball.possession, tackles, etc. We will use points and 10 of the other variables as the fixed-effects covariates. It is common in practice to center and scale the covariates before running the lasso: soccer[,c(4,5,9:16)] &lt;- scale(soccer[,c(4,5,9:16)], center=TRUE, scale=TRUE) soccer &lt;- data.frame(soccer) To fit an lmm-lasso with \\(\\lambda = 100\\) and a random intercept for each team, we can use the following code lm.lambda100 &lt;- glmmLasso(points ~ transfer.spendings + ave.unfair.score + ball.possession + tackles + ave.attend + sold.out, rnd = list(team=~1), lambda=100, data = soccer) Note that the random effects model (i.e., the model for \\(b_{ij}\\)) is specified through the rnd argument. team = ~1 means that \\(b_{ij} = u_{i}\\) for each \\(i,j\\). To look at the summary of the parameter estimates, use summary summary(lm.lambda100) ## Call: ## glmmLasso(fix = points ~ transfer.spendings + ave.unfair.score + ## ball.possession + tackles + ave.attend + sold.out, rnd = list(team = ~1), ## data = soccer, lambda = 100) ## ## ## Fixed Effects: ## ## Coefficients: ## Estimate StdErr z.value p.value ## (Intercept) 46.587693 NA NA NA ## transfer.spendings 2.889109 NA NA NA ## ave.unfair.score -0.090639 NA NA NA ## ball.possession 2.157890 NA NA NA ## tackles 1.643980 NA NA NA ## ave.attend 2.024174 NA NA NA ## sold.out 3.027830 NA NA NA ## ## Random Effects: ## ## StdDev: ## team ## team 0.4805113 All coefficient estimates are non-zero except for the “average unfariness score per match” variable If we set \\(\\lambda = 500\\), all of the coefficient estimates will be zero: lm.lambda500 &lt;- glmmLasso(points ~ transfer.spendings + ave.unfair.score + ball.possession + tackles + ave.attend + sold.out, rnd = list(team=~1), lambda=500, data = soccer) summary(lm.lambda500) ## Call: ## glmmLasso(fix = points ~ transfer.spendings + ave.unfair.score + ## ball.possession + tackles + ave.attend + sold.out, rnd = list(team = ~1), ## data = soccer, lambda = 500) ## ## ## Fixed Effects: ## ## Coefficients: ## Estimate StdErr z.value p.value ## (Intercept) 44.321 NA NA NA ## transfer.spendings 0.000 NA NA NA ## ave.unfair.score 0.000 NA NA NA ## ball.possession 0.000 NA NA NA ## tackles 0.000 NA NA NA ## ave.attend 0.000 NA NA NA ## sold.out 0.000 NA NA NA ## ## Random Effects: ## ## StdDev: ## team ## team 11.81332 We can find the value of BIC by looking at the $bic component of lm.lambda100 lm.lambda100$bic ## [,1] ## [1,] 449.7875 4.3.2 Choosing the tuning parameter for the soccer data Because \\(\\lambda = 500\\) implies that all of the coefficient estimates are zero, we know that the “best” value of \\(\\lambda\\) should be somewhere between \\(0\\) and \\(500\\). Let’s compute the BIC across a grid of \\(\\lambda\\) values from \\(0\\) to \\(500\\) and plot the result lam.seq &lt;- seq(0, 500, by=5) BIC.values &lt;- rep(0, length(lam.seq)) for(k in 1:length(lam.seq)) { lm.tmp &lt;- glmmLasso(points ~ transfer.spendings + ave.unfair.score + ball.possession + tackles + ave.attend + sold.out, rnd = list(team=~1), lambda=lam.seq[k], data = soccer) BIC.values[k] &lt;- lm.tmp$bic } plot(lam.seq, BIC.values, xlab=expression(lambda), ylab=&quot;BIC&quot;, main=&quot;BIC for soccer data&quot;) It looks like the lowest BIC value is in between 90 and 130. Let’s plot the BIC values for a denser grid of \\(\\lambda\\) values between 90 and 130 lam.seq &lt;- seq(90, 130, by=1) BIC.values &lt;- rep(0, length(lam.seq)) for(k in 1:length(lam.seq)) { lm.tmp &lt;- glmmLasso(points ~ transfer.spendings + ave.unfair.score + ball.possession + tackles + ave.attend + sold.out, rnd = list(team=~1), lambda=lam.seq[k], data = soccer) BIC.values[k] &lt;- lm.tmp$bic } plot(lam.seq, BIC.values, xlab=expression(lambda), ylab=&quot;BIC&quot;, main=&quot;BIC for soccer data&quot;) lines(lam.seq, BIC.values) The best value of \\(\\lambda\\) according to the BIC criterion is \\(119\\): lam.seq[which.min(BIC.values)] ## [1] 119 Let’s look at the regression coefficient estimates using \\(\\lambda = 119\\) lm.lambda19 &lt;- glmmLasso(points ~ transfer.spendings + ave.unfair.score + ball.possession + tackles + ave.attend + sold.out, rnd = list(team=~1), lambda=119, data = soccer) summary(lm.lambda19) ## Call: ## glmmLasso(fix = points ~ transfer.spendings + ave.unfair.score + ## ball.possession + tackles + ave.attend + sold.out, rnd = list(team = ~1), ## data = soccer, lambda = 119) ## ## ## Fixed Effects: ## ## Coefficients: ## Estimate StdErr z.value p.value ## (Intercept) 46.5882 NA NA NA ## transfer.spendings 2.9058 NA NA NA ## ave.unfair.score 0.0000 NA NA NA ## ball.possession 2.1752 NA NA NA ## tackles 1.6407 NA NA NA ## ave.attend 2.0073 NA NA NA ## sold.out 3.0555 NA NA NA ## ## Random Effects: ## ## StdDev: ## team ## team 0.4964462 4.4 Cross-Validation for Longitudinal Data Cross-validation without any longitudinal or repeated-measures structure is pretty straightforward. For longitudinal data, the type of cross-validation can depend on the prediction goals/context. In many cases, it makes sense to hold out random individuals (or groups) in each test set. In other words, each training set would look like the following: \\[\\begin{equation} \\mathcal{T}_{r} = \\{ \\textrm{ all } (Y_{ij}, \\mathbf{x}_{ij}), \\textrm{such that } i \\in \\mathcal{S} \\} \\end{equation}\\] where \\(\\mathcal{S}\\) is a random subset of indeces. In cases where you are thinking of using your model for forecasting, it may make sense to use an alternative strategy for cross-validation. In this case, you may want to construct the test sets so that they only contain observations at “future” time points when compared with the training set. Let’s try doing 5-fold cross-validation with the soccer data. To do this, it’s easier to just create a team id variable first team.labels&lt;-data.frame(team=unique(soccer$team),team.id=as.numeric(unique(soccer$team))) soccer &lt;- merge(soccer, team.labels, by=&quot;team&quot;) head(soccer) ## team pos points transfer.spendings transfer.receits yellow.card ## 1 1. FC Koeln 12 39 -0.47350170 -0.6774385 70 ## 2 1. FC Koeln 13 38 -0.08019776 -0.6624274 70 ## 3 1. FC Nuernberg 16 31 -0.45801729 -0.1170267 61 ## 4 1. FC Nuernberg 16 31 -0.76460855 -0.5623539 53 ## 5 1. FSV Mainz 05 9 47 -0.60666760 -0.7324789 58 ## 6 1899 Hoffenheim 7 55 -0.05232583 -0.7399844 70 ## yellow.red.card red.card unfair.score ave.unfair.score ball.possession ## 1 2 2 1.0002188 1.0005114 -0.2847609 ## 2 1 3 1.1494227 1.1527733 -0.5154026 ## 3 3 0 -0.1934125 -0.1922072 0.5910938 ## 4 2 0 -1.0140340 -1.0042709 0.1210518 ## 5 1 3 0.2541992 0.2645786 -0.6526198 ## 6 3 3 1.5970344 1.5841821 -0.1037509 ## tackles capacity total.attend ave.attend sold.out team.id ## 1 -0.5594251 0.1908255 0.52774513 0.52776771 0.9696315 1 ## 2 -0.7152112 0.1908255 0.41510241 0.41510371 -0.1303706 1 ## 3 0.8561964 0.1027586 0.13278238 0.13280873 -1.0103723 2 ## 4 -0.2613995 0.1027586 0.05084294 0.05086578 -0.3503711 2 ## 5 -0.3765458 -1.5253268 -1.29153165 -1.29154724 0.9696315 3 ## 6 0.2601452 -0.9568110 -0.81106503 -0.81107731 2.2896341 4 Now create each of the 5 test sets. set.seed(2352) ## first create the indices for the test sets nfolds &lt;- 5 test.groups &lt;- sample(1:nfolds, size=23, replace=TRUE) test.groups ## [1] 2 5 2 5 4 3 1 4 1 2 3 5 3 1 5 2 1 4 3 2 4 5 4 ## test.groups == k means that the observation will be in the kth test set ## For such a small dataset, you may want to randomly generate the ## test sets so that they all have the same size. Now, compute cross-validation estimates of the mean-squared error over a grid of \\(\\lambda\\) values lam.seq &lt;- seq(80, 200, by=10) MSE &lt;- matrix(0, nfolds, length(lam.seq)) for(j in 1:length(lam.seq)) { for(k in 1:nfolds) { soccer.test &lt;- soccer[test.groups==k,] soccer.train &lt;- soccer[test.groups!=k,] tmp.lm &lt;- glmmLasso(points ~ transfer.spendings + ave.unfair.score + ball.possession + tackles + ave.attend + sold.out, rnd = list(team=~1), lambda=lam.seq[j], data = soccer.train) predicted.values &lt;- predict(tmp.lm, newdata=soccer.test) MSE[k,j] &lt;- mean((predicted.values - soccer.test$points)^2) } } plot(lam.seq, colMeans(MSE), xlab=expression(lambda), ylab=&quot;MSE&quot;, main=&quot;5-fold cross-validation for the soccer data&quot;) lines(lam.seq, colMeans(MSE)) According to the cross-validation estimates of prediction error, the best value of \\(\\lambda\\) is somewhere between \\(150\\) and \\(200\\). As another example of applying glmmLasso to longitudinal data, we can use the LongituRF package. This has a function called DataLongGenerator. To generate a longitudinal dataset with 30 individuals and 6 covariates, use the following code: library(LongituRF) DF &lt;- DataLongGenerator(n=30, p=6) sim_long &lt;- data.frame(y=DF$Y, time=DF$time, DF$X, id=DF$id) sim_long$id &lt;- factor(sim_long$id) # id variables should be factors # for glmmLasso head(sim_long) ## y time X1 X2 X3 X4 X5 ## 1 1.799842 1 -0.05305192 -1.17862404 -0.6402514 1.08949573 1.78153877 ## 2 4.889512 2 1.51164592 0.07041142 -0.1110482 0.68765869 0.99507885 ## 3 13.116280 3 2.48639472 1.31250596 0.6618778 1.39961688 0.36944962 ## 4 15.394771 4 2.58113362 1.55329246 0.6282454 0.91656040 0.05963849 ## 5 16.274372 5 2.65570681 2.73986512 1.3471802 0.66421057 -0.19163578 ## 6 18.363802 6 2.90758445 3.01181338 1.3152902 0.06486541 -0.29723344 ## X6 id ## 1 -4.5727670 1 ## 2 -6.8566505 1 ## 3 -2.1983103 1 ## 4 0.9380251 1 ## 5 0.9206991 1 ## 6 1.5710471 1 length(unique(sim_long$id)) # 30 unique individuals ## [1] 30 Then, to fit a mixed effects model using glmmLasso with a random intercept and a random slope for the time variable, you can use the following code. glmm_fit_sim &lt;- glmmLasso(y ~ time + X1 + X2 + X3 + X4 + X5 + X6, rnd = list(id=~1+time), lambda=10, data = sim_long) ## Warning in est.glmmLasso.RE(fix = fix, rnd = rnd, data = data, lambda = ## lambda, : Random slopes are not standardized back! The summary output should have estimates of both the random intercept standard deviation and the random slope standard deviation: summary(glmm_fit_sim) ## Call: ## glmmLasso(fix = y ~ time + X1 + X2 + X3 + X4 + X5 + X6, rnd = list(id = ~1 + ## time), data = sim_long, lambda = 10) ## ## ## Fixed Effects: ## ## Coefficients: ## Estimate StdErr z.value p.value ## (Intercept) 4.2850628 NA NA NA ## time 0.3659943 NA NA NA ## X1 0.9394721 NA NA NA ## X2 1.0545024 NA NA NA ## X3 0.4744803 NA NA NA ## X4 -0.0045651 NA NA NA ## X5 -1.2023977 NA NA NA ## X6 0.0268859 NA NA NA ## ## Random Effects: ## ## StdDev: ## id id:time ## id 5.9167427 0.9256047 ## id:time 0.9256047 0.5467104 4.5 Penalized Generalized Estimating Equations Without any penalization, a generalized estimating equation (GEE) approach to estimating \\(\\boldsymbol{\\beta}\\) works by choosing \\(\\boldsymbol{\\beta}\\) to solve the following system of equations \\[\\begin{equation} S_{\\alpha}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{m} \\mathbf{D}_{i}^{T}\\mathbf{V}_{i}^{-1}\\left(\\mathbf{Y}_{i} - \\boldsymbol{\\mu}_{i}(\\boldsymbol{\\beta}) \\right) = \\mathbf{0} \\end{equation}\\] \\(\\boldsymbol{\\mu}_{i}(\\boldsymbol{\\beta}) = g^{-1}(\\mathbf{X}_{i}\\boldsymbol{\\beta})\\): this is a \\(n_{i} \\times 1\\) vector \\(\\mathbf{D}_{i} = \\partial \\boldsymbol{\\mu}_{i}/\\partial \\boldsymbol{\\beta}\\): this is a \\(n_{i} \\times p\\) matrix. \\(\\mathbf{V}_{i}\\) is the “working” covariance matrix of \\(\\mathbf{Y}_{i}\\) which can depend on the parameter \\(\\alpha\\). For penalized GEE, we are going to solve the equation \\(U_{\\alpha,\\lambda}(\\boldsymbol{\\beta}) = \\mathbf{0}\\), where \\(U_{\\alpha, \\lambda}(\\boldsymbol{\\beta})\\) is defined as \\[\\begin{equation} U_{\\alpha, \\lambda}(\\boldsymbol{\\beta}) = S_{\\alpha}(\\boldsymbol{\\beta}) - \\sum_{j=1}^{p} q_{\\lambda}(|\\beta_{j}|)\\textrm{sign}(\\beta_{j}) \\end{equation}\\] Here, \\(q_{\\lambda}()\\) is some choice of “penalty” function and \\(\\textrm{sign}(\\beta_{j}) = 1\\) if \\(\\beta_{j} &gt; 0\\) and \\(\\textrm{sign}(\\beta_{j}) = -1\\) if \\(\\beta_{j} &lt; 0\\). The reason for considering \\(\\textrm{sign}(\\beta_{j})\\) is that we are no longer trying to minimize \\(U_{\\alpha, \\lambda}(\\boldsymbol{\\beta})\\), but rather trying to solve \\(U_{\\alpha,\\lambda}(\\boldsymbol{\\beta}) = \\mathbf{0}\\). You can think of this as setting the derivative of a quasi-penalized log-likelihood to zero and solving it. There are a number of possible choices for \\(q_{\\lambda}()\\). Penalized GEE as implemented by the PGEE package uses the derivative of the “SCAD” penalty. For \\(t &gt; 0\\), the derivative of the SCAD penalty is defined as \\[\\begin{equation} q_{\\lambda}(t) = \\begin{cases} t &amp; \\text{ for } t &lt; \\lambda \\\\ \\frac{a\\lambda - t}{(a - 1)\\lambda} &amp; \\text{ for } \\lambda \\leq t &lt; a\\lambda \\\\ 0 &amp; \\text{ for } t &gt; a\\lambda \\end{cases} \\end{equation}\\] 4.5.1 The PGEE package Just to show the basics of how the PGEE package works, we can look at the yeastG1 dataset from the PGEE package library(PGEE) data(yeastG1) ## look at first 6 rows and first 5 columns yeastG1[1:6, 1:5] ## id y time ABF1 ACE2 ## 1 1 0.88 3 -0.09702788 8.3839614 ## 2 1 0.32 4 -0.09702788 8.3839614 ## 3 1 1.09 12 -0.09702788 8.3839614 ## 4 1 0.73 13 -0.09702788 8.3839614 ## 5 2 0.66 3 -0.34618104 -0.1418099 ## 6 2 -0.05 4 -0.34618104 -0.1418099 The response of interest is the continuous measurement y. There are 96 covariates (besides time). I think these are all just different transcription factors. These 96 covariates are not time-varying. Suppose we want to fit the following marginal mean model \\[\\begin{equation} E(Y_{ij}|\\mathbf{x}_{ijk}) = \\gamma_{0} + \\gamma_{1}t_{ij} + \\sum_{k=1}^{96}\\beta_{j}x_{ijk} \\end{equation}\\] Note that \\(x_{ijk}\\) does not change across values of \\(j\\). To fit the above model with an AR(1) correlation structure and \\(\\lambda = 0.1\\), you can use the following R code m0 &lt;- PGEE(y ~. -id, id=id, corstr=&quot;AR-1&quot;, lambda=0.1, data=yeastG1) Let’s look at the values of the first 5 estimated regression coefficients: m0$coefficients[1:5] ## (Intercept) time ABF1 ACE2 ADR1 ## 1.879532e-03 1.795182e-02 -1.366213e-02 4.360565e-06 2.988796e-07 The PGEE function does not automatically return exactly zero regression coefficients, but you can set those coefficients whose absolute value is less than some small threshold equal to zero. length(m0$coefficients) ## [1] 98 ## 71 out of 98 coefficients are &quot;zero&quot; sum(abs(m0$coefficients) &lt; 1e-4) ## [1] 71 The PGEE package does have a function to select the best value of \\(\\lambda\\) through cross-validation. However, you do have to provide a range of lambda values for the function to search over. By trial and error, you can find a small value \\(\\lambda_{min}\\) where most of the coefficients are nonzero. a large value \\(\\lambda_{max}\\) where most of the coefficients are zero. then, perform cross-validation over the range \\((\\lambda_{min}, \\lambda_{max})\\). Setting \\(\\lambda_{min} = 0.01\\) and \\(\\lambda_{max} = 0.3\\) seems reasonable. This gives a range of 12-92 for the number of zero coefficients mlow &lt;- PGEE(y ~. -id, id=id, corstr=&quot;AR-1&quot;, lambda=0.01, data=yeastG1) mhigh &lt;- PGEE(y ~. -id, id=id, corstr=&quot;AR-1&quot;, lambda=0.3, data=yeastG1) sum(abs(mlow$coefficients) &lt; 1e-4) ## only 12 out of 98 are zero sum(abs(mhigh$coefficients) &lt; 1e-4) ## now, 92 out of 98 are zero The “Quasi Information Criterion” (QIC) is a model selection tool for GEEs. This is similar to AIC or BIC in likelihood-based methods. QIC could potentially be used to compute \\(\\lambda\\). However, QIC is not implemented by the PGEE package. QIC is implemented in geepack. So, if you’re doing model comparison with different covariates in geepack, you can compare their QIC values. Now, let’s use the CVfit function from the PGEE package to get the best value of \\(\\lambda\\) over the range \\((\\lambda_{min}, \\lambda_{max})\\). Use 5-fold cross-validation using the fold argument in CVfit. Use a lambda sequence of length 10 from 0.01 to 0.3 cv.yeast &lt;- CVfit(y ~. -id, id=id, lambda=seq(0.01, 0.3, length.out=10), fold = 5, data=yeastG1) The cross-validation done by CVfit does automatically assume an “independent” working correlation structure. The lam.opt component of cv.yeast gives the optimal value of lambda. cv.yeast$lam.opt ## [1] 0.04222222 Now, we can just use PGEE with the optimal value of lambda. mfinal &lt;- PGEE(y ~. -id, id=id, corstr=&quot;AR-1&quot;, lambda=cv.yeast$lam.opt, data=yeastG1) From the mfinal object returned by PGEE, we can look at the selected nonzero coefficients mfinal$coefficients[abs(mfinal$coefficients) &gt; 1e-4] ## (Intercept) time ABF1 ACE2 ARG81 ## 0.0654041421 0.0124562882 -0.0325838863 0.0127874415 0.0169445034 ## ASH1 CAD1 CIN5 DAL82 DIG1 ## -0.0482950685 0.0022875036 0.0197437839 -0.0033708398 0.0064621372 ## FKH1 FKH2 FZF1 GAT1 GAT3 ## -0.0224616164 -0.0854029778 -0.0141534498 0.0157663787 0.4004159946 ## GCN4 GCR1 GCR2 GRF10.Pho2. GTS1 ## -0.0072661953 -0.0052707577 -0.0313007398 -0.0149795248 -0.0196408251 ## HAL9 HIR1 HIR2 IXR1 LEU3 ## 0.0064780824 -0.0067448440 -0.0005240435 -0.0036456617 -0.0002425373 ## MATa1 MBP1 MET31 MET4 MSN4 ## -0.0019846420 0.1338670401 -0.0058896291 -0.0356033691 0.0090765224 ## MTH1 NDD1 NRG1 PHD1 RCS1 ## -0.0199223945 -0.1048632039 0.0071845565 0.0244530145 -0.0072457243 ## REB1 RFX1 RLM1 RME1 ROX1 ## -0.0183030215 0.0024567990 0.0017190724 0.0112334737 0.0099033888 ## RTG1 SFP1 SKN7 SMP1 STB1 ## 0.0041439296 0.0161008010 0.0194410387 0.0163084797 0.0712247908 ## STP1 SWI4 SWI5 SWI6 YAP1 ## 0.0402297692 0.0309969037 0.0190456062 0.0376147418 -0.0021417459 ## YAP5 YAP6 YJL206C ZAP1 ## -0.3194214925 -0.0247157203 0.0014576098 -0.0182930638 We can also look at the estimated working correlation matrix round(mfinal$working.correlation, 3) ## [,1] [,2] [,3] [,4] ## [1,] 1.000 0.115 0.013 0.002 ## [2,] 0.115 1.000 0.115 0.013 ## [3,] 0.013 0.115 1.000 0.115 ## [4,] 0.002 0.013 0.115 1.000 PGEE also returns most of the other types of components that functions like lm, glm, geeglm return: e.g., fitted.values, residuals, etc. 4.6 GLMM-Lasso with Binary Outcomes You can use glmmLasso with binary outcomes by adding the family=binomial() argument. As a quick example, let’s look at the ohio data from the geepack package. library(geepack) data(ohio) head(ohio) ## resp id age smoke ## 1 0 0 -2 0 ## 2 0 0 -1 0 ## 3 0 0 0 0 ## 4 0 0 1 0 ## 5 0 1 -2 0 ## 6 0 1 -1 0 For the glmmLasso function, you do need to make sure the “id variable” is a factor. ohio$id &lt;- factor(ohio$id) Let’s now fit a penalized generalized linear mixed model with \\(\\lambda = 10\\): ohio.fit10 &lt;- glmmLasso(smoke ~ resp + age, family=binomial(), rnd = list(id=~1), lambda=10, data = ohio) It looks like the wheeze status variable was selected while the age variable was not. summary(ohio.fit10) ## Call: ## glmmLasso(fix = smoke ~ resp + age, rnd = list(id = ~1), data = ohio, ## lambda = 10, family = binomial()) ## ## ## Fixed Effects: ## ## Coefficients: ## Estimate StdErr z.value p.value ## (Intercept) -0.67603 NA NA NA ## resp 0.00000 NA NA NA ## age 0.00000 NA NA NA ## ## Random Effects: ## ## StdDev: ## id ## id 0.9496094 References "],["risk-prediction.html", "Chapter 5 Risk Prediction and Validation (Part I) 5.1 Risk Prediction/Stratification 5.2 Area under the ROC curve and the C-statistic 5.3 Area under the ROC curve 5.4 Calibration 5.5 Longitudinal Data and Risk Score Validation", " Chapter 5 Risk Prediction and Validation (Part I) 5.1 Risk Prediction/Stratification For binary outcomes, a risk prediction is related to the predicted probability that an individual has or will develop a certain condition/trait. Examples of a risk prediction for a binary outcome: Probability that someone has hypertension. Probability that someone has type-2 diabetes. Probability someone will be hospitalized over the next 1 year. Risk stratification: Often, a risk prediction model will only report whether or not someone belongs to one of several risk strata. For example, high, medium, or low risk. 5.2 Area under the ROC curve and the C-statistic 5.2.1 Sensitivity and Specificity Let \\(g(\\mathbf{x}_{i})\\) be a risk score for an individual with covariate vector \\(\\mathbf{x}_{i}\\). Higher values of \\(g(\\mathbf{x}_{i})\\) are supposed to imply greater probability that the binary outcome \\(Y_{i}\\) is equal to \\(1\\). Though \\(g(\\mathbf{x}_{i})\\) does not necessarily have to be a predicted probability. The value \\(t\\) will be a threshold which determines whether or not, we predict \\(Y_{i} = 1\\) or not. \\(g(\\mathbf{x}_{i}) \\geq t\\) implies \\(Y_{i} = 1\\) \\(g(\\mathbf{x}_{i}) &lt; t\\) implies \\(Y_{i} = 0\\) The sensitivity of the risk score \\(g(\\mathbf{x}_{i})\\) with the threshold \\(t\\) is defined as the probability you “predict” \\(Y_{i} = 1\\) assuming that \\(Y_{i}\\) is, in fact, equal to \\(1\\). In other words, the sensitivity is the probability of making the right decision given that \\(Y_{i} = 1\\). Sensitivity is often called the “true positive rate”. The sensitivity is defined as: \\[\\begin{eqnarray} \\textrm{Sensitivity}(t; g) &amp;=&amp; P\\big\\{ g(\\mathbf{x}_{i}) \\geq t| Y_{i} = 1 \\big\\} \\nonumber \\\\ &amp;=&amp; \\frac{P\\big\\{ g(\\mathbf{x}_{i}) \\geq t, Y_{i} = 1 \\big\\}}{P\\big\\{ Y_{i} = 1 \\big\\} }\\nonumber \\\\ &amp;=&amp; \\frac{P\\big\\{ g(\\mathbf{x}_{i}) \\geq t, Y_{i} = 1 \\big\\}}{ P\\big\\{ g(\\mathbf{x}_{i}) \\geq t, Y_{i} = 1 \\big\\} + P\\big\\{ g(\\mathbf{x}_{i}) &lt; t, Y_{i} = 1 \\big\\}} \\end{eqnarray}\\] For a worthless risk score that is totally uninformative about the outcome, we should expect the sensitivity to be close to \\(P\\{ g(\\mathbf{x}_{i}) \\geq t \\}\\). You can compute the in-sample sensitivity with \\[\\begin{eqnarray} \\hat{\\textrm{Sensitivity}}(t; g) &amp;=&amp; \\frac{\\sum_{i=1}^{n} I\\big\\{ g(\\mathbf{x}_{i}) \\geq t, Y_{i} = 1 \\big\\} }{ \\sum_{i=1}^{n} I\\big\\{g(\\mathbf{x}_{i}) \\geq t, Y_{i} = 1 \\big\\} + \\sum_{i=1}^{n} I\\big\\{ g(\\mathbf{x}_{i}) &lt; t, Y_{i} = 1 \\big\\}} \\nonumber \\\\ &amp;=&amp; \\frac{\\textrm{number of true positives}}{\\textrm{number of true positives} + \\textrm{number of false negatives} } \\end{eqnarray}\\] The specificity of the risk score \\(g(\\mathbf{x}_{i})\\) with the threshold \\(t\\) is defined as the probability you “predict” \\(Y_{i} = 0\\) assuming that \\(Y_{i}\\) is, in fact, equal to \\(0\\). The specificity is defined as: \\[\\begin{eqnarray} \\textrm{Specificity}(t; g) &amp;=&amp; P\\big\\{ g(\\mathbf{x}_{i}) &lt; t| Y_{i} = 0 \\big\\} \\nonumber \\\\ &amp;=&amp; \\frac{P\\big\\{ g(\\mathbf{x}_{i}) &lt; t, Y_{i} = 0 \\big\\}}{ P\\big\\{ g(\\mathbf{x}_{i}) &lt; t, Y_{i} = 0 \\big\\} + P\\big\\{ g(\\mathbf{x}_{i}) \\geq t, Y_{i} = 0 \\big\\}} \\end{eqnarray}\\] Note that \\(1 - \\textrm{Specificity}(t; g) = P\\big\\{ g(\\mathbf{x}_{i}) \\geq t| Y_{i} = 0 \\big\\}\\). \\(1 - \\textrm{Specificity}(t; g)\\) is often called the “false positive rate” You can compute the in-sample specificity with \\[\\begin{eqnarray} \\hat{\\textrm{Specificity}}(t; g) &amp;=&amp; \\frac{\\sum_{i=1}^{n} I\\big\\{ g(\\mathbf{x}_{i}) &lt; t, Y_{i} = 0 \\big\\} }{ \\sum_{i=1}^{n} I\\big\\{g(\\mathbf{x}_{i}) &lt; t, Y_{i} = 0 \\big\\} + \\sum_{i=1}^{n} I\\big\\{ g(\\mathbf{x}_{i}) \\geq t, Y_{i} = 0 \\big\\}} \\nonumber \\\\ &amp;=&amp; \\frac{\\textrm{number of true negatives}}{\\textrm{number of true negatives} + \\textrm{number of false positives} } \\end{eqnarray}\\] For a worthless risk score that is totally uninformative about the outcome, we should expect the specificity to be close to \\(P\\{ g(\\mathbf{x}_{i}) &lt; t \\}\\). Note that high values of both sensitivity and specificity is good. For a “perfect” risk score, both sensitivity and specificity would be equal to \\(1\\). 5.2.2 The ROC curve The receiver operating characteristic (ROC) curve graphically depicts how sensitivity and specificity change as the threshold \\(t\\) varies. Let \\(t_{i} = g(\\mathbf{x}_{i})\\) and let \\(t_{(1)} &gt; t_{(2)} &gt; ... &gt; t_{(n)}\\) be the ordered values of \\(t_{1}, \\ldots, t_{n}\\). To construct an ROC curve we are going to plot sentivity vs. 1 - specificity for each of the thresholds \\(t_{(1)}, \\ldots, t_{(n)}\\). Let \\(x_{i} = 1 - \\hat{\\textrm{Specificity}}(t_{(i)}; g)\\) and \\(y_{i} = \\hat{\\textrm{Sensitivity}}(t_{(i)}; g)\\). We will define \\(x_{0} = y_{0} = 0\\) and \\(x_{n+1} = y_{n+1} = 1\\). \\(x_{0}\\), \\(y_{0}\\) represent 1 - specificity and sensitivity when using \\(t = \\infty\\). \\(x_{n+1}\\), \\(y_{n+1}\\) represent 1 - specificity and sensitivity when using \\(t = -\\infty\\). Plotting \\(y_{i}\\) vs. \\(x_{i}\\) for \\(i = 0, \\ldots, n+1\\) will give you the ROC curve. Ideally, the values of \\(y_{i}\\) will close to \\(1\\) for all \\(i\\). For a worthless risk score we should expect both \\(y_{i}\\) and \\(x_{i}\\) to be roughly equal to \\(P(g(\\mathbf{x}_{i}) \\geq t)\\). Hence, plotting \\(y_{i}\\) vs. \\(x_{i}\\) should be fairly close to the line \\(y=x\\). 5.2.3 Computing the ROC curve To try computing an ROC curve, we will use the Wisconsin Breast Cancer dataset which is available in the biopsy dataset from the MASS package. library(MASS) data(biopsy) head(biopsy) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 1 1000025 5 1 1 1 2 1 3 1 1 benign ## 2 1002945 5 4 4 5 7 10 3 2 1 benign ## 3 1015425 3 1 1 1 2 2 3 1 1 benign ## 4 1016277 6 8 8 1 3 4 3 7 1 benign ## 5 1017023 4 1 1 3 2 1 3 1 1 benign ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant ## Look at number of &quot;benign&quot; and &quot;malignant&quot; tumors table(biopsy$class) ## ## benign malignant ## 458 241 biopsy$tumor.type &lt;- ifelse(biopsy$class==&quot;malignant&quot;, 1, 0) table(biopsy$tumor.type) ## ## 0 1 ## 458 241 Let’s compute risk scores for tumor malignancy by using a logistic regression model with class as the outcome and variables V1, V3, V4, V7, V8 as the covariates. Our risk score for the \\(i^{th}\\) case, will be the predicted probability of having a malignant tumor given the covariate information logreg.model &lt;- glm(tumor.type ~ V1 + V3 + V4 + V7 + V8, family=&quot;binomial&quot;, data=biopsy) risk.score &lt;- logreg.model$fitted.values Let’s now compute the sensitivity and specificity for a threshold of \\(t = 0.5\\) Sensitivity &lt;- function(thresh, Y, risk.score) { sum((risk.score &gt;= thresh)*Y)/sum(Y) } Specificity &lt;- function(thresh, Y, risk.score) { sum((risk.score &lt; thresh)*(1 - Y))/sum(1 - Y) } Sensitivity(0.5, Y=biopsy$tumor.type, risk.score) ## [1] 0.9502075 Specificity(0.5, Y=biopsy$tumor.type, risk.score) ## [1] 0.9759825 For the threshold of \\(t = 0.5\\), we have a sensitivity of about \\(0.95\\) and a specificity of about \\(0.975\\). Now, let’s compute the ROC curve by computing sensitivity and specificity for each risk score (plus the values of 0 and 1). sorted.riskscores &lt;- c(1, sort(risk.score, decreasing=TRUE), 0) mm &lt;- length(sorted.riskscores) roc.y &lt;- roc.x &lt;- rep(0, mm) for(k in 1:mm) { thresh.val &lt;- sorted.riskscores[k] roc.y[k] &lt;- Sensitivity(thresh.val, Y=biopsy$tumor.type, risk.score) roc.x[k] &lt;- 1 - Specificity(thresh.val, Y=biopsy$tumor.type, risk.score) } plot(roc.x, roc.y, main=&quot;ROC curve for biopsy data&quot;, xlab=&quot;1 - Specificity&quot;, ylab=&quot;Sensitivity&quot;, las=1) lines(roc.x, roc.y, type=&quot;s&quot;) abline(0, 1) Let’s compare the logistic regression ROC curve with a worthless risk score where we generate risk scores randomly from a uniform distribution. rr &lt;- runif(nrow(biopsy)) sorted.rr &lt;- c(1, sort(rr, decreasing=TRUE), 0) mm &lt;- length(sorted.rr) roc.random.y &lt;- roc.random.x &lt;- rep(0, mm) for(k in 1:mm) { thresh.val &lt;- sorted.rr[k] roc.random.y[k] &lt;- Sensitivity(thresh.val, Y=biopsy$tumor.type, rr) roc.random.x[k] &lt;- 1 - Specificity(thresh.val, Y=biopsy$tumor.type, rr) } plot(roc.random.x, roc.random.y, main=&quot;ROC curve for biopsy data&quot;, xlab=&quot;1 - Specificity&quot;, ylab=&quot;Sensitivity&quot;, las=1, col=&quot;red&quot;, type=&quot;n&quot;) lines(roc.random.x, roc.random.y, type=&quot;s&quot;, col=&quot;red&quot;) lines(roc.x, roc.y, type=&quot;s&quot;) legend(&quot;bottomright&quot;, legend=c(&quot;logistic regression risk scores&quot;, &quot;random risk scores&quot;), col=c(&quot;black&quot;, &quot;red&quot;), lwd=2, bty=&#39;n&#39;) abline(0, 1) 5.3 Area under the ROC curve The Area Under the ROC curve AUC is the area under the graph of the points \\((x_{i}, y_{i})\\), \\(i = 0, \\ldots, n+1\\). The AUC is given by \\[\\begin{equation} AUC = \\sum_{i=0}^{n} y_{i}(x_{i+1} - x_{i}) \\end{equation}\\] 5.3.1 Rewriting the formula for the AUC Note that \\[\\begin{equation} y_{i} = \\sum_{k=1}^{n} I\\{ g(\\mathbf{x}_{k}) \\geq t_{(i)}\\} I\\{ Y_{k} = 1 \\}\\Big/\\sum_{k=1}^{n} I\\{ Y_{k} = 1\\} = \\frac{1}{n\\hat{p}}\\sum_{k=1}^{n} a_{ki}, \\end{equation}\\] where \\(n\\hat{p} = \\sum_{k=1}^{n} I\\{ Y_{k} = 1\\}\\) and \\(a_{ki} = I\\{ g(\\mathbf{x}_{k}) \\geq t_{(i)}\\} I\\{ Y_{k} = 1 \\}\\). Because \\(t_{(i)} \\geq t_{(i+1)}\\): \\[\\begin{eqnarray} x_{i+1} - x_{i} &amp;=&amp; \\sum_{k=1}^{n} \\Big(I\\{ g(\\mathbf{x}_{k}) \\geq t_{(i+1)}, Y_{k} = 0 \\} - I\\{ g(\\mathbf{x}_{k}) \\geq t_{(i)}, Y_{k} = 0 \\}\\Big)\\Big/\\sum_{k=1}^{n} I\\{ Y_{k} = 0\\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n(1 - \\hat{p})}\\sum_{k=1}^{n} I\\{g(\\mathbf{x}_{k}) &lt; t_{(i)}\\} I\\{ g(\\mathbf{x}_{k}) \\geq t_{(i+1)}\\} I\\{Y_{k} = 0 \\} \\\\ &amp;=&amp; \\frac{1}{n(1 - \\hat{p})}\\sum_{k=1}^{n} I\\{ g(\\mathbf{x}_{k}) = t_{(i+1)}\\} I\\{Y_{k} = 0 \\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n(1 - \\hat{p})}\\sum_{k=1}^{n} b_{ki} \\end{eqnarray}\\] where \\(n(1 - \\hat{p}) = \\sum_{k=1}^{n} I\\{ Y_{k} = 0\\}\\) and \\(b_{ki} = I\\{ g(\\mathbf{x}_{k}) = t_{(i+1)}\\} I\\{Y_{k} = 0 \\}\\). So, we can express the AUC as: \\[\\begin{eqnarray} AUC &amp;=&amp; \\frac{1}{n^{2}\\hat{p}(1 - \\hat{p})}\\sum_{i=0}^{n} \\sum_{k=0}^{n} a_{ki} \\sum_{k=0}^{n} b_{ki} = \\frac{1}{n^{2}\\hat{p}(1 - \\hat{p})} \\sum_{k=0}^{n} \\sum_{j=0}^{n}\\sum_{i=0}^{n} a_{ki} b_{ji} \\\\ &amp;=&amp; \\frac{1}{n^{2}\\hat{p}(1 - \\hat{p})} \\sum_{k=0}^{n} \\sum_{j=0}^{n} I\\{ Y_{k} = 1 \\} I\\{Y_{j} = 0 \\} \\sum_{i=0}^{n} I\\{ g(\\mathbf{x}_{k}) \\geq t_{(i)}\\}I\\{ g(\\mathbf{x}_{j}) = t_{(i+1)}\\} \\tag{5.1} \\end{eqnarray}\\] Note now that because the \\(t_{(i)}\\) are the ordered values of the \\(g(\\mathbf{x}_{h})\\) \\[\\begin{equation} \\sum_{i=0}^{n} I\\{ g(\\mathbf{x}_{k}) \\geq t_{(i)}\\}I\\{ g(\\mathbf{x}_{j}) = t_{(i+1)}\\} = I\\{ g(\\mathbf{x}_{k}) \\geq t_{(j^{*} - 1)}\\} = I\\{ g(\\mathbf{x}_{k}) &gt; t_{(j^{*})}\\}, \\end{equation}\\] where \\(j^{*}\\) is the index such that \\(t_{(j^{*})} = g(\\mathbf{x}_{j})\\). Hence, \\[\\begin{equation} \\sum_{i=0}^{n} I\\{ g(\\mathbf{x}_{k}) \\geq t_{(i)}\\}I\\{ g(\\mathbf{x}_{j}) = t_{(i+1)}\\} = I\\{ g(\\mathbf{x}_{k}) &gt; g(\\mathbf{x}_{j}) \\} \\tag{5.2} \\end{equation}\\] Now, by plugging (5.2) into (5.1), we can finally express the AUC as \\[\\begin{eqnarray} AUC &amp;=&amp; \\frac{1}{n^{2}\\hat{p}(1 - \\hat{p})} \\sum_{k=0}^{n} \\sum_{j=0}^{n} I\\{ g(\\mathbf{x}_{k}) &gt; g(\\mathbf{x}_{j}) \\}I\\{ Y_{k} = 1 \\} I\\{Y_{j} = 0 \\} \\end{eqnarray}\\] 5.3.2 Interpreting the AUC We can write the AUC as \\[\\begin{equation} \\textrm{AUC} = S_{1}/S_{2} \\end{equation}\\] The sum \\(S_{2}\\) counts the number of all discordant pairs of responses The pair of outcomes \\((Y_{k}, Y_{j})\\) is discordant if \\(Y_{k} = 1\\) and \\(Y_{j}=0\\) or vice versa. \\[\\begin{equation} S_{2} = \\sum_{i=1}^{n}I\\{ Y_{i} = 0\\}\\sum_{i=1}^{n} I\\{Y_{i} = 1\\} = \\sum_{j=1}^{n}\\sum_{k=1}^{n} I\\{ Y_{j} = 0\\} I\\{Y_{k} = 1\\} \\end{equation}\\] Now, look at the sum \\(S_{1}\\): \\[\\begin{eqnarray} S_{1} &amp;=&amp; \\sum_{k=0}^{n} \\sum_{j=0}^{n} I\\{ g(\\mathbf{x}_{k}) &gt; g(\\mathbf{x}_{j}) \\}I\\{ Y_{k} = 1 \\} I\\{Y_{j} = 0 \\} \\end{eqnarray}\\] \\(S_{1}\\) looks at all discordant pairs of responses and counts the number of pairs where the risk score ordering agrees with the ordering of the responses. To summarize: The AUC is the proportion of discordant outcome pairs where the risk score ordering for that pair agrees with the ordering of the outcomes An AUC of \\(0.5\\) means that the risk score is performing about the same as a risk score generated at random. The AUC is often referred to as the concordance index, or c-index. Let’s compute the AUC for our logistic regression-based risk score for the biopsy data: AUC.biopsy &lt;- sum(roc.y[-length(roc.y)]*diff(roc.x)) round(AUC.biopsy, 4) ## [1] 0.9928 This is a very high AUC: \\(0.9928\\) 5.3.3 Computing the AUC in R You can compute the AUC in R without writing all your own functions by using the pROC package. With the pROC package, you start by inputting your binary outcomes and risk scores into the roc function in order to get an “roc object”. library(pROC) roc.biopsy &lt;- roc(biopsy$tumor.type, risk.score) To find the AUC, you can then use auc(roc.biopsy) auc( roc.biopsy ) ## Area under the curve: 0.9928 You can plot the ROC curve by just plugging in the roc object into the plot function. plot(roc.biopsy) To also print the AUC value, you can just add print.auc=TRUE plot(roc.biopsy, print.auc=TRUE) 5.4 Calibration The AUC, or c-index is a measure of the statistical discrimination of the risk score. However, a high value of the AUC does not imply that the risk score is well-calibrated. Calibration refers to the agreement between observed frequency of the outcome and the fitted probabilities of those outcomes. For example, if we look at a group of individuals all of whom have a fitted probability of \\(0.2\\), then we should expect that roughly \\(20\\%\\) of the outcomes should equal \\(1\\) from this group. We can examine this graphically by looking at the observed proportion of successes vs. the fitted probabilities for several risk strata. Specifically, if we have risk score-cutoffs \\(r_{1}, \\ldots, r_{G}\\), the observed proportion of successes in the \\(k^{th}\\) risk stratum is \\[\\begin{equation} O_{k} = \\sum_{i=1}^{n} Y_{i}I\\{ r_{k-1} &lt; g(\\mathbf{x}_{i}) \\leq r_{k} \\} \\Big/\\sum_{i=1}^{n} I\\{ r_{k-1} &lt; g(\\mathbf{x}_{i}) \\leq r_{k} \\} \\end{equation}\\] The expected proportion of successes in the \\(k^{th}\\) risk stratum is \\[\\begin{equation} P_{k} = \\sum_{i=1}^{n} g(\\mathbf{x}_{i})I\\{ r_{k-1} &lt; g(\\mathbf{x}_{i}) \\leq r_{k} \\} \\Big/\\sum_{i=1}^{n} I\\{ r_{k-1} &lt; g(\\mathbf{x}_{i}) \\leq r_{k} \\} \\end{equation}\\] If \\(g(\\mathbf{x}_{i})\\) is well-calibrated, \\(O_{k}\\) and \\(P_{k}\\) should be fairly similar for each \\(k\\). Let’s make a calibration plot for the biopsy data. First, let’s make 10 risk strata using the quantiles of our logistic regression-based risk score. rr &lt;- c(0, quantile(risk.score, prob=seq(0.1, 0.9, by=0.1)), 1) rr ## 10% 20% 30% 40% 50% ## 0.000000000 0.001505966 0.002756161 0.005385182 0.010154320 0.019231612 ## 60% 70% 80% 90% ## 0.074574998 0.841857858 0.989561027 0.999471042 1.000000000 Now, compute observed and expected frequencies for each of the risk strata and plot the result: observed.freq &lt;- pred.freq &lt;- rep(0, 10) for(k in 2:11) { ind &lt;- risk.score &lt;= rr[k] &amp; risk.score &gt; rr[k-1] # stratum indicators observed.freq[k] &lt;- sum(biopsy$tumor.type[ind])/sum(ind) pred.freq[k] &lt;- sum(risk.score[ind])/sum(ind) } plot(observed.freq, pred.freq, xlab=&quot;Observed Frequency&quot;, ylab = &quot;Predicted Frequency&quot;, las=1, main=&quot;Calibration plot for biopsy data&quot;) lines(observed.freq, pred.freq) abline(0, 1, lty=2) Most of the risk scores are more concentrated near zero, but this calibration plot shows fairly good calibration. Expected vs. Observed frequencies are mostly close to the \\(y = x\\) straight line. Sometimes an estimated intercept and slope from a regression of expected vs. observed frequencies is reported. We should expect that the intercept should be close to \\(0\\) and the slope should be close to \\(1\\) for a well-calibrated risk score. lm(pred.freq ~ observed.freq) ## ## Call: ## lm(formula = pred.freq ~ observed.freq) ## ## Coefficients: ## (Intercept) observed.freq ## 0.001759 0.994948 The Hosmer–Lemeshow test is a more formal test that compares these types of observed vs. expected frequencies. 5.5 Longitudinal Data and Risk Score Validation When you have longitudinal responses \\(Y_{ij}\\), sensitivity, specificity, and the AUC/c-index will depend on what exactly you are trying to predict. If you are trying to predict \\(Y_{ij} = 1\\) vs. \\(Y_{ij} = 0\\) for each \\(j\\), sensitivity and specificity will be defined by \\[\\begin{equation} P\\{ g(\\mathbf{x}_{ij}) \\geq t|Y_{ij} = 1\\} \\qquad \\textrm{ and } \\qquad P\\{ g(\\mathbf{x}_{ij}) &lt; t|Y_{ij} = 0\\} \\end{equation}\\] Here, \\(g(\\mathbf{x}_{ij})\\) would be a risk score based on covariate information up to and including time \\(t_{ij}\\) and could include responses before time \\(t_{ij}\\). In this case, the AUC would be calculated in the same way as the non-longitudinal case - you would just sum over all responses and risk scores across all individuals and time points. In other cases, you may want to predict a single outcome \\(\\tilde{Y}_{i}\\) even though the covariates are collected longitudinally over additional time points. For example, \\(\\tilde{Y}_{i}\\) might be an indicator of whether or not a patient is hypertensive over a particular time window. In this case, you might compute a single risk score \\(\\hat{g}_{i}\\) for each individual. For example, \\(\\hat{g}_{i} = \\frac{1}{n_{i}}\\sum_{j=1}^{n_{i}} g(\\mathbf{x}_{ij})\\). For example, \\(\\hat{g}_{i} = \\textrm{median} \\{ g(\\mathbf{x}_{i1}), \\ldots, g(\\mathbf{x}_{in_{i}}) \\}\\). For example, \\(\\hat{g}_{i} = \\max\\{ g(\\mathbf{x}_{i1}), \\ldots, g(\\mathbf{x}_{in_{i}}) \\}\\). To “validate” a risk score, you generally want to look at out-of-sample performance of the risk score. For example, you might build your risk model using data from individuals enrolled within a specific six-month time window and look at the AUC statistic for outcomes for individuals who enrolled in a different time window. Looking at the out-of-sample performance over a different time window or using data from a different source is often a good way of justifying the robustness/generalizability of a particular risk model. "],["risk-prediction2.html", "Chapter 6 Risk Prediction and Validation (Part II) 6.1 The Brier Score 6.2 Brier Scores with Longitudinal Data", " Chapter 6 Risk Prediction and Validation (Part II) 6.1 The Brier Score Binary outcomes - \\(Y_{i}\\) for individual \\(i\\). Let \\(g(\\mathbf{x}_{i})\\) be a risk score for an individual with covariate vector \\(\\mathbf{x}_{i}\\). If \\(g(\\mathbf{x}_{i})\\) is interpreted as an estimate of the probability that \\(Y_{i} = 1\\), the Brier score is a measure of the predictive accuracy of \\(g(\\mathbf{x}_{i})\\). For binary outcomes \\(Y_{i}\\) and risk scores \\(g(\\mathbf{x}_{i})\\), the Brier score is defined as \\[\\begin{equation} BS(Y, g) = \\frac{1}{n}\\sum_{i=1}^{n} \\{ Y_{i} - g(\\mathbf{x}_{i}) \\}^{2} \\end{equation}\\] The Brier score is typically used to compare the accuracy of risk scores. It is hard to tell if a risk score is good just by looking at the Brier score itself without comparing it with other methods. Even if \\(g(\\mathbf{x}_{i})\\) is a class prediction rather than a probability (i.e., \\(g(\\mathbf{x}_{i}) = 0\\) or \\(g(\\mathbf{x}_{i}) = 1\\)), the Brier score can be interpreted as the proportion of “misclassified” outcomes \\[\\begin{equation} BS(Y, g) = \\frac{1}{n}\\sum_{i=1}^{n} \\{ Y_{i} - g(\\mathbf{x}_{i}) \\}^{2} = \\frac{1}{n} \\sum_{i=1}^{n} I\\{Y_{i} \\neq g(\\mathbf{x}_{i}) \\} \\end{equation}\\] The Brier score is a single measure that is affected by both the discrimination and calibration of the risk score. The usual justification for this is to look at the following approximation of the Brier score \\[\\begin{equation} BS(Y, g) \\approx \\tilde{BS}(Y,g) = \\frac{1}{n}\\sum_{s=1}^{S} \\sum_{i=1}^{n} a_{is}\\{Y_{i} - \\bar{g}_{s} \\}^{2} \\end{equation}\\] This approximation assumes \\(S\\) strata: \\([s_{0}, s_{1}), [s_{1}, s_{2}), \\ldots, [s_{S-1}, s_{S})\\). \\(a_{is}\\) is an indicator of whether or not the \\(i^{th}\\) observation falls into the \\(s^{th}\\) stratum: That is, \\(a_{is} = 1\\) if \\(s_{s-1} \\leq g(\\mathbf{x}_{i}) &lt; s_{s}\\) \\(\\bar{g}_{s}\\) is the average value of the risk score within stratum \\(s\\) \\[\\begin{equation} \\bar{g}_{s} = \\frac{1}{n_{s}} \\sum_{i=1}^{n} a_{is} g(\\mathbf{x}_{i}) \\end{equation}\\] and \\(n_{s}\\) is the number of observation where the risk score is in stratum \\(s\\). If we let \\(\\bar{Y}_{s} = \\frac{1}{n_{s}}\\sum_{i=1}^{n} a_{is}Y_{i}\\) be the proportion of 1’s in stratum \\(s\\), we can simplify the expression for the approximate Brier score as follows: \\[\\begin{eqnarray} \\tilde{BS}(Y,g) &amp;=&amp; \\frac{1}{n}\\sum_{s=1}^{S} \\sum_{i=1}^{n} a_{is}\\{Y_{i} - \\bar{Y}_{s} + \\bar{Y}_{s} - \\bar{g}_{s} \\}^{2} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n}\\sum_{s=1}^{S} \\sum_{i=1}^{n} a_{is}\\{Y_{i} - \\bar{Y}_{s} \\}^{2} + \\frac{1}{n}\\sum_{s=1}^{S} n_{s}\\{\\bar{Y}_{s} - \\bar{g}_{s} \\}^{2} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n}\\sum_{s=1}^{S} n_{s}\\bar{Y}_{s}(1 - \\bar{Y}_{s}) + \\frac{1}{n}\\sum_{s=1}^{S} n_{s}\\{\\bar{Y}_{s} - \\bar{g}_{s} \\}^{2} \\nonumber \\\\ &amp;=&amp; B_{D} + B_{C} \\end{eqnarray}\\] The quantity \\(B_{D} = \\frac{1}{n}\\sum_{s=1}^{S} n_{s}\\bar{Y}_{s}(1 - \\bar{Y}_{s})\\) is related to the discrimination of the risk score \\(g\\). To get a sense of why \\(B_{D}\\) measures discrimination, suppose \\(s\\) is a stratum where \\(s_{s} \\leq t\\). If the risk score has good discrimination, this threshold should separate most of the “successes” from the “failures”. That is most with \\(g(\\mathbf{x}_{i}) \\leq t\\) has \\(Y_{i} = 0\\) and most with \\(g(\\mathbf{x}_{i}) &gt; t\\) has \\(Y_{i}=1\\). This \\(\\bar{Y}_{s}(1 - \\bar{Y}_{s})\\) is close to \\(0\\). If this is close to \\(0\\) for most threholds, \\(B_{D}\\) will be small. The quantity \\(B_{C} = \\frac{1}{n}\\sum_{s=1}^{S} n_{s}\\{\\bar{Y}_{s} - \\bar{g}_{s} \\}^{2}\\) is a measure of calibration. For a well-calibrated risk score, the proportion of successes \\(\\bar{Y}_{s}\\) in stratum \\(s\\) should be close to the average value of the risk score \\(g_{s}\\) in stratum \\(s\\). 6.1.1 Brier scores for biopsy data Let’s compute the Brier score using the biopsy data again from the MASS package. library(rpart) library(MASS) data(biopsy) # Create binary outcome for tumor class biopsy$tumor.type &lt;- ifelse(biopsy$class==&quot;malignant&quot;, 1, 0) head(biopsy) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class tumor.type ## 1 1000025 5 1 1 1 2 1 3 1 1 benign 0 ## 2 1002945 5 4 4 5 7 10 3 2 1 benign 0 ## 3 1015425 3 1 1 1 2 2 3 1 1 benign 0 ## 4 1016277 6 8 8 1 3 4 3 7 1 benign 0 ## 5 1017023 4 1 1 3 2 1 3 1 1 benign 0 ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 1 Let’s try constructing fitted probabilities using three approaches: logistic regression CART random forest library(rpart) library(randomForest) # Fit CART, logistic regression, and random forest cart.model &lt;- rpart(class ~ V1 + V3 + V4 + V7 + V8, data=biopsy) logreg.model &lt;- glm(tumor.type ~ V1 + V3 + V4 + V7 + V8, family=&quot;binomial&quot;, data=biopsy) RF.model &lt;- randomForest(class ~ V1 + V3 + V4 + V7 + V8, data=biopsy, ntree = 500) Let’s then compute “risk scores” from each method. probs.cart &lt;- predict(cart.model) head(probs.cart) ## benign malignant ## 1 0.9837587 0.0162413 ## 2 0.0952381 0.9047619 ## 3 0.9837587 0.0162413 ## 4 0.0952381 0.9047619 ## 5 0.9837587 0.0162413 ## 6 0.0952381 0.9047619 # We want the second column of this matrix for the CART fitted probabilities risk.cart &lt;- probs.cart[,2] # logistic regression risk.logreg &lt;- predict(logreg.model, newdata=biopsy, type=&quot;response&quot;) # random forest risk scores risk.RF &lt;- predict(RF.model, newdata=biopsy, type = &quot;prob&quot;)[,2] The in-sample Brier scores for each method are: “In-sample” here meaning we are computing the Brier score using the same outcomes we used to construct the risk scores. brier.cart &lt;- mean((risk.cart - biopsy$tumor.type)^2) brier.logreg &lt;- mean((risk.logreg - biopsy$tumor.type)^2) brier.RF &lt;- mean((risk.RF - biopsy$tumor.type)^2) round(c(brier.cart, brier.logreg, brier.RF), 4) ## [1] 0.0450 0.0280 0.0054 When comparing in-sample Brier scores, CART is the worst, logistic regression is in the middle, and random forest is the best. Figure 6.1: Fitted CART model for the biopsy data 6.1.2 Out-of-sample comparisons Looking at out-of-sample performance is a better way to validate our risk scores. Let’s try a validation exercise by “training” our models on the first \\(400\\) observations of biopsy and then testing relative performance on the remaining observations. # Create train/test splits for biopsy data biopsy.train &lt;- biopsy[1:400,] biopsy.test &lt;- biopsy[401:699,] # Now, use each type of method on this training data cart.model.train &lt;- rpart(class ~ V1 + V3 + V4 + V7 + V8, data=biopsy.train) logreg.model.train &lt;- glm(tumor.type ~ V1 + V3 + V4 + V7 + V8, family=&quot;binomial&quot;, data=biopsy.train) RF.model.train &lt;- randomForest(class ~ V1 + V3 + V4 + V7 + V8, data=biopsy.train, ntree = 500) Using these models built on the training data, get the fitted probabilities for the test data risk.cart.test &lt;- predict(cart.model.train, newdata=biopsy.test)[,2] risk.logreg.test &lt;- predict(logreg.model.train, newdata=biopsy.test, type=&quot;response&quot;) risk.RF.test &lt;- predict(RF.model.train, newdata=biopsy.test, type = &quot;prob&quot;)[,2] Now, using these fitted probabilities on the test data, compute the Brier scores: brier.cart.test &lt;- mean((risk.cart.test - biopsy.test$tumor.type)^2) brier.logreg.test &lt;- mean((risk.logreg.test - biopsy.test$tumor.type)^2) brier.RF.test &lt;- mean((risk.RF.test - biopsy.test$tumor.type)^2) round(c(brier.cart.test, brier.logreg.test, brier.RF.test), 4) ## [1] 0.0359 0.0135 0.0181 For the out-of-sample Brier score, both logistic regression and random forest are notably better than CART. Logistic regression is actually slightly better than random forest in out of sample performance with random forest (at least for this particular train/test split). 6.2 Brier Scores with Longitudinal Data Let’s revisit the ohio data again from the geepack package library(geepack) data(ohio) head(ohio, 10) ## resp id age smoke ## 1 0 0 -2 0 ## 2 0 0 -1 0 ## 3 0 0 0 0 ## 4 0 0 1 0 ## 5 0 1 -2 0 ## 6 0 1 -1 0 ## 7 0 1 0 0 ## 8 0 1 1 0 ## 9 0 2 -2 0 ## 10 0 2 -1 0 Each individual has 4 follow-up visits. The variable resp is the wheezing status of the individual at each follow up. The variable smoke is a baseline variable that does not change over time. This is an indicator of maternal smoking. age is a time-varying covariate. 6.2.1 Option 1 If we want to build a risk score for wheezing, evaluating the performance of this risk score will depend on exactly what we are trying to predict. Option 1: We want to predict whether or not some someone is diagnosed with wheezing over some time window, and we do not want to predict wheezing status at each time point. As an example of this, let’s build a risk score which: Only uses smoke as a covariate. Builds the risk score from the first two follow-up times. Let’s first construct a dataset called atleastone1 that records whether or not an individual had at least one positive wheezing status over the first two visits: atleastone2 will record whether or not an individual has at least one positive wheezing status over the last two visits. data(ohio) baseline.ohio &lt;- subset(ohio, age== -1) # data from baseline visit firsttwo.ohio &lt;- subset(ohio, age== -1 | age == 0) # data from visits 1-2 lasttwo.ohio &lt;- subset(ohio, age==1 | age==2) # data from visits 3-4 tmp &lt;- aggregate(firsttwo.ohio$resp, by=list(id=firsttwo.ohio$id), FUN=max) tmp2 &lt;- aggregate(lasttwo.ohio$resp, by=list(id=lasttwo.ohio$id), FUN=max) atleastone1 &lt;- merge(tmp, baseline.ohio, by=&quot;id&quot;) atleastone2 &lt;- merge(tmp2, baseline.ohio, by=&quot;id&quot;) head(atleastone1) ## id x resp age smoke ## 1 0 0 0 -1 0 ## 2 1 0 0 -1 0 ## 3 2 0 0 -1 0 ## 4 3 0 0 -1 0 ## 5 4 0 0 -1 0 ## 6 5 0 0 -1 0 table(atleastone1$resp) ## ## 0 1 ## 446 91 Let’s now build a risk score using only variable smoke as a covariate and a random choice of \\(300\\) observations set.seed(1234) train.ind &lt;- sample(1:537, size=300) atleastone1.train &lt;- atleastone1[train.ind,] mod1 &lt;- glm(resp ~ smoke, family=&quot;binomial&quot;, data=atleastone1.train) summary(mod1) ## ## Call: ## glm(formula = resp ~ smoke, family = &quot;binomial&quot;, data = atleastone1.train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5978 -0.5978 -0.5537 -0.5537 1.9754 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.7979 0.2078 -8.653 &lt;2e-16 *** ## smoke 0.1665 0.3311 0.503 0.615 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 253.63 on 299 degrees of freedom ## Residual deviance: 253.37 on 298 degrees of freedom ## AIC: 257.37 ## ## Number of Fisher Scoring iterations: 4 The risk scores for the hold-out individuals for the later follow-up times are given by risk1 &lt;- predict(mod1, newdata=atleastone2[-train.ind,], type=&quot;response&quot;) Now, let’s look at the Brier score on the remaining individuals in the atleast2 data set. mean((risk1 - atleastone2[-train.ind,]$resp)^2) ## [1] 0.1574466 An AUC probably does not make much sense here since there are only two possible values of the risk score. For calibration, we can just compare the predicted risk scores and the mean number of successes in each of the smoking categories in the hold-out sample: table(risk1) ## risk1 ## 0.142105263157904 0.163636363636364 ## 160 77 table(atleastone2[-train.ind,]$resp, atleastone2[-train.ind,]$smoke) ## ## 0 1 ## 0 135 56 ## 1 25 21 # Compare risk scores with 25/(25 + 135) ## [1] 0.15625 21/(21 + 56) ## [1] 0.2727273 This risk score does not seem all that well-calibrated for later time points. The overall prevalence of wheezing is a little bit higher in the hold-out sample. There is a larger difference between the smoking and non-smoking groups in the hold-out sample. 6.2.2 Option 2 As another option, you may want to predict outcomes for each time point in the hold-out dataset. In this case, I would probably just compute a Brier score by just averaging over all time points. For example, with a GEE, you might do the following: ind &lt;- c(1:400, 801:1200, 1601:2000) ## index of training set ohio.train &lt;- subset(ohio[ind,], age==-1 | age == 0) ohio.test &lt;- subset(ohio[-ind,], age==1 | age==2) fit.ex &lt;- geeglm(resp ~ age + smoke + age:smoke, id=id, data=ohio.train, family=binomial, corstr=&quot;exch&quot;) rs2 &lt;- predict(fit.ex, newdata=ohio.test, type=&quot;response&quot;) ## Now, find Brier score that the takes average across all individuals and time points mean((rs2 - ohio.test$resp)^2) ## [1] 0.1270949 "],["ordinal-regression.html", "Chapter 7 Ordinal Regression 7.1 Ordinal Logistic Regression 7.2 Ordinal Regression Details 7.3 Generalized Estimating Equations 7.4 Penalized Regression with Ordinal Outcomes", " Chapter 7 Ordinal Regression 7.1 Ordinal Logistic Regression In many applications, the possible values of the response should be thought of as ordered categories. Examples: Survey ratings of a product: “poor”, “good”, “excellent”. Patient reported scale of pain from 1-10. Different possible responses on a questionnaire. 7.2 Ordinal Regression Details With ordinal regression, only the ranking of the different categories is relevant. Let \\(Y_{i}\\) represent the ordinal response of interest. Assume \\(Y_{i}\\) can take an integer value from 1 to C: \\(Y_{i} = 1\\), or \\(Y_{i} = 2\\), …, or \\(Y_{i} = C\\). Although we are assuming \\(Y_{i}\\) takes numerical values, the ordinal regression analysis would be the same if we assumed that \\(Y_{i}\\) could take values \\(2, 4, 6, ..., 2C\\). The most common regression model for ordinal data is an ordinal logistic regression model. The ordinal logistic regression model uses a regression model for the “lesser-than probabilities” \\(P( Y_{i} \\leq c|\\mathbf{x}_{i})\\). Specifically, \\[\\begin{equation} P(Y_{i} \\leq c|\\mathbf{x}_{i}) = \\frac{1}{1 + \\exp(-\\alpha_{c} + \\mathbf{x}_{i}^{T}\\boldsymbol{\\beta})}, \\quad c = 1, \\ldots, C-1 \\end{equation}\\] This is equivalent to saying that the log-odds (for the event \\(\\{Y_{i} \\leq c\\}\\)) is a linear function of the covariates: \\[\\begin{equation} \\log \\{ \\text{odds}_{c}( \\mathbf{x}_{i} ) \\} = \\log\\left( \\frac{ P(Y_{i} \\leq c|\\mathbf{x}_{i}) }{ 1 - P(Y_{i} \\leq c|\\mathbf{x}_{i}) } \\right) = \\alpha_{c} - \\mathbf{x}_{i}^{T}\\boldsymbol{\\beta} \\tag{7.1} \\end{equation}\\] Note that the ordinal logistic regression model is often expressed using the probabilities \\(P(Y_{i} &gt; c|\\mathbf{x}_{i})\\) instead of \\(P( Y_{i} &gt; c|\\mathbf{x}_{i})\\) where it is assumed that \\[\\begin{equation} \\log\\left( \\frac{ P(Y_{i} &gt; c|\\mathbf{x}_{i}) }{ 1 - P(Y_{i} &gt; c|\\mathbf{x}_{i}) } \\right) = \\alpha_{c} + \\mathbf{x}_{i}^{T}\\boldsymbol{\\beta} \\end{equation}\\] The regression model (7.1) is often called a proportional odds model. This is because the odds ratio for the probability \\(P(Y \\leq c|\\mathbf{x}_{i})\\) vs. the probability \\(P(Y \\leq c|\\mathbf{z}_{i})\\) does not depend on the value of \\(c\\). To see the proportional odds property, note that \\[\\begin{equation} \\frac{ \\text{odds}_{c}( \\mathbf{x}_{i} ) }{ \\text{odds}_{c}( \\mathbf{z}_{i} ) } = \\frac{ \\exp( \\alpha_{c} - \\mathbf{x}_{i}^{T}\\boldsymbol{\\beta} ) }{ \\exp( \\alpha_{c} - \\mathbf{z}_{i}^{T}\\boldsymbol{\\beta} ) } = \\exp\\left[ (\\mathbf{z}_{i}^{T} - \\mathbf{x}_{i}^{T})\\boldsymbol{\\beta} \\right] \\end{equation}\\] The odds ratio only depends on \\(\\boldsymbol{\\beta}\\) and does not depend on the value of \\(\\alpha_{c}\\). The proportional odds assumption allows the elements of \\(\\boldsymbol{\\beta}\\) to have an interpretation that does not depend on \\(c\\). Interpretation: If you have only one covariate \\(x_{i}\\), then a one-unit increase in \\(x_{i}\\) leads to an increase of \\(\\beta\\) in the log-odds ratio (for the event \\(\\{Y_{i}\\leq c\\}\\)): \\[\\begin{equation} \\log\\left( \\frac{ \\text{odds}_{c}( x_{i} + 1 ) }{ \\text{odds}_{c}( x_{i} ) } \\right) = -\\beta \\end{equation}\\] The model (7.1) uses a logit link function, but it’s possible to choose any other link function. Common alternative choices are the complementary log-log and probit link functions. 7.2.1 Ordinal Logistic Regression in R The polr function in the MASS package let’s you fit ordered logistic or ordered probit regression models. The example dataset in the polr documentation is the housing data library(MASS) # load MASS package head(housing) # look at first 6 rows ## Sat Infl Type Cont Freq ## 1 Low Low Tower Low 21 ## 2 Medium Low Tower Low 21 ## 3 High Low Tower Low 28 ## 4 Low Medium Tower Low 34 ## 5 Medium Medium Tower Low 22 ## 6 High Medium Tower Low 36 dim(housing) # 72 rows and 5 variables ## [1] 72 5 The syntax for using polr is similar to other common regression-fitting functions in R such as lm or glm. The main thing to be aware of when using polr is that the response variable should be an ordered factor. str(housing) ## &#39;data.frame&#39;: 72 obs. of 5 variables: ## $ Sat : Ord.factor w/ 3 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 1 2 3 1 2 3 1 2 3 1 ... ## $ Infl: Factor w/ 3 levels &quot;Low&quot;,&quot;Medium&quot;,..: 1 1 1 2 2 2 3 3 3 1 ... ## $ Type: Factor w/ 4 levels &quot;Tower&quot;,&quot;Apartment&quot;,..: 1 1 1 1 1 1 1 1 1 2 ... ## $ Cont: Factor w/ 2 levels &quot;Low&quot;,&quot;High&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Freq: int 21 21 28 34 22 36 10 11 36 61 ... The response variable of interest here is Sat which represents householder satisfaction. This has 3 possible responses: “Low”, “Medium”, or “High”. This is a categorical variable with a natural ordering (“Low” is worse than “Medium” which is worse than “High”). table(housing$Sat) ## ## Low Medium High ## 24 24 24 The categorial variables Sat, Infl, Type, and Cont don’t have the “individual-level” data. Each row just represents one of the 72 possible configurations of these variables. The variable Freq actually tells us how many responses we had for each of the 72 categories. To fit this without using the individual-level data, we can use the values of Freq as weights in an ordinal logistic regression. If we want to fit an ordinal logistic regression with Sat as the response and Infl as a covariate, we can use the following code: infl.plr &lt;- polr(Sat ~ Infl, weight=Freq, data = housing) The above R code is going to fit the model \\[\\begin{equation} \\log \\left( \\frac{ P(Y_{i} \\leq c|\\mathbf{x}_{i}) }{1 - P(Y_{i} \\leq c|\\mathbf{x}_{i})} \\right) = \\alpha_{c} - \\beta_{1}x_{i1} - \\beta_{2}x_{i2}, \\end{equation}\\] \\(Y_{i}\\) is the satisfaction level with three levels \\(x_{i1} = 1\\) if influence is medium and \\(0\\) otherwise \\(x_{i2} = 1\\) if influence is high and \\(0\\) otherwise. To see the estimates of these parameters use the summary function: summary(infl.plr) ## ## Re-fitting to get Hessian ## Call: ## polr(formula = Sat ~ Infl, data = housing, weights = Freq) ## ## Coefficients: ## Value Std. Error t value ## InflMedium 0.5636 0.1036 5.441 ## InflHigh 1.2487 0.1248 10.006 ## ## Intercepts: ## Value Std. Error t value ## Low|Medium -0.2014 0.0766 -2.6280 ## Medium|High 0.9485 0.0802 11.8199 ## ## Residual Deviance: 3543.416 ## AIC: 3551.416 The above coefficient estimates tell us that the odds of low satisfaction given Medium influence level vs. the odds of low satisfaction given a Low influence level is \\[\\begin{equation} \\frac{odds_{c}(\\text{Medium Influence})}{odds_{c}(\\text{Low Influence})} = \\exp(-\\hat{\\beta}_{1}) = \\exp(-0.563) \\approx 0.57 \\end{equation}\\] In other words, the odds of having low satisfaction are higher in the low influence group when compared to the high influence group. Similarly, the odds ratio for the high influence group vs. the low influence group is given by \\[\\begin{equation} \\frac{\\text{odds}_{c}(\\text{High Influence})}{\\text{odds}_{c}(\\text{Low Influence})} = \\exp(-\\hat{\\beta}_{2}) = \\exp(-1.248) \\approx 0.29 \\end{equation}\\] From this, you can get the odds ratio for high vs. medium influence levels \\[\\begin{equation} \\frac{\\text{odds}_{c}(\\text{High Influence})}{\\text{odds}_{c}(\\text{Medium Influence})} = \\exp(-1.248) \\exp(0.563) \\approx 0.5 \\end{equation}\\] The summary output from a polr fit will also give estimates of the “intercept terms” \\(\\alpha_{c}\\). Since there are only 3 possible levels of the response Sat, there should only be two possible values of \\(\\alpha_{c}\\). The estimates \\(\\hat{\\alpha}_{c}\\) are given in the Intercepts part of the summary output. The estimate of \\(\\hat{\\alpha}_{1}\\) means that the probability that satisfaction is low (i.e., \\(Y_{i} \\leq 1\\)) given low influence is \\[\\begin{equation} \\hat{P}(Y_{i} \\leq 1| \\text{InflLow}) = 1/[1 + \\exp(-\\hat{\\alpha}_{1})] \\approx 0.45 \\end{equation}\\] Similarly, the estimated probability that satisfaction is low given medium influence is \\[\\begin{equation} \\hat{P}(Y_{i} \\leq 1| \\text{InflMed}) = 1/[1 + \\exp(-\\hat{\\alpha}_{1} + \\hat{\\beta}_{1})] \\approx 0.31 \\end{equation}\\] In this case, because we are only looking at a single covariate, we can verify these probability estimates with a direct calculation. The estimate of \\(P(Y_{i} \\leq 1| \\text{InflLow})\\) should be equal to the following ratio \\[\\begin{equation} \\hat{P}(Y_{i} \\leq 1| \\text{InflLow}) = \\frac{ N_{satlow, infllow} }{ N_{infllow} } \\end{equation}\\] This can be computed with the R code N_ll &lt;- sum(housing$Freq[housing$Sat==&quot;Low&quot; &amp; housing$Infl==&quot;Low&quot;]) N_l &lt;- sum(housing$Freq[housing$Infl==&quot;Low&quot;]) N_ll/N_l ## [1] 0.4497608 A direct estimate of \\(\\hat{P}(Y_{i} \\leq 1| \\text{InflMed})\\) can be computed with the following R code N_lm &lt;- sum(housing$Freq[housing$Sat==&quot;Low&quot; &amp; housing$Infl==&quot;Medium&quot;]) N_m &lt;- sum(housing$Freq[housing$Infl==&quot;Medium&quot;]) N_lm/N_m ## [1] 0.3125948 7.2.2 The respdis data As another example, let’s look at the respdis data from the geepack package library(geepack) head(respdis) ## y1 y2 y3 y4 trt ## 1 1 1 1 1 1 ## 2 1 1 1 1 0 ## 3 1 1 1 1 0 ## 4 1 1 1 1 0 ## 5 1 1 1 1 0 ## 6 1 1 1 1 0 This is a longitudinal dataset. For this analysis, we will only look at the outcome at the first visit respdis_first &lt;- respdis[,c(1,5)] head(respdis_first) ## y1 trt ## 1 1 1 ## 2 1 0 ## 3 1 0 ## 4 1 0 ## 5 1 0 ## 6 1 0 The y1 variable represents an ordinal outcome (poor, good, or excellent) at the first visit The outcomes are coded as 1,2,3 in the respdis_first dataset table(respdis$y1) ## ## 1 2 3 ## 14 63 34 Let’s fit an ordered regression model using trt as the only covariate. We need to put ordered(y1) in the model formula since y1 is not stored as a factor in respdis_first respmod_first &lt;- polr(ordered(y1) ~ trt, data=respdis_first) The above R code is going to fit the model \\[\\begin{equation} \\log \\left( \\frac{ P(Y_{i} \\leq c|\\mathbf{x}_{i}) }{1 - P(Y_{i} \\leq c|\\mathbf{x}_{i})} \\right) = \\alpha_{c} - \\beta x_{i}, \\end{equation}\\] \\(Y_{i}\\) is the response level with three levels \\(x_{i} = 1\\) if received treatment, \\(x_{i} = 0\\) if received placebo Let’s look at the summary output from respmod_first: summary(respmod_first) ## ## Re-fitting to get Hessian ## Call: ## polr(formula = ordered(y1) ~ trt, data = respdis_first) ## ## Coefficients: ## Value Std. Error t value ## trt 0.4467 0.3743 1.194 ## ## Intercepts: ## Value Std. Error t value ## 1|2 -1.7302 0.3300 -5.2429 ## 2|3 1.0512 0.2885 3.6439 ## ## Residual Deviance: 208.3581 ## AIC: 214.3581 The estimate of \\(\\hat{\\beta}\\) implies that: \\[\\begin{equation} \\frac{\\text{odds}_{c}(\\text{Treatment})}{\\text{odds}_{c}(\\text{Placebo})} = \\exp(-\\hat{\\beta}) = \\exp(-0.447) \\approx 0.64 \\end{equation}\\] In this context, having a low odds of \\(Y_{i} \\leq c\\) (for c=1,2) is good. Hence, the above odds ratio suggests a benefit of treatment vs. placebo. Using the estimates of \\(\\alpha_{1}\\) and \\(\\alpha_{2}\\), you can get estimates of \\(Y_{i} \\leq c\\) for the treatment and placebo groups: \\[\\begin{eqnarray} \\hat{P}( Y_{i} \\leq 1| \\text{treatment} ) &amp;=&amp; \\frac{1}{ 1 + \\exp(-\\hat{\\alpha}_{1} + \\hat{\\beta})} = \\frac{1}{ 1 + \\exp(1.73 + 0.45 )} \\approx 0.10 \\nonumber \\\\ \\hat{P}( Y_{i} \\leq 1| \\text{placebo} ) &amp;=&amp; \\frac{1}{ 1 + \\exp(-\\hat{\\alpha}_{1})} = \\frac{1}{ 1 + \\exp(1.73)} \\approx 0.15 \\end{eqnarray}\\] 7.3 Generalized Estimating Equations With longitudinal data, you can fit random effects ordinal regression models with the repolr package. A GEE approach for ordinal outcomes can be done with the ordgee function from the geepack package. For longitudinal data, let \\(Y_{ij}\\) be the ordinal response for individual \\(i\\) at time \\(t_{ij}\\). The GEE approach described in (Heagerty and Zeger (1996)) assumes the “mean part” of the model has the form \\[\\begin{equation} P(Y_{ij} &gt; c|\\mathbf{x}_{i}) = \\frac{1}{1 + \\exp(-\\alpha_{c} - \\mathbf{x}_{i}^{T}\\boldsymbol{\\beta})} \\end{equation}\\] The above equation handles the marginal distribution of \\(Y_{ij}\\) at a specific time point. For a GEE, the only remaining part is how to handle the “correlation” structure between observations from individual \\(i\\). Setting up a “correlation” structure for ordinal outcomes is not that straightforward. The approach described below is that described in ( Heagerty and Zeger (1996) ). You don’t want to work with the usual definition of correlation because we are not treating these outcomes as numeric outcomes. Instead the dependence is expressed through different joint probabilities. The dependence between ordinal outcomes \\(Y_{ij}\\) and \\(Y_{ik}\\) is measured by the “marginal odds ratios” \\[\\begin{equation} \\psi_{i(j,k)(c_{1},c_{2})} = \\frac{P(Y_{ij} &gt; c_{1}, Y_{ik} &gt; c_{2}|\\mathbf{x}_{ij},\\mathbf{x}_{ij}) P(Y_{ij} \\leq c_{1}, Y_{ik} \\leq c_{2}|\\mathbf{x}_{ij},\\mathbf{x}_{ij})}{P(Y_{ij} &gt; c_{1}, Y_{ik} \\leq c_{2}|\\mathbf{x}_{ij},\\mathbf{x}_{ij}) P(Y_{ij} \\leq c_{1}, Y_{ik} &gt; c_{2}|\\mathbf{x}_{ij},\\mathbf{x}_{ij})} \\end{equation}\\] If \\(Y_{ij}\\) and \\(Y_{ik}\\) have high dependence, then \\(\\psi_{i(j,k)(c_{1},c_{2})}\\) should be large. The “exchangeable” correlation structure assumes that the odds-ratio parameter is constant \\[\\begin{equation} \\log\\left( \\psi_{i(j,k)(c_{1},c_{2})} \\right) = \\alpha \\end{equation}\\] for all \\((j,k)\\) and \\((c_{1}, c_{2})\\). The “independent” correlation structure assumes that \\(\\alpha = 0\\) since \\(\\psi_{i(j,k)(c_{1},c_{2})} = 1\\) if we assume that \\(Y_{ij}\\) and \\(Y_{ik}\\) are independent. 7.3.1 Using geepack and ordgee The ordgee function from the geepack package allows you to fit a GEE with ordinal responses. To show how to use ordgee, we can look at the respdis dataset from the geepack package library(geepack) data(respdis) resp.l &lt;- reshape(respdis, varying =list(c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;)), v.names = &quot;resp&quot;, direction = &quot;long&quot;) resp.l &lt;- resp.l[order(resp.l$id, resp.l$time),] head(resp.l) ## trt time resp id ## 1.1 1 1 1 1 ## 1.2 1 2 1 1 ## 1.3 1 3 1 1 ## 1.4 1 4 1 1 ## 2.1 0 1 1 2 ## 2.2 0 2 1 2 The response of interest here is the variable resp which has 3 possible values: table(resp.l$resp) ## ## 1 2 3 ## 80 212 152 The syntax for ordgee is basically the same as geeglm. Just make sure the response is an ordered factor. The main available correlation structures are “independence” and “exchangeable”. ## Fit GEE with just trt as a covariate fit.indep &lt;- ordgee(ordered(resp) ~ trt, id=id, corstr=&quot;independence&quot;, data=resp.l) summary(fit.indep) ## ## Call: ## ordgee(formula = ordered(resp) ~ trt, id = id, data = resp.l, ## corstr = &quot;independence&quot;) ## ## Mean Model: ## Mean Link: logit ## Variance to Mean Relation: binomial ## ## Coefficients: ## estimate san.se wald p ## Inter:1 1.139907 0.2365456 23.222502 1.442990e-06 ## Inter:2 -1.139907 0.2417271 22.237619 2.409072e-06 ## trt 0.976700 0.3161920 9.541586 2.008680e-03 ## ## Scale is fixed. ## ## Correlation Model: ## Correlation Structure: independence ## ## Returned Error Value: 0 ## Number of clusters: 111 Maximum cluster size: 4 The terms Inter:1, Inter:2 in the summary output represent the estimates of the intercept parameters \\(\\alpha_{1}\\) and \\(\\alpha_{2}\\). The estimate of the trt regression coefficient implies that the odds ratio (between trt = 1 and trt = 0) for having a positive outcome is roughly \\(2.6\\) \\[\\begin{equation} \\frac{\\text{odds}_{Y_{i} &gt; c}(trt=1) }{\\text{odds}_{Y_{i} &gt; c}(trt=0) } = \\exp(0.97) \\approx 2.65 \\end{equation}\\] To fit the same model with an exchangeable correlation structure, just use the corstr = exchangeable argument: ## Fit GEE with just trt as a covariate fit.ex &lt;- ordgee(ordered(resp) ~ trt, id=id, corstr=&quot;exchangeable&quot;, data=resp.l) summary(fit.ex) ## ## Call: ## ordgee(formula = ordered(resp) ~ trt, id = id, data = resp.l, ## corstr = &quot;exchangeable&quot;) ## ## Mean Model: ## Mean Link: logit ## Variance to Mean Relation: binomial ## ## Coefficients: ## estimate san.se wald p ## Inter:1 1.140030 0.2369495 23.148392 1.499686e-06 ## Inter:2 -1.140030 0.2421280 22.168817 2.496965e-06 ## trt 1.012671 0.3357436 9.097488 2.559608e-03 ## ## Scale is fixed. ## ## Correlation Model: ## Correlation Structure: exchangeable ## Correlation Link: log ## ## Estimated Correlation Parameters: ## estimate san.se wald p ## alpha 2.456858 0.2752256 79.68609 0 ## ## Returned Error Value: 0 ## Number of clusters: 111 Maximum cluster size: 4 7.4 Penalized Regression with Ordinal Outcomes The ordinalgmifs package fits L1-penalized regression with ordinal outcomes. ordinalNet is another package that performs penalized regression for ordinal outcomes. To use ordinalgmifs, we will use the eyedisease dataset in the package. library(ordinalgmifs) ## Loading required package: survival data(eyedisease) names(eyedisease) ## [1] &quot;rme&quot; &quot;lme&quot; &quot;rre&quot; &quot;lre&quot; &quot;riop&quot; &quot;liop&quot; &quot;age&quot; &quot;diab&quot; &quot;gh&quot; &quot;sbp&quot; ## [11] &quot;dbp&quot; &quot;bmi&quot; &quot;pr&quot; &quot;sex&quot; &quot;prot&quot; &quot;dose&quot; &quot;rerl&quot; &quot;lerl&quot; &quot;id&quot; A variable of interest is rerl. This is right eye severity of retinopathy. This is an ordered factor with levels “None”, “Mild”, “Moderate”, and “Proliferative”. table( eyedisease$rerl ) ## ## None Mild Moderate Proliferative ## 275 270 128 47 To fit a penalized ordinal regression where we can penalize all variables, use the following code: ## Fit ordinal logistic regression with covariates ## dose, prot, sex, bmi, dbp, sbp, pr, age eye.fit &lt;- ordinalgmifs(rerl ~ 1, x=c(&quot;dose&quot;, &quot;prot&quot;, &quot;sex&quot;, &quot;bmi&quot;, &quot;dbp&quot;, &quot;sbp&quot;, &quot;pr&quot;, &quot;age&quot;), data=eyedisease) If we look at a summary of eye.fit it will display the best regression coefficient estimates according to an AIC criterion: summary(eye.fit) ## Cumulative model using a logit link ## at step = 1339 ## logLik = -813.1043 ## AIC = 10358.21 ## BIC = 22327.05 ## (Intercept):1 (Intercept):2 (Intercept):3 dose prot ## -0.5779889 1.2531085 2.9824752 0.1080000 -0.2960000 ## sex bmi dbp sbp pr ## 0.0000000 -0.1410000 -0.2350000 -0.4370000 -0.0720000 ## age ## 0.0500000 According to AIC, all variables are selected except for sex. To get the “full path” of coefficient solutions, look at the beta component of eye.fit. eye.fit$beta is a 1453 x 8 matrix. Row \\(k\\) of eye.fit$beta has the value of the regression coefficients at step \\(k\\) Row \\(k\\) of eye.fit$alpha has the value of the intercept coefficient at step \\(k\\) dim(eye.fit$beta) ## [1] 1453 8 ## Look at an &quot;early&quot; row of eye.fit$beta ## Most of the coefficients here should be zero eye.fit$beta[10,] ## dose prot sex bmi dbp sbp pr age ## 0.00 0.00 0.00 0.00 0.00 -0.01 0.00 0.00 If we look at a later row, we should have more non-zero coefficients eye.fit$beta[800,] ## dose prot sex bmi dbp sbp pr age ## 0.000 -0.202 0.000 -0.022 -0.188 -0.388 0.000 0.000 The model.select component gives the row index of the selected model eye.fit$model.select ## [1] 1339 eye.fit$beta[eye.fit$model.select,] ## dose prot sex bmi dbp sbp pr age ## 0.108 -0.296 0.000 -0.141 -0.235 -0.437 -0.072 0.050 You can plot the “lasso path” for the regression coefficient estimates: plot( eye.fit ) References "],["extra-topics.html", "Chapter 8 Extra Topics 8.1 Uncertainty in Variable Importance Measures", " Chapter 8 Extra Topics 8.1 Uncertainty in Variable Importance Measures Many machine learning/penalized regression methods generate measures of variable importance. Random forests generate variable importance scores. Partial dependence plots are useful for assessing the impact of a covariate for any learning method. Lasso has selection/magnitude of regression coefficients. These variable importance measures do not usually come with a measure of uncertainty for each variable importance score. For example, you may want to report a confidence interval for the VIMP scores. 8.1.1 Subsampling for Random Forest VIMP scores A general approach to assessing the uncertainty of a variable importance measure is to use some form of repeated subsampling/sample splitting. The basic idea is to draw subsamples, and for each subsample compute variable importance scores. Then, estimate the variance of each variable importance score using their variation across different subsamples See Ishwaran and Lu (2019) for more details of this approach. Steps in subsampling approach: Draw a subsample of size b from the original dataset. Call it \\(D_{s}\\). Using random forest on dataset \\(D_{s}\\), compute VIMP scores \\(I_{s,j}\\) for variables \\(j=1,\\ldots,p\\). Repeat steps 1-2 \\(S\\) times. This will produce \\(I_{s,j}\\) for all subsamples \\(s = 1, \\ldots, S\\) and all variables \\(j = 1, \\ldots, p\\). Estimate the variance of \\(I_{s,j}\\) with the quantity \\[\\begin{equation} \\hat{v}_{j} = \\frac{b}{nK} \\sum_{s=1}^{S}\\Big( I_{s,j} - \\bar{I}_{.,j} \\Big)^{2} \\end{equation}\\] A \\(95\\%\\) confidence interval for the variable importance of variable \\(j\\) will then be \\[\\begin{equation} I_{j} \\pm 1.96 \\times \\sqrt{\\hat{v}_{j}} \\end{equation}\\] Here, \\(I_{j}\\) is the variable importance score from the full dataset. To test this out, we will use the diabetes data. This can be obtained from https://hastie.su.domains/CASI/data.html This dataset has 442 observations and 10 covariates The outcome variable of interest is prog dim(diabetes) ## [1] 442 11 head(diabetes) ## age sex bmi map tc ldl hdl tch ltg glu prog ## 1 59 1 32.1 101 157 93.2 38 4 2.110590 87 151 ## 2 48 0 21.6 87 183 103.2 70 3 1.690196 69 75 ## 3 72 1 30.5 93 156 93.6 41 4 2.029384 85 141 ## 4 24 0 25.3 84 198 131.4 40 5 2.123852 89 206 ## 5 50 0 23.0 101 192 125.4 52 4 1.863323 80 135 ## 6 23 0 22.6 89 139 64.8 61 2 1.819544 68 97 Let’s first fit a randomForest to the entire dataset and plot the variable importance measures library(randomForest) ## randomForest 4.7-1 ## Type rfNews() to see new features/changes/bug fixes. rf.full &lt;- randomForest(prog ~ ., data=diabetes) varImpPlot(rf.full) You can extract the actual values of the variable importance scores by using the importance function. Imp.Full &lt;- importance(rf.full) Imp.Full ## This is a 10 x 1 matrix ## IncNodePurity ## age 140873.38 ## sex 34081.46 ## bmi 580488.33 ## map 282349.31 ## tc 146381.70 ## ldl 158215.95 ## hdl 208460.38 ## tch 172102.74 ## ltg 555204.71 ## glu 212546.01 Now, let’s compute variable importance scores across \\(S = 100\\) subsamples (each of size 50) and store it in a \\(10 \\times S\\) matrix called Imp.Subs S &lt;- 5 b &lt;- 100 Imp.Subs &lt;- matrix(0, nrow=nrow(Imp.Full), ncol=S) rownames(Imp.Subs) &lt;- rownames(Imp.Full) for(k in 1:S) { ## sample without replacement subs &lt;- sample(1:nrow(diabetes), size=b) diabetes.sub &lt;- diabetes[subs,] rf.sub &lt;- randomForest(prog ~ ., data=diabetes.sub) Imp.Subs[,k] &lt;- importance(rf.sub) } From Imp.Subs, we can compute the variance estimates \\(\\hat{v}_{j}\\). imp.mean &lt;- rowMeans(Imp.Subs) vhat &lt;- (b/nrow(diabetes))*rowMeans((Imp.Subs - imp.mean)^2) print(vhat) ## age sex bmi map tc ldl ## 3031403.0 694289.6 141185451.0 49990020.4 4161847.5 3378843.2 ## hdl tch ltg glu ## 64905548.1 32434785.5 119250726.4 8760093.6 We can now report confidence intervals for the variable importance scores: vi.upper &lt;- Imp.Full[,1] + 1.96*sqrt(vhat) vi.lower &lt;- Imp.Full[,1] - 1.96*sqrt(vhat) VIMP_CI &lt;- cbind(Imp.Full[,1], vi.lower, vi.upper) colnames(VIMP_CI) &lt;- c(&quot;estimate&quot;, &quot;lower&quot;, &quot;upper&quot;) VIMP_CI[order(-VIMP_CI[,1]),] ## estimate lower upper ## bmi 580488.33 557199.32 603777.34 ## ltg 555204.71 533801.12 576608.30 ## map 282349.31 268491.40 296207.22 ## glu 212546.01 206744.91 218347.11 ## hdl 208460.38 192669.84 224250.92 ## tch 172102.74 160940.24 183265.24 ## ldl 158215.95 154613.15 161818.75 ## tc 146381.70 142383.19 150380.22 ## age 140873.38 137460.84 144285.92 ## sex 34081.46 32448.31 35714.61 8.1.2 Stability Selection for Penalized Regression Let’s use the lasso with penalty \\(\\lambda = 10\\) on the diabetes data: library(glmnet) ## Loading required package: Matrix ## Loaded glmnet 4.1-3 diabet.mod &lt;- glmnet(x=diabetes.sub[,1:10],y=diabetes.sub$prog, lambda=20) We can look at the estimated coefficients to see which variables were selected coef(diabet.mod) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) -83.750755 ## age . ## sex . ## bmi 6.295754 ## map . ## tc . ## ldl . ## hdl . ## tch . ## ltg 36.637885 ## glu . The selected variables are those with nonzero coefficients: # Look at selected coefficients ignoring the intercept: selected &lt;- abs(coef(diabet.mod)[-1]) &gt; 0 selected ## [1] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE When thinking about how certain you might be about a given set of selected variables, one natural question is how would this set of selected variables change if you re-ran the lasso on a different subset. You might have more confidence in a variable that is consistently selected across random subsets of your data. For a given choice of \\(\\lambda\\), the stability of variable \\(j\\) is defined as \\[\\begin{equation} \\hat{\\pi}_{j}(\\lambda) = \\frac{1}{S}\\sum_{s=1}^{S} I( A_{j,s}(\\lambda) = 1), \\end{equation}\\] where … \\(A_{j,s}(\\lambda) = 1\\) if variable \\(j\\) in data subsample \\(s\\) is selected and \\(A_{j,s}(\\lambda) = 0\\) if variable \\(j\\) in data subsample \\(s\\) is not selected Meinshausen and Bühlmann (2010) recommend drawing subsamples of size \\(n/2\\). The quantity \\(\\hat{\\pi}_{j}(\\lambda)\\) can be thought of as an estimate of the probability that variable \\(j\\) is in the “selected set” of variables. Variables with a large value of \\(\\hat{\\pi}_{j}(\\lambda)\\) have a greater “selection stability”. You can plot \\(\\hat{\\pi}_{j}(\\lambda)\\) across different values of \\(\\lambda\\) to get a sense of the range of selection stability. For the diabetes data, let’s first compute an \\(S \\times m \\times 10\\) array, where the \\((k, h, j)\\) element of this array equals \\(1\\) if variable \\(j\\) was selected in subsample \\(k\\) with penalty term \\(\\lambda_{h}\\): nsamps &lt;- 200 b &lt;- floor(nrow(diabetes)/2) nlambda &lt;- 40 ## 40 different lambda values lambda.seq &lt;- seq(0.1, 20.1, length.out=nlambda) ## Create an nsamps x nlambda x 10 array SelectionArr &lt;- array(0, dim=c(nsamps, nlambda, 10)) for(k in 1:nsamps) { subs &lt;- sample(1:nrow(diabetes), size=b) diabetes.sub &lt;- diabetes[subs,] for(h in 1:nlambda) { sub.fit &lt;- glmnet(x=diabetes.sub[,1:10],y=diabetes.sub$prog, lambda=lambda.seq[h]) selected &lt;- abs(coef(sub.fit)[-1]) &gt; 0 SelectionArr[k,h,] &lt;- selected } } From this array, we can compute a matrix containing selection probability estimates \\(\\hat{\\pi}_{j}(\\lambda)\\). The \\((j, h)\\) component of this matrix has the value \\(\\hat{\\pi}_{j}(\\lambda_{h})\\) SelectionProb &lt;- matrix(0, nrow=10, ncol=nlambda) rownames(SelectionProb) &lt;- names(diabetes)[1:10] for(h in 1:nlambda) { SelectionProb[,h] &lt;- colMeans(SelectionArr[,h,]) } The first few columns of SelectionProb look like the following: SelectionProb[,1:5] ## [,1] [,2] [,3] [,4] [,5] ## age 0.970 0.790 0.61 0.450 0.350 ## sex 1.000 1.000 1.00 1.000 0.995 ## bmi 1.000 1.000 1.00 1.000 1.000 ## map 1.000 1.000 1.00 1.000 1.000 ## tc 0.955 0.880 0.83 0.690 0.540 ## ldl 0.900 0.185 0.20 0.265 0.300 ## hdl 0.860 0.945 0.99 1.000 1.000 ## tch 0.905 0.655 0.44 0.270 0.205 ## ltg 1.000 1.000 1.00 1.000 1.000 ## glu 0.970 0.850 0.75 0.705 0.635 We can now plot the stability measures as a function of \\(\\lambda\\) ## Convert to long form: df &lt;- data.frame(varname=rep(names(diabetes)[1:10], each=nlambda), selection.prob=c(t(SelectionProb)), lambda=rep(lambda.seq, 10)) head(df) ## varname selection.prob lambda ## 1 age 0.97 0.1000000 ## 2 age 0.79 0.6128205 ## 3 age 0.61 1.1256410 ## 4 age 0.45 1.6384615 ## 5 age 0.35 2.1512821 ## 6 age 0.26 2.6641026 library(ggplot2) ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;package:randomForest&#39;: ## ## margin ggplot(df) + aes(x=lambda, y=selection.prob, group=varname, color=varname) + geom_line() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
