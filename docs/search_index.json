[
["index.html", "Notes for Case Studies in Health Big Data Preface", " Notes for Case Studies in Health Big Data Nicholas Henderson 2021-02-10 Preface This will contain some of the course notes for Biostatistics 629, Winter 2021. "],
["mixed-models.html", "Chapter 1 Mixed Models for Longitudinal Data Analysis 1.1 Methods for Analyzing Longitudinal Data 1.2 Mixed Models for Continuous Outcomes 1.3 Advantages of using random effects 1.4 Generalized linear mixed models (GLMMs) 1.5 Fitting Linear Mixed Models (LMMs) and Generalized Linear Mixed models (GLMMs) in R", " Chapter 1 Mixed Models for Longitudinal Data Analysis 1.1 Methods for Analyzing Longitudinal Data Longitudinal data refers to data that: Has multiple individuals/subjects. Each individual has multiple observations that were taken across time. We will denote the outcomes of interest with \\(Y_{ij}\\). \\(Y_{ij}\\) - outcome for individual \\(i\\) at time \\(t_{ij}\\). The \\(i^{th}\\) individual has \\(n_{i}\\) observations: \\(Y_{i1}, \\ldots, Y_{in_{i}}\\). There will be \\(m\\) individuals in the study (so \\(1 \\leq i \\leq m\\)). The above figure shows an example of outcomes from a longitudinal study (the sleepstudy data in the lme4 package). In the sleepstudy data: The time points of observation \\(t_{ij}\\) are the same for each individual \\(i\\). So, we can say \\(t_{ij} = t_{j}\\) for all \\(i\\). The outcome \\(Y_{ij}\\) is the reaction time for the \\(i^{th}\\) individual at time point \\(t_{j}\\). The 10 time points are \\((t_{1}, \\ldots, t_{10}) = (0, 1, \\ldots, 9)\\). Most of the well-known regression-based methods for analyzing longitudinal data can be classified (see Diggle et al. (2013)) into one of the three following categories: Random effects/mixed models, Marginal models, Transition models Random effects/Mixed Models “Random effects” are added to the regression model describing the outcomes for each individual. These “random regression coefficients” are viewed as a sample from some distribution. Marginal models Regression coefficients have a “population average” interpretation. Only mean of \\(Y_{ij}\\) and correlation structure of \\((Y_{i1}, \\ldots, Y_{in_{i}})\\) are modeled. Generalized estimating equations (GEEs) are often used for estimating model parameters. Transition models Uses a probability model for the distribution of \\(Y_{ij}\\) given the value of the outcome at the previous time point \\(Y_{ij-1}\\). 1.2 Mixed Models for Continuous Outcomes If each \\(Y_{ij}\\) is a continuous outcome and we were to build a regression model without any random effects, we might assume something like: \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + e_{ij} \\tag{1.1} \\end{equation}\\] \\(\\mathbf{x}_{ij} = (x_{i1}, \\ldots, x_{ip})\\) is the vector of covariates for individual \\(i\\) at time \\(j\\). The vector \\(\\mathbf{x}_{ij}\\) could contain individual information such as smoking status or age. \\(\\mathbf{x}_{ij}\\) could also contain some of the actual time points: \\(t_{ij}, t_{ij-1}, ...\\) or transformations of these time points. The regression model (1.1) assumes the same mean function \\(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta}\\) holds for all individuals that have the same value of \\(\\mathbf{x}_{ij}\\). It is often reasonable to assume that the regression coefficients vary across individuals. This can often better account for heterogeneity across individuals. The figure below shows 3 different regression lines from the sleepstudy data. Each regression line was estimated using only data from one individual. Figure 1.1: Separately estimated regression lines for 3 subjects in the sleepstudy data. Figure 1.1 suggests there is some heterogeneity in the relationship between study day and response time across individuals. The response time of Subject 309 changes very little over time. For Subject 308, there is a more clear positive association between response time and day of study. For the sleepstudy data, a linear regression for reaction time vs. study day which assumes that Expected response time is a linear function of study day, All individuals have the same regression coefficients, would have the form: \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1} t_{j} + e_{ij} \\end{equation}\\] If we allowed each individual to have his/her own intercept and slope, we could instead consider the following model \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1} t_{j} + u_{i0} + u_{i1}t_{j} + e_{ij} \\tag{1.2} \\end{equation}\\] \\(\\beta_{0} + u_{i0}\\) - intercept for individual \\(i\\). \\(\\beta_{1} + u_{i1}\\) - slope for individual \\(i\\). If we assume \\((u_{i0}, u_{i1})\\) are sampled from some distribution, \\(u_{i0}\\) and \\(u_{i1}\\) are referred to as random effects. Typically, it is assumed that \\((u_{i0}, u_{i1})\\) are sampled from a multivariate normal distribution with mean zero: \\[\\begin{equation} (u_{i0}, u_{i1}) \\sim \\textrm{Normal}( \\mathbf{0}, \\boldsymbol{\\Sigma}_{\\tau} ) \\end{equation}\\] Model (1.2) is called a mixed model because it contains both fixed effects \\((\\beta_{0}, \\beta_{1})\\) and random effects \\((u_{i0}, u_{i1})\\). More generally, a linear mixed model (LMM) for longitudinal data will have the form: \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} + e_{ij} \\tag{1.3} \\end{equation}\\] \\(\\boldsymbol{\\beta}\\) - vector of fixed effects \\(\\mathbf{u}_{i}\\) - vector of random effects If we stack the responses into a long vector \\(\\mathbf{Y}\\) and random effects into a long vector \\(\\mathbf{u}\\) \\(\\mathbf{Y} = (Y_{11}, Y_{12}, ...., Y_{mn_{m}})\\) - this vector has length \\(\\sum_{k=1}^{m} n_{k}\\) \\(\\mathbf{u} = (u_{10}, u_{11}, ...., u_{mq})\\) - this vector has length \\(m \\times (q + 1)\\). Then, we can write the general form (1.3) of the LMM as \\[\\begin{equation} \\mathbf{Y} = \\mathbf{X}\\tilde{\\boldsymbol{\\beta}} + \\mathbf{Z}\\mathbf{u} + \\mathbf{e} \\end{equation}\\] \\(i^{th}\\) row of \\(\\mathbf{X}\\) is \\((1, \\mathbf{x}_{ij}^{T})\\). \\(i^{th}\\) row of \\(\\mathbf{Z}\\) is \\(\\mathbf{z}_{ij}^{T}\\). \\(\\tilde{\\boldsymbol{\\beta}} = (1, \\boldsymbol{\\beta})\\). Constructing an LMM can be thought of as choosing the desired “\\(\\mathbf{X}\\)” and “\\(\\mathbf{Z}\\)” matrices. 1.3 Advantages of using random effects 1.3.1 Within-subject correlation Using an LMM automatically accounts for the “within-subject” correlation. That is, the correlation between two observations from the same individual. This correlation arises because observations on the same individual “share” common random effects. The correlation between the \\(j^{th}\\) and \\(k^{th}\\) observation from individual \\(i\\) is \\[\\begin{equation} \\textrm{Corr}(Y_{ij}, Y_{ik}) = \\frac{ \\mathbf{z}_{ij}^{T}\\boldsymbol{\\Sigma}_{\\tau}\\mathbf{z}_{ik} }{ \\sqrt{\\sigma^{2} + \\mathbf{z}_{ij}^{T}\\boldsymbol{\\Sigma}_{\\tau}\\mathbf{z}_{ij}}\\sqrt{\\sigma^{2} + \\mathbf{z}_{ik}^{T}\\boldsymbol{\\Sigma}_{\\tau}\\mathbf{z}_{ik}}} \\end{equation}\\] When using only a random intercept, the correlation between \\(Y_{ij}\\) and \\(Y_{ik}\\) is \\[\\begin{equation} \\textrm{Corr}(Y_{ij}, Y_{ik}) = \\frac{ \\sigma_{u}^{2} }{ \\sigma^{2} + \\sigma_{u}^{2} } \\end{equation}\\] In this case, \\(\\mathbf{z}_{ij} = 1\\) and \\(u_{i} \\sim \\textrm{Normal}(0, \\sigma_{u}^{2})\\) \\(\\sigma^{2}\\) is the variance of the residual term \\(e_{ij}\\) For longitudinal data, one criticism of the random intercept model is that the within-subject correlation does not vary across time. 1.3.2 Inference about Heterogeneity - Variance of Random Effects One of the goals of the data analysis may be to characterize the heterogeneity in the relationship between the outcome and some of the covariates across individuals. Looking at the estimates of the variance of the random effects is one way of addressing this goal. An estimate of \\(\\textrm{Var}( u_{ih} )\\) “substantially greater than zero” is an indication that there is variability in the regression coefficient corresponding to \\(u_{ih}\\) across individuals. For example, with the random intercept and slope model for the sleepstudy data \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1}t_{j} + u_{i0} + u_{i1}t_{j} + e_{ij} \\end{equation}\\] If \\(\\textrm{Var}( u_{i1} )\\) is “large”, this implies that the response to additional days of sleep deprivation varies considerably across individuals. The response time of some individuals is not impacted much by additional days of little sleep. Some individuals respond strongly to additional days of little sleep. 1.3.3 Best Linear Unbiased Prediction You may want to estimate or “predict” the mean function/trajectory of a given individual. This means you want to estimate/predict the following quantity: \\[\\begin{equation} \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} \\end{equation}\\] The “Best Linear Unbiased Predictor” (BLUP) of this is \\[\\begin{equation} \\textrm{BLUP}(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i}) = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}E(\\mathbf{u}_{i}|Y_{i1}, \\ldots, Y_{in_{i}}) \\end{equation}\\] I would think of the values of \\(\\textrm{BLUP}(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i})\\) (for different values of \\(j\\)) as an estimate of the “true trajectory” (i.e., the true mean) of the \\(i^{th}\\) individual. The observed longitudinal outcomes from individual \\(i\\) are a “noisy estimate” of that individual’s true trajectory. The BLUPs are more stable “shrinkage” estimates of the trajectory of individual \\(i\\). These are called shrinkage estimates because often shrinks the estimate that would be obtained using only data from individual \\(i\\) towards the “overall” estimate \\(\\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta}\\). For example, if we had the intercept-only model \\(Y_{ij} = \\beta_{0} + u_{i} + e_{ij}\\), the value of the BLUPs is \\[\\begin{equation} \\textrm{BLUP}(\\beta_{0} + u_{i}) = \\frac{n_{i}\\sigma_{u}^{2}}{\\sigma^{2} + n_{i}\\sigma_{u}^{2} }\\bar{Y}_{i.} + \\Big(1 - \\frac{n_{i}\\sigma_{u}^{2}}{\\sigma^{2} + n_{i}\\sigma_{u}^{2} }\\Big)\\bar{Y}_{..} \\end{equation}\\] \\(\\bar{Y}_{i.}\\) is the sample mean from individual-\\(i\\) data \\(\\bar{Y}_{i.}\\) would be the estimate of the intercept if we only looked at data from the \\(i^{th}\\) individual. \\(\\bar{Y}_{..}\\) - overall mean \\(\\bar{Y}_{..}\\) would be the estimate of the intercept if we ignored variation in intercepts across individuals. You can also think of \\(\\textrm{BLUP}(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i})\\) as a prediction of what the observed trajectory for individual \\(i\\) would be if that individual were in a future study under the same conditions. Say \\(Y_{i1}&#39;, \\ldots, Y_{in_{i}}&#39;\\) are the observations for individual \\(i\\) in a future study. The outcomes in the future study are determined by \\[\\begin{equation} Y_{ij}&#39; = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} + e_{ij}&#39; \\end{equation}\\] The expectation of \\(Y_{ij}&#39;\\) given the observed data in our longitudinal study is \\[\\begin{eqnarray} E(Y_{ij}&#39;|Y_{i1}, \\ldots, Y_{in_{i}}) &amp;=&amp; \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}E(\\mathbf{u}_{i}|Y_{i1}, \\ldots, Y_{in_{i}}) \\nonumber \\\\ &amp;=&amp; \\textrm{BLUP}(\\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i}) \\nonumber \\end{eqnarray}\\] 1.4 Generalized linear mixed models (GLMMs) Generalized linear models (GLMs) are used to handle “non-continuous” data that can’t be reasonably modeled with a Gaussian distribution. The most common scenarios where you would use GLMs in practice are binary, count, and multinomial outcomes. With a generalized linear mixed model (GLMM), you assume that a GLM holds conditional on the value of the random effects. 1.4.1 GLMMs with Binary Outcomes Under the GLM framework, the usual approach for handling binary outcomes is logistic regression. The assumptions underying logistic regression are: The outcomes are independent Each outcome follows a Bernoulli distribution. The log-odds parameter is assumed to be a linear combination of the covariates. With the GLMM version of logistic regression, we will make almost the same assumptions as the regular GLM version of logistic regression. The main difference is that each assumption in the GLMM will be conditional on the values of the random effects. To be specific, for longitudinal binary outcomes \\(Y_{ij}\\), the GLMM version of logistic regression assumes the following: Conditional on the vector of random effects \\(\\mathbf{u}_{i}\\) \\[\\begin{equation} Y_{i1}, \\ldots, Y_{in_{i}}|\\mathbf{u}_{i} \\textrm{ are independent } \\end{equation}\\] Conditional on \\(\\mathbf{u}_{i}\\), each \\(Y_{ij}\\) has a Bernoulli distribution \\[\\begin{equation} Y_{ij}|\\mathbf{u}_{i} \\sim \\textrm{Bernoulli}\\big\\{ p_{ij}(\\mathbf{u}_{i}) \\big\\} \\end{equation}\\] so that \\(p_{ij}( \\mathbf{u}_{i} ) = P(Y_{ij} = 1| \\mathbf{u}_{i})\\). The “conditional” log-odds term \\(\\log\\{ p_{ij}(\\mathbf{u}_{i})/[1 - p_{ij}(\\mathbf{u}_{i})] \\}\\) is a linear combination of the covariates and the random effects vector \\(\\mathbf{u}_{i}\\): \\[\\begin{equation} \\textrm{logit}\\{ p_{ij}(\\mathbf{u}_{i}) \\} = \\log\\Big( \\frac{ p_{ij}(\\mathbf{u}_{i})}{ 1 - p_{ij}(\\mathbf{u}_{i}) } \\Big) = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} \\end{equation}\\] As with a linear mixed model, we assume that the random-effects vector \\(\\mathbf{u}_{i}\\) has a multivariate normal distribution with mean zero and covariance matrix \\(\\boldsymbol{\\Sigma}_{\\tau}\\) \\[\\begin{equation} \\mathbf{u}_{i} \\sim \\textrm{Normal}( \\mathbf{0}, \\boldsymbol{\\Sigma}_{\\tau}) \\end{equation}\\] 1.4.2 GLMMs with Count Outcomes For count outcomes, responses are typically assumed to follow a Poisson distribution and sometimes a negative binomial distribution - conditional on the values of the random effects. For the Poisson model, we assume \\(Y_{ij}|\\mathbf{u}_{i} \\sim \\textrm{Poisson}\\{ \\mu_{ij}( \\mathbf{u}_{i} ) \\}\\), \\[\\begin{equation} E(Y_{ij}| \\mathbf{u}_{i}) = \\mu_{ij}(\\mathbf{u}_{i}) \\qquad \\textrm{Var}( Y_{ij}| \\mathbf{u}_{i} ) = \\mu_{ij}(\\mathbf{u}_{i}) \\end{equation}\\] One common problem with the Poisson distribution is overdispersion (i.e., variance is greater than the mean). The variance of the Poisson equals the mean. While the marginal variance will not equal the mean in a GLMM, requiring the conditional means and variances to be equal could lead to a poor fit. For the negative binomial model, we assume \\(Y_{ij}|\\mathbf{u}_{i} \\sim \\textrm{NB}\\{ \\mu_{ij}( \\mathbf{u}_{i}) , \\phi \\}\\), \\[\\begin{equation} E(Y_{ij}| \\mathbf{u}_{i}) = \\mu_{ij}(\\mathbf{u}_{i}) \\qquad \\textrm{Var}( Y_{ij}| \\mathbf{u}_{i} ) = \\mu_{ij}(\\mathbf{u}_{i}) + \\phi\\mu_{ij}^{2}(\\mathbf{u}_{i}) \\end{equation}\\] \\(\\phi\\) is often referred to as the overdispersion parameter. With a GLMM model for count data, it is typical to model the log of the conditional mean \\(\\mu_{ij}(\\mathbf{u}_{i})\\) with a linear regression: \\[\\begin{equation} \\log\\{ \\mu_{ij}(\\mathbf{u}_{i}) \\} = \\beta_{0} + \\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}^{T}\\mathbf{u}_{i} \\end{equation}\\] Again, for the Poisson GLMM, the usual assumption for the random effects is that \\[\\begin{equation} \\mathbf{u}_{i} \\sim \\textrm{Normal}( \\mathbf{0}, \\boldsymbol{\\Sigma}_{\\tau}) \\end{equation}\\] 1.5 Fitting Linear Mixed Models (LMMs) and Generalized Linear Mixed models (GLMMs) in R The lme4 package is probably the most general package for fitting LMMs and GLMMs. library(lme4) 1.5.1 Fitting LMMs with the sleepstudy data To start off, let’s use the sleepstudy longitudinal data in lme4 and look at the data from the first two individuals in this data. data(sleepstudy) dim(sleepstudy) # 18 individuals, each with 10 observations ## [1] 180 3 sleepstudy[1:20,] # Data from the subjects with ids: 308 and 309 ## Reaction Days Subject ## 1 249.5600 0 308 ## 2 258.7047 1 308 ## 3 250.8006 2 308 ## 4 321.4398 3 308 ## 5 356.8519 4 308 ## 6 414.6901 5 308 ## 7 382.2038 6 308 ## 8 290.1486 7 308 ## 9 430.5853 8 308 ## 10 466.3535 9 308 ## 11 222.7339 0 309 ## 12 205.2658 1 309 ## 13 202.9778 2 309 ## 14 204.7070 3 309 ## 15 207.7161 4 309 ## 16 215.9618 5 309 ## 17 213.6303 6 309 ## 18 217.7272 7 309 ## 19 224.2957 8 309 ## 20 237.3142 9 309 The sleepstudy data is an example of longitudinal data stored in long format (as opposed to “wide” format). In long format, each row of the dataset corresponds to an observation from one individual at one time point. The lmer function in lme4 fits linear mixed models. This has many of the same features as the lm function in R. To fit an LMM with lmer, the main thing to do is to specify the “X” part of the model (i.e., the fixed effects) and the “Z” part of the model (i.e., the random effects). The “X” part of the model is done using the exact same “formula notation” used in the lm function. The “Z” part of the model is done using the following type of syntax: (formula | group_var) group_var is the “grouping variable” used for the random effects This would be the variable telling you which 1.5.1.1 LMM with a single, random intercept for each subject Let’s fit an LMM where there is a fixed slope for time and only a random intercept for each Subject \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1}t_{j} + u_{i} + e_{ij} \\tag{1.4} \\end{equation}\\] For the “X” part of this model, we use Reaction ~ Days. This gives us a fixed intercept and a fixed slope for the Days variable. For the “Z” part of this model, we just add (1|Subject). This says that there is only a random intercept within the grouping variable Subject. Putting these two together, we can fit the LMM (1.4) using the following code: lmm.sleep.intercept &lt;- lmer(Reaction ~ Days + (1|Subject), data = sleepstudy) You can always use the model.matrix method on the fitted lmer object to check that the “X” and “Z” matrices correspond to the model you want. Let’s look at the first 5 rows of the “X” matrix from lmm.sleep.intercept x.mat &lt;- model.matrix(lmm.sleep.intercept) ## This design matrix should have an intercept column ## and a column which stores the &quot;Days&quot; variable x.mat[1:5,] ## (Intercept) Days ## 1 1 0 ## 2 1 1 ## 3 1 2 ## 4 1 3 ## 5 1 4 Let’s look at the first 20 rows of the “Z” matrix from lmm.intercept ## Use argument type = &quot;random&quot; to get random-effects design matrix z.mat &lt;- model.matrix(lmm.sleep.intercept, type=&quot;random&quot;) z.mat[1:20,] # The . values in zmat correspond to zeros ## 20 x 18 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 18 column names &#39;308&#39;, &#39;309&#39;, &#39;310&#39; ... ]] ## ## 1 1 . . . . . . . . . . . . . . . . . ## 2 1 . . . . . . . . . . . . . . . . . ## 3 1 . . . . . . . . . . . . . . . . . ## 4 1 . . . . . . . . . . . . . . . . . ## 5 1 . . . . . . . . . . . . . . . . . ## 6 1 . . . . . . . . . . . . . . . . . ## 7 1 . . . . . . . . . . . . . . . . . ## 8 1 . . . . . . . . . . . . . . . . . ## 9 1 . . . . . . . . . . . . . . . . . ## 10 1 . . . . . . . . . . . . . . . . . ## 11 . 1 . . . . . . . . . . . . . . . . ## 12 . 1 . . . . . . . . . . . . . . . . ## 13 . 1 . . . . . . . . . . . . . . . . ## 14 . 1 . . . . . . . . . . . . . . . . ## 15 . 1 . . . . . . . . . . . . . . . . ## 16 . 1 . . . . . . . . . . . . . . . . ## 17 . 1 . . . . . . . . . . . . . . . . ## 18 . 1 . . . . . . . . . . . . . . . . ## 19 . 1 . . . . . . . . . . . . . . . . ## 20 . 1 . . . . . . . . . . . . . . . . The . values in z.mat are just zeros. Notice that each Subject has its own “intercept” column. This what we want - each Subject has its own intercept. Let’s look at the estimated parameters from the LMM with random intercepts using summary summary(lmm.sleep.intercept) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Reaction ~ Days + (1 | Subject) ## Data: sleepstudy ## ## REML criterion at convergence: 1786.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.2257 -0.5529 0.0109 0.5188 4.2506 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Subject (Intercept) 1378.2 37.12 ## Residual 960.5 30.99 ## Number of obs: 180, groups: Subject, 18 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 251.4051 9.7467 25.79 ## Days 10.4673 0.8042 13.02 ## ## Correlation of Fixed Effects: ## (Intr) ## Days -0.371 The estimated fixed-effects intercept is \\(\\hat{\\beta}_{0} = 251.4\\), and the estimated fixed-effects slope is \\(\\hat{\\beta}_{1} = 10.5\\). The estimated variance of the random intercept is \\(\\hat{\\tau}^{2} = 1378.2\\) (standard deviation is \\(\\hat{\\tau} = 37.1\\)). i.e., it is estimated that \\(u_{i} \\sim \\textrm{Normal}(0, 1378.2)\\). 1.5.1.2 LMM with both a random intercept and slope for each subject Now, let’s fit an LMM where there is a fixed slope for time and both a random intercept and slope for each Subject \\[\\begin{equation} Y_{ij} = \\beta_{0} + \\beta_{1}t_{j} + u_{i0} + u_{i1}t_{j} + e_{ij} \\tag{1.5} \\end{equation}\\] This is done with lmer using the following code: lmm.sleep.slope &lt;- lmer(Reaction ~ Days + (Days|Subject), data = sleepstudy) Again, let’s check the “X” and “Z” matrices from lmm.sleep.slope to double-check that everything makes sense x.mat2 &lt;- model.matrix(lmm.sleep.slope) ## This design matrix should be the same as that from lmm.sleep.intercept x.mat2[1:5,] ## (Intercept) Days ## 1 1 0 ## 2 1 1 ## 3 1 2 ## 4 1 3 ## 5 1 4 First 20 rows of the “Z” matrix from lmm.sleep.slope: ## Use argument type = &quot;random&quot; to get random-effects design matrix z.mat2 &lt;- model.matrix(lmm.sleep.slope, type=&quot;random&quot;) z.mat2[1:20,] # The . values in zmat2 correspond to zeros ## 20 x 36 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 36 column names &#39;308&#39;, &#39;308&#39;, &#39;309&#39; ... ]] ## ## 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 2 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 3 1 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 4 1 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 5 1 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 6 1 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 7 1 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 8 1 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 9 1 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 10 1 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 11 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 12 . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 13 . . 1 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 14 . . 1 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 15 . . 1 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 16 . . 1 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 17 . . 1 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 18 . . 1 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 19 . . 1 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## 20 . . 1 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Note that the two columns for each Subject in z.mat2 are of the form \\((1, t_{j})\\), which is what we want. Let’s look at the estimated parameters from lmm.sleep.slope summary(lmm.sleep.slope) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Reaction ~ Days + (Days | Subject) ## Data: sleepstudy ## ## REML criterion at convergence: 1743.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.9536 -0.4634 0.0231 0.4634 5.1793 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 612.10 24.741 ## Days 35.07 5.922 0.07 ## Residual 654.94 25.592 ## Number of obs: 180, groups: Subject, 18 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 251.405 6.825 36.838 ## Days 10.467 1.546 6.771 ## ## Correlation of Fixed Effects: ## (Intr) ## Days -0.138 The estimated fixed-effects coefficients are \\(\\hat{\\beta}_{0} = 251.4\\), and \\(\\hat{\\beta}_{1} = 10.5\\) respectively. The estimated standard deviation and correlation of the random effects are Estimated standard deviation of \\(u_{i0}\\) is \\(24.7\\). Estimated standard deviation of \\(u_{i1}\\) is \\(5.9\\). Estimated correlation between \\(u_{i0}\\) and \\(u_{i1}\\) is \\(0.07\\). Rather than always printing out the entire summary, you can directly extract the estimates of the fixed effects with coef( summary(lmm.sleep.slope) ) ## Estimate Std. Error t value ## (Intercept) 251.40510 6.824597 36.838090 ## Days 10.46729 1.545790 6.771481 To directly extract the estimates of the variance (or standard deviation) of the random effects, you can use: VarCorr( lmm.sleep.slope ) ## Groups Name Std.Dev. Corr ## Subject (Intercept) 24.7407 ## Days 5.9221 0.066 ## Residual 25.5918 Interpreting the estimated variance of the random effects One way I like to think about the magnitude of the variance components is to look at the 5th and 95th percentiles of the random effects distribution. For example, if you only have a random intercept term, then roughly \\(90\\%\\) of individuals will have an intercept that falls in the interval \\([\\hat{\\beta}_{0} - 1.64\\hat{\\sigma}_{u0}, \\hat{\\beta}_{0} + 1.64\\hat{\\sigma}_{u0}]\\). If you have a random slope term, then roughly \\(90\\%\\) of individuals will have an intercept that falls in the interval \\([\\hat{\\beta}_{1} - 1.64\\hat{\\sigma}_{u1}, \\hat{\\beta}_{1} + 1.64\\hat{\\sigma}_{u1}]\\). Another idea for helping to interpret the magnitude of the random effects is to plot many random trajectories for specific choices of the covariate vector \\(\\mathbf{x}_{i}\\) (if the \\(\\mathbf{x}_{i}\\) vary across individuals). For example, in the sleepstudy data, you could plot \\(\\hat{\\beta}_{0} + u_{i0} + \\hat{\\beta}_{1}t_{j} + u_{i1}t_{j}\\) where the pairs \\((u_{i0}, u_{i1})\\) are generated from the estimated joint Normal distribution. Sigma.hat &lt;- VarCorr( lmm.sleep.slope )$Subject # This is the random-effects # covariance matrix ndraws &lt;- 100 Sigma.hat.sqrt &lt;- chol(Sigma.hat) beta.hat &lt;- coef( summary(lmm.sleep.slope) )[,1] # estimated fixed effects print(beta.hat) ## (Intercept) Days ## 251.40510 10.46729 plot(sleepstudy$Days, sleepstudy$Reaction, type=&quot;n&quot;, xlab=&quot;Days&quot;, ylab=&quot;Response&quot;, las=1, main=&quot;sleepstudy: Variation in subject-specific trajectories&quot;) for(k in 1:ndraws) { uvec.draw &lt;- Sigma.hat.sqrt%*%rnorm(2) # draw random (ui0, ui1) pair trajectory &lt;- beta.hat[1] + uvec.draw[1] + (0:9)*(beta.hat[2] + uvec.draw[2]) lines(0:9, trajectory) } Figure 1.2: Random trajectories for sleepstudy data using the estimated intercept and slope random-effects variances. 1.5.1.3 Extracting BLUPs in lme4 To get the “BLUPs” the intercepts and slopes \\(\\textrm{BLUP}(u_{i0})\\) and \\(\\textrm{BLUP}(u_{i1})\\), use ranef blups.slope &lt;- ranef( lmm.sleep.slope ) To plot these, use dotplot (you will need to load the lattice package first) library(lattice) dotplot(blups.slope) ## $Subject ## This plots subjects sorted by individual-specific estimates of intercepts To extract the “predicted” random effects into a DataFrame use ranef.df &lt;- as.data.frame( blups.slope) head(ranef.df) ## grpvar term grp condval condsd ## 1 Subject (Intercept) 308 2.258551 12.07086 ## 2 Subject (Intercept) 309 -40.398738 12.07086 ## 3 Subject (Intercept) 310 -38.960409 12.07086 ## 4 Subject (Intercept) 330 23.690620 12.07086 ## 5 Subject (Intercept) 331 22.260313 12.07086 ## 6 Subject (Intercept) 332 9.039568 12.07086 This returns a data frame of the BLUPs for each random effect along with a “standard error” for each BLUP. What we discussed earlier in Section 1.3, were the BLUPs for \\(\\mathbf{x}_{ij}^{T}\\boldsymbol{\\beta} + \\mathbf{z}_{ij}\\mathbf{u}_{i}\\) not just the individual components of \\(\\mathbf{u}_{i}\\). For this random intercept and slope model, this is \\(\\textrm{BLUP}(\\beta_{0} + \\beta_{1}t_{j} + u_{i0} + u_{i1}t_{j})\\) These are obtained by using the fitted method blup.full &lt;- fitted( lmm.sleep.slope ) # Should be a vector of length 180 If we plot \\(\\textrm{BLUP}(\\beta_{0} + \\beta_{1}t_{j} + u_{i0} + u_{i1}t_{j})\\) as a function of time for all individuals, it will look like the following: library(ggplot2) # First add the BLUPs to the sleepstudy data as a separate variable sleepstudy$blups &lt;- blup.full # Now plot BLUPs vs. study data for each subject ggplot(sleepstudy, aes(x=Days, y=blups, group=Subject)) + geom_line(aes(color=Subject)) + labs(title = &quot;BLUPs in Random Intercept + Random Slope Model&quot;, y = &quot;Reaction Time&quot;) 1.5.2 Fitting Binary GLMMs using the Ohio data To use the ohio data, we will first load the geepack R package: library(geepack) This dataset has 2148 observations from 537 individuals data(ohio) head(ohio, 12) # look at first 12 rows of ohio ## resp id age smoke ## 1 0 0 -2 0 ## 2 0 0 -1 0 ## 3 0 0 0 0 ## 4 0 0 1 0 ## 5 0 1 -2 0 ## 6 0 1 -1 0 ## 7 0 1 0 0 ## 8 0 1 1 0 ## 9 0 2 -2 0 ## 10 0 2 -1 0 ## 11 0 2 0 0 ## 12 0 2 1 0 The outcome of interest in ohio is “wheezing status”: 1 - yes, 0 - no. The resp variable contains wheezing status. The id variable contains the unique identifier for each individual. The age in the ohio dataset is the time variable. The age variable is recorded as: (age in years - 9). Each individual starts the study at 7 years of age. The smoke variable is an indicator of maternal smoking at the starting year of the study. In lme4, fitting a GLMM with binary responses can be done with the glmer function. The glmer function has the following syntax: glmer(formula, data, family) The formula argument uses the same syntax as lmer When handling binary outcomes, you need to specify the family argument as: family = binomial. Just exploring this data by looking at the raw proportions, it appears that probability of wheezing decreases as age increases (within each level of smoking) maternal smoking increases the probability of wheezing at each age library(dplyr) prop_summary_ohio &lt;- ohio %&gt;% group_by(smoke, age) %&gt;% summarize( prop_wheeze = mean(resp) ) prop_summary_ohio ## # A tibble: 8 x 3 ## # Groups: smoke [2] ## smoke age prop_wheeze ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 -2 0.16 ## 2 0 -1 0.149 ## 3 0 0 0.143 ## 4 0 1 0.106 ## 5 1 -2 0.166 ## 6 1 -1 0.209 ## 7 1 0 0.187 ## 8 1 1 0.139 So, we are probably going to want to include both age and smoke in our model. 1.5.2.1 A Random Intercept Model Let’s use a GLMM to explore the relationship between wheezing status and the: age of the child maternal smoking status A GLMM for wheezing status which has age and smoking status as fixed effects and random individual-specific intercepts can be expressed as \\[\\begin{equation} \\textrm{logit}\\{ p_{ij}(u_{i}) \\} = \\beta_{0} + \\beta_{age}\\textrm{age}_{ij} + \\beta_{smk}\\textrm{smoke}_{i} + u_{i} \\tag{1.6} \\end{equation}\\] Model (1.6) can be fit with the following code # id is the grouping variable ohio.intercept &lt;- glmer(resp ~ age + smoke + (1 | id), data = ohio, family = binomial) coef(summary(ohio.intercept)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.3739539 0.27497502 -12.270038 1.311914e-34 ## age -0.1767645 0.06796698 -2.600741 9.302258e-03 ## smoke 0.4147806 0.28704052 1.445024 1.484510e-01 VarCorr(ohio.intercept) ## Groups Name Std.Dev. ## id (Intercept) 2.3432 For a binary GLMM, the estimated standard deviation for the random intercept can be a little hard to interpret, though this value seems rather large to me. One way to report this is to look at the variation in \\(p_{ij}(u_{i})\\) for different values of age and smoking status. For this purpose, you could report the interval \\(\\textrm{expit}\\big(\\hat{\\beta}_{0} + \\hat{\\beta}_{age}\\times \\textrm{age} + \\hat{\\beta}_{smk}\\times \\textrm{smoke} \\pm 1.64\\hat{\\sigma}_{u0} \\big)\\), where \\(\\textrm{expit}(x) = 1/(1 + e^{-x})\\) One way to interpret the variation visually is to randomly generate many values of \\(p_{ij}( u_{i} )\\) using the estimated distribution of \\(u_{i}\\) to simulate the values of \\(u_{i}\\). This can help us to get a sense of how much variability there is in wheezing probability across individuals. To do this, I simulated values of \\(p_{ij}( u_{i} )\\) for each combination of age/smoking status and plotted the results in 8 densities in 4 panels. It would probably be better to use some sort of bounded density estimator for these plots. beta.hat &lt;- coef(summary(ohio.intercept))[,1] n &lt;- 1000 pneg2.smoke &lt;- plogis(rnorm(n,sd=2.34) + beta.hat[1] - 2*beta.hat[2] + beta.hat[3]) #age -2 with smoke pneg2 &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1] - 2*beta.hat[2]) #age -2 w/o smoke pneg1.smoke &lt;- plogis(rnorm(n,sd=2.34) + beta.hat[1] - 1*beta.hat[2] + beta.hat[3]) #age -1 with smoke pneg1 &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1] - 1*beta.hat[2]) # age -1 w/o smoke p0.smoke &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1] + beta.hat[3]) # age 0 with smoke p0 &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1]) # age 0 w/o smoke p1.smoke &lt;- plogis(rnorm(n,sd=2.34) + beta.hat[1] + beta.hat[2] + beta.hat[3]) # age 1 with smoke p1 &lt;- plogis(rnorm(n, sd=2.34) + beta.hat[1] + beta.hat[2]) # age 1 w/o smoke par(mfrow=c(2,2), mar=c(4.1, 4.1, .5, .5)) plot(density(pneg2), lwd=2, xlab = &quot;Probability of Wheezing at Age 7&quot;, main=&quot;&quot;, col=&quot;red&quot;) d &lt;- density(pneg2.smoke) lines(d$x, d$y, lwd=2) legend(&quot;topright&quot;, legend = c(&quot;Smoke&quot;, &quot;No Smoke&quot;), col=c(&quot;black&quot;, &quot;red&quot;), bty=&#39;n&#39;, lwd=2) plot(density(pneg1), lwd=2, xlab = &quot;Probability of Wheezing at Age 8&quot;, main=&quot;&quot;, col=&quot;red&quot;) d &lt;- density(pneg1.smoke) lines(d$x, d$y, lwd=2) legend(&quot;topright&quot;, legend = c(&quot;Smoke&quot;, &quot;No Smoke&quot;), col=c(&quot;black&quot;, &quot;red&quot;), bty=&#39;n&#39;, lwd=2) plot(density(p0), lwd=2, xlab = &quot;Probability of Wheezing at Age 9&quot;, main=&quot;&quot;, col=&quot;red&quot;) d &lt;- density(p0.smoke) lines(d$x, d$y, lwd=2) legend(&quot;topright&quot;, legend = c(&quot;Smoke&quot;, &quot;No Smoke&quot;), col=c(&quot;black&quot;, &quot;red&quot;), bty=&#39;n&#39;, lwd=2) plot(density(p1), lwd=2, xlab = &quot;Probability of Wheezing at Age 10&quot;, main=&quot;&quot;, col=&quot;red&quot;) d &lt;- density(p1.smoke) lines(d$x, d$y, lwd=2) legend(&quot;topright&quot;, legend = c(&quot;Smoke&quot;, &quot;No Smoke&quot;), col=c(&quot;black&quot;, &quot;red&quot;), bty=&#39;n&#39;, lwd=2) Figure 1.3: Distribution of Wheezing probability across individuals for different values of age and smoking status References "],
["missing-data.html", "Chapter 2 Missing Data and Multiple Imputation 2.1 Missing Data in R and “Direct Approaches” for Handling Missing Data 2.2 Multiple Imputation 2.3 What is MICE doing? 2.4 Longitudinal Data 2.5 Different Missing Data Mechanisms", " Chapter 2 Missing Data and Multiple Imputation The book “Flexible Imputation of Missing Data” is a resource you also might find useful. It is available online at: https://stefvanbuuren.name/fimd/ 2.1 Missing Data in R and “Direct Approaches” for Handling Missing Data In a wide range of datasets, it is very common to encounter missing values. In R, missing values are stored as NA, meaning “Not Available”. For example, look at the airquality dataframe available in base R data(airquality) head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 You can see that the 5th observation for the Ozone variable is missing, and the 5th and 6th observations for the Solar.R variable are missing. You can use is.na to find which data entries have missing values. If is.na returns TRUE, the entry is missing. If is.na returns FALSE, the entry is not missing head( is.na(airquality) ) ## Ozone Solar.R Wind Temp Month Day ## [1,] FALSE FALSE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE FALSE ## [3,] FALSE FALSE FALSE FALSE FALSE FALSE ## [4,] FALSE FALSE FALSE FALSE FALSE FALSE ## [5,] TRUE TRUE FALSE FALSE FALSE FALSE ## [6,] FALSE TRUE FALSE FALSE FALSE FALSE Computing the sum of is.na(airquality) tells us how many missing values there are in this dataset. sum( is.na(airquality) ) ## 44 missing entries in total ## [1] 44 dim( airquality ) ## Dataset has a total of 153 x 6 = 918 entries ## [1] 153 6 2.1.1 Complete Case Analysis (Listwise Deletion) Suppose we wanted to run a regression with Ozone as the response and Solar.R, Wind, and Temp as the covariates. If we do this in R, we will get the following result: air.lm1 &lt;- lm( Ozone ~ Solar.R + Wind + Temp, data = airquality ) summary( air.lm1 ) ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.485 -14.219 -3.551 10.097 95.619 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -64.34208 23.05472 -2.791 0.00623 ** ## Solar.R 0.05982 0.02319 2.580 0.01124 * ## Wind -3.33359 0.65441 -5.094 1.52e-06 *** ## Temp 1.65209 0.25353 6.516 2.42e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21.18 on 107 degrees of freedom ## (42 observations deleted due to missingness) ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.5948 ## F-statistic: 54.83 on 3 and 107 DF, p-value: &lt; 2.2e-16 How is R handling all the missing values in airquality. As a default, lm conducts a complete case analysis (sometimes referred to as listwise deletion). (This is the default unless you changed the default na.action setting with options(...)) A complete case analysis for regression will first delete all rows from the dataset if any of the variables used from that row have a missing value. In this example, a row will be dropped if either the value of Ozone, Solar.R, Wind, or Temp is missing. After these observations have been deleted, the usual regression is fit to the remaining “complete” dataset. To remove all rows where there is at least one missing value, you can use na.omit: complete.air &lt;- na.omit( airquality ) ## Check that there are no missing values sum( is.na(complete.air) ) # This should be zero ## [1] 0 Because our regression model only uses the variables Let’s now fit a linear regression using the complete dataset complete.air air.lm2 &lt;- lm( Ozone ~ Solar.R + Wind + Temp, data = complete.air ) The estimated regression coefficients should be the same when using the “incomplete dataset” airquality as when using the “complete dataset” complete.air ## The estimated regression coefficients should be the same round(air.lm1$coefficients, 3) ## (Intercept) Solar.R Wind Temp ## -64.342 0.060 -3.334 1.652 round(air.lm2$coefficients, 3) ## (Intercept) Solar.R Wind Temp ## -64.342 0.060 -3.334 1.652 2.1.2 Other “Direct” Methods In general, doing a complete-case analysis is not advisable. A complete-case analysis should really only be used if you are confident that the data are missing completely at random (MCAR). A few other direct ways of handling missing data that you may have used or seen used in practice include: Mean imputation. Missing values are replaced by the value of the mean of that variable. Regression imputation. Missing values are replaced by a regression prediction from the values of the other variables. 2.2 Multiple Imputation 2.2.1 Short Overview of Multiple Imputation To summarize, multiple imputation consists of the following steps: Create \\(K\\) different “complete datasets” which contain no missing data. For each of these \\(K\\) complete datasets, compute the estimates of interest. “Pool” these separate estimates to get a final estimate. The nice thing about multiple imputation is that you can always use your original analysis approach. That is, you can just apply your original analysis method to each of the \\(K\\) complete datasets. The complicated part in multiple imputation is generating the \\(K\\) complete datasets in a “valid” way. Luckily, there are a number of R packages that implement different ways of creating the \\(K\\) imputed datasets. 2.2.2 Multiple imputation with mice I will primarily focus on the mice package. mice stands for “Multivariate Imputation by Chained Equations” The mice function within the mice package is the primary function for performing multiple imputation. To use mice, just use mice(df) where df is the name of the dataframe. (Set print = FALSE if you don’t want it to print out the number of the iteration). Choose a value of seed so that the results are reproducible. library(mice) imputed.airs &lt;- mice(airquality, print=FALSE, seed=101) The object returned by mice will have a component called imp which is a list. Each component of imp is a dataframe corresponding to a single variable in the original dataframe This dataframe will contain the imputed values for the missing values of that variable. For example, imputed.airs$imp will be a list with each component of the list being one of the variables from airquality names( imputed.airs$imp ) ## [1] &quot;Ozone&quot; &quot;Solar.R&quot; &quot;Wind&quot; &quot;Temp&quot; &quot;Month&quot; &quot;Day&quot; The Ozone component of imputed.airs$imp will be a dataframe with 37 rows and 5 columns. This is because the Ozone variable had 37 missing values, and there are 5 multiple imputations (the default number in mice) dim(imputed.airs$imp$Ozone) ## [1] 37 5 ## Imputed missing ozone values across the five multiple imputations head(imputed.airs$imp$Ozone) ## 1 2 3 4 5 ## 5 6 8 18 6 37 ## 10 12 18 27 18 30 ## 25 8 14 6 18 18 ## 26 13 1 13 37 13 ## 27 19 18 4 18 34 ## 32 40 47 45 23 18 The row names in imputed.airs$imp$Ozone correspond to the index of the observation in the original airquality dataframe. For example, the 5th observation of the Ozone variable has 6 in the 1st imputation, 8 in the 2nd imputation, 18 in the 3rd imputation, etc. …. Similarly, the Solar.R component of imputed.airs$imp will be a data frame This is because the Ozone variable had 7 missing values, and there are 5 multiple imputations. dim(imputed.airs$imp$Solar.R) ## [1] 7 5 ## Imputed missing ozone values across the five multiple imputations head(imputed.airs$imp$Solar.R) ## 1 2 3 4 5 ## 5 131 285 274 92 139 ## 6 127 248 175 167 175 ## 11 71 71 238 115 284 ## 27 238 8 49 223 238 ## 96 258 203 229 223 291 ## 97 313 259 274 83 272 For example, the 5th observation of the Solar.R variable has 131 in the 1st imputation, 285 in the 2nd imputation, 274 in the 3rd imputation, etc. …. 2.2.2.1 with(), pool(), complete() You could use the components of imputed.airs$imp to directly fit 5 separate regression on the multiply imputed datasets and then average the results. However, this is much easier if you just use the with function from mice air.multi.imputelm &lt;- with(imputed.airs, lm( Ozone ~ Solar.R + Wind + Temp)) This will produce 5 different sets of estimates of the regression coefficients: summary(air.multi.imputelm) ## # A tibble: 20 x 6 ## term estimate std.error statistic p.value nobs ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 (Intercept) -90.3 19.1 -4.72 5.44e- 6 153 ## 2 Solar.R 0.0642 0.0199 3.22 1.56e- 3 153 ## 3 Wind -2.31 0.550 -4.20 4.61e- 5 153 ## 4 Temp 1.83 0.212 8.64 8.13e-15 153 ## 5 (Intercept) -49.4 19.0 -2.60 1.02e- 2 153 ## 6 Solar.R 0.0548 0.0196 2.79 5.92e- 3 153 ## 7 Wind -3.43 0.545 -6.28 3.44e- 9 153 ## 8 Temp 1.46 0.211 6.92 1.27e-10 153 ## 9 (Intercept) -56.7 19.0 -2.98 3.36e- 3 153 ## 10 Solar.R 0.0646 0.0199 3.25 1.42e- 3 153 ## 11 Wind -3.22 0.546 -5.90 2.38e- 8 153 ## 12 Temp 1.50 0.211 7.09 4.97e-11 153 ## 13 (Intercept) -58.4 18.9 -3.09 2.41e- 3 153 ## 14 Solar.R 0.0687 0.0199 3.46 7.08e- 4 153 ## 15 Wind -3.27 0.544 -6.01 1.35e- 8 153 ## 16 Temp 1.57 0.210 7.50 5.33e-12 153 ## 17 (Intercept) -58.9 22.2 -2.66 8.69e- 3 153 ## 18 Solar.R 0.0404 0.0231 1.75 8.27e- 2 153 ## 19 Wind -3.31 0.636 -5.21 6.11e- 7 153 ## 20 Temp 1.64 0.245 6.68 4.37e-10 153 To get the “pooled estimates and standard errors” from these 5 different sets of regression coefficients use the pool function from mice: summary( pool(air.multi.imputelm) ) ## term estimate std.error statistic df p.value ## 1 (Intercept) -62.73141323 26.25695768 -2.389135 16.63162 2.903387e-02 ## 2 Solar.R 0.05855766 0.02400718 2.439173 36.55628 1.969933e-02 ## 3 Wind -3.10764673 0.75290758 -4.127527 16.76039 7.227036e-04 ## 4 Temp 1.60026617 0.27155700 5.892929 23.89788 4.511087e-06 The pooled “final estimates” of the regression coefficients are just the means of the estimated regression coefficients from the 5 multiply imputed datasets. The pooled standard error for the \\(j^{th}\\) regression coefficient is given by \\[\\begin{equation} (\\textrm{pooled } SE_{j})^{2} = \\frac{1}{K}\\sum_{k=1}^{K} SE_{jk}^{2} + \\frac{K+1}{K(K-1)}\\sum_{k=1}^{K}(\\hat{\\beta}_{jk} - \\bar{\\hat{\\beta}}_{j.})^{2}, \\end{equation}\\] where \\(SE_{jk}\\) is the standard error for the \\(j^{th}\\) regression coefficient from the \\(k^{th}\\) complete dataset, and \\(\\hat{\\beta}_{jk}\\) is the estimate of \\(\\beta_{j}\\) from the \\(k^{th}\\) complete dataset. It is sometimes useful to actually extract each of the completed datasets. This is true, for example, in longitudinal data where you may want to go back and forth between “wide” and “long” formats. To extract each of the completed datasets, you can use the complete function from mice. The following code will return the 5 complete datasets from imputed.airs completed.airs &lt;- mice::complete(imputed.airs, action=&quot;long&quot;) completed.airs will be a dataframe that has 5 times as many rows as the airquality data frame The variable .imp is an indicator of which of the 5 imputations that row corresponds to. head(completed.airs) ## .imp .id Ozone Solar.R Wind Temp Month Day ## 1 1 1 41 190 7.4 67 5 1 ## 2 1 2 36 118 8.0 72 5 2 ## 3 1 3 12 149 12.6 74 5 3 ## 4 1 4 18 313 11.5 62 5 4 ## 5 1 5 6 131 14.3 56 5 5 ## 6 1 6 28 127 14.9 66 5 6 dim(completed.airs) ## [1] 765 8 dim(airquality) ## [1] 153 6 Using complete.airs, we can compute the multiple imputation-based estimates of the regression coefficients “by hand” This should give us the same results as when using with BetaMat &lt;- matrix(NA, nrow=5, ncol=4) for(k in 1:5) { ## Find beta.hat from kth imputed dataset BetaMat[k,] &lt;- lm(Ozone ~ Solar.R + Wind + Temp, data=completed.airs[completed.airs$.imp==k,])$coefficients } round(colMeans(BetaMat), 3) # compare with the results from using the &quot;with&quot; function ## [1] -62.731 0.059 -3.108 1.600 2.3 What is MICE doing? Suppose we have data from \\(q\\) variables \\(Z_{i1}, \\ldots, Z_{iq}\\). Let \\(\\mathbf{Z}_{mis}\\) denote the collection of missing observations and \\(\\mathbf{Z}_{obs}\\) the collection of observed values, and let \\(\\mathbf{Z} = (\\mathbf{Z}_{obs}, \\mathbf{Z}_{mis})\\). The basic idea behind multiple imputation is to, in some way, generate samples \\(\\mathbf{Z}_{mis}^{(1)}, \\ldots, \\mathbf{Z}_{mis}^{(K)}\\) from a flexible probability model \\(p(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs})\\) \\(p(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs})\\) represents the conditional distribution of \\(\\mathbf{Z}_{mis}\\) given the observed \\(\\mathbf{Z}_{obs}\\). The parameter of interest \\[\\begin{eqnarray} \\hat{\\theta} &amp;=&amp; E( \\theta |\\mathbf{Z}_{obs} ) = \\int E\\Big\\{ \\theta \\Big| \\mathbf{Z}_{obs}, \\mathbf{Z}_{mis} \\Big\\} p(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs}) d\\mathbf{Z}_{mis} \\nonumber \\\\ &amp;\\approx&amp; \\frac{1}{K} \\sum_{k=1}^{K} E\\Big\\{ \\theta \\Big| \\mathbf{Z}_{obs}, \\mathbf{Z}_{mis}^{(k)} \\Big\\} \\end{eqnarray}\\] There are two main approaches for setting up a model for which you can sample \\(\\mathbf{Z}_{mis}\\) from the conditional distribution of \\(\\mathbf{Z}_{mis}|\\mathbf{Z}_{obs}\\). One approach is to directly specify a full joint model for \\(\\mathbf{Z} = (\\mathbf{Z}_{mis}, \\mathbf{Z}_{obs})\\) For example, assume that \\(\\mathbf{Z}\\) follows a multivariate a normal distribution. The fully conditional specification (FCS) approach specifies the distribution of each variable \\(\\mathbf{Z}_{j}\\) conditional on the remaining variables \\(\\mathbf{Z}_{-j}\\). The FCS approach is the one used by mice. With mice, 2.4 Longitudinal Data A direct way to do multiple imputation with longitudinal data is to use mice on the dataset stored in wide format. Remember that in wide format, each row corresponds to a different individual. Applying multiple imputation to the wide-format dataset can account for the fact that observations across individuals will be correlated. Let’s look at the ohio data from the geepack package again library(geepack) data(ohio) head(ohio) ## resp id age smoke ## 1 0 0 -2 0 ## 2 0 0 -1 0 ## 3 0 0 0 0 ## 4 0 0 1 0 ## 5 0 1 -2 0 ## 6 0 1 -1 0 The ohio dataset is in long format. We need to first convert this into wide format. With tidyr you can convert from long to wide using spread: (Use gather to go from wide to long) library( tidyr ) ohio.wide &lt;- spread(ohio, key=age, value=resp) ## Change variable names to so that ages go from 7 to 10 names(ohio.wide) &lt;- c(&quot;id&quot;, &quot;smoke&quot;, &quot;age7&quot;, &quot;age8&quot;, &quot;age9&quot;, &quot;age10&quot;) head(ohio.wide) ## id smoke age7 age8 age9 age10 ## 1 0 0 0 0 0 0 ## 2 1 0 0 0 0 0 ## 3 2 0 0 0 0 0 ## 4 3 0 0 0 0 0 ## 5 4 0 0 0 0 0 ## 6 5 0 0 0 0 0 The variable age7 now represents the value of resp at age 7, age8 represents the value of resp at age 8, etc… reshape from base R can also be used to go from long to wide # Example of using reshape #ohio.wide2 &lt;- reshape(ohio, v.names=&quot;resp&quot;, idvar=&quot;id&quot;, timevar=&quot;age&quot;, direction=&quot;wide&quot;) The ohio dataset does not have any missing values. Let’s introduce missing values for the variable resp values by assuming that the probability of being missing is positively related to smoking status. Let \\(R_{ij}\\) be an indicator of missingness of resp for individual \\(i\\) at the \\(j^{th}\\) follow-up time. When randomly generating missing values, we will assume that: \\[\\begin{equation} P( R_{ij} = 1| \\textrm{smoke}_{i}) = \\begin{cases} 0.05 &amp; \\textrm{ if } \\textrm{smoke}_{i} = 0 \\\\ 0.25 &amp; \\textrm{ if } \\textrm{smoke}_{i} = 1 \\end{cases} \\tag{2.1} \\end{equation}\\] To generate missing values according to assumption (2.1), we can use the following R code: We will call the new data frame ohio.wide.miss ohio.wide.miss &lt;- ohio.wide m &lt;- nrow(ohio.wide.miss) ## number of individuals in study for(k in 1:m) { resp.values &lt;- ohio.wide[k, 3:6] # values of resp for individual k if(ohio.wide[k,2] == 1) { # if smoke = 1 Rij &lt;- sample(0:1, size=4, replace=TRUE, prob=c(0.75, 0.25)) } else { # if smoke = 0 Rij &lt;- sample(0:1, size=4, replace=TRUE, prob=c(0.95, 0.05)) } resp.values[Rij==1] &lt;- NA # insert NA values where Rij = 1 ohio.wide.miss[k, 3:6] &lt;- resp.values } head(ohio.wide.miss, 10) ## id smoke age7 age8 age9 age10 ## 1 0 0 0 0 NA 0 ## 2 1 0 0 0 0 0 ## 3 2 0 0 0 0 0 ## 4 3 0 0 0 0 0 ## 5 4 0 0 0 0 0 ## 6 5 0 0 0 0 0 ## 7 6 0 0 0 0 0 ## 8 7 0 0 NA 0 0 ## 9 8 0 0 0 0 0 ## 10 9 0 0 0 0 0 ohio.wide.miss now has 257 missing entries sum( is.na(ohio.wide.miss)) ## [1] 257 Before using multiple imputation with ohio.wide.miss, let’s look at the regression coefficient estimates that would be obtained with a complete case analysis. To use glmer on the missing-data version of ohio, we need to first convert ohio.wide.miss back into long form: ohio.miss &lt;- gather(ohio.wide.miss, age, resp, age7:age10) ohio.miss$age[ohio.miss$age == &quot;age7&quot;] &lt;- -2 ohio.miss$age[ohio.miss$age == &quot;age8&quot;] &lt;- -1 ohio.miss$age[ohio.miss$age == &quot;age9&quot;] &lt;- 0 ohio.miss$age[ohio.miss$age == &quot;age10&quot;] &lt;- 1 ohio.miss &lt;- ohio.miss[order(ohio.miss$id),] ## sort everything according to id head(ohio.miss) ## id smoke age resp ## 1 0 0 -2 0 ## 538 0 0 -1 0 ## 1075 0 0 0 NA ## 1612 0 0 1 0 ## 2 1 0 -2 0 ## 539 1 0 -1 0 Let’s use a random intercept model as we did in our earlier discussion of generalized linear mixed models: ## Complete case analysis library(lme4) ohio.cca &lt;- glmer(resp ~ age + smoke + (1 | id), data = ohio, family = binomial) # Now look at estimated regression coefficients for complete case analysis: round(coef(summary(ohio.cca)), 4) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.3740 0.275 -12.2700 0.0000 ## age -0.1768 0.068 -2.6007 0.0093 ## smoke 0.4148 0.287 1.4450 0.1485 Now, let’s use mice to create several “completed versions” of ohio.wide.miss imputed.ohio &lt;- mice(ohio.wide.miss, print=FALSE, seed=101) For the case of longitudinal data, we probably want to actually extract each complete dataset. (This is because many of the analysis methods such as lmer assume the data is in long form). This can be done with the following code: completed.ohio &lt;- mice::complete(imputed.ohio, &quot;long&quot;) head(completed.ohio) ## .imp .id id smoke age7 age8 age9 age10 ## 1 1 1 0 0 0 0 0 0 ## 2 1 2 1 0 0 0 0 0 ## 3 1 3 2 0 0 0 0 0 ## 4 1 4 3 0 0 0 0 0 ## 5 1 5 4 0 0 0 0 0 ## 6 1 6 5 0 0 0 0 0 completed.ohio will be a dataframe that has 5 times as many rows as the original ohio.wide data frame dim(ohio.wide) ## [1] 537 6 dim(completed.ohio) ## [1] 2685 8 The variable .imp in completed.ohio is an indicator of which of the five “imputed datasets” this is from: table( completed.ohio$.imp ) # Tabulate impute indicators ## ## 1 2 3 4 5 ## 537 537 537 537 537 2.5 Different Missing Data Mechanisms 2.5.1 Missing Completely at Random (MCAR) 2.5.2 Missing at Random (MAR) 2.5.3 Missing not at Random (MNAR) "]
]
