# Mixed Models for Longitudinal Data Analysis {#mixed-models}


---
  
## Methods for Analyzing Longitudinal Data {#sec:methods-overview}

* **Longitudinal data** refers to data that:
    + Has multiple individuals/subjects.  
    
    + Each individual has multiple observations that were taken across time. 

* We will denote the outcomes of interest with $Y_{ij}$.
    + $Y_{ij}$ - outcome for individual $i$ at time $t_{ij}$.
    
    + The $i^{th}$ individual has $n_{i}$ observations: $Y_{i1}, \ldots, Y_{in_{i}}$.
    
    + There will be $m$ individuals in the study (so $1 \leq j \leq m$).

---

```{r, echo=FALSE, message=FALSE}
library(lme4)
data(sleepstudy)
ids <- unique(sleepstudy$Subject)

plot(sleepstudy$Days, sleepstudy$Reaction, type="n",
     ylab="Reaction Time", xlab="Day of Study", las=1,
     main="Sleep Study Longitudinal Data")
for(k in 1:7) {
    ind <- sleepstudy$Subject == ids[k]
    lines(sleepstudy$Days[ind], sleepstudy$Reaction[ind])
    points(sleepstudy$Days[ind], sleepstudy$Reaction[ind], pch=16)
}
```

* The above figure shows an example of outcomes from a longitudinal study (the **sleepstudy** data in the **lme4** package).
   
* In the **sleepstudy** data:   

    + The time points of observation $t_{ij}$ are the same for each individual $i$. So, we can say $t_{ij} = t_{j}$ for all $i$.

    + The outcome $Y_{ij}$ is the **reaction time** for the $i^{th}$ individual at time point $t_{j}$.

    + The 10 time points are $(t_{1}, \ldots, t_{10}) = (0, 1, \ldots, 9)$.

---

* Most of the well-known regression-based methods for analyzing longitudinal
data can be classified (see @diggle2013) into one of the three following
categories: 
    + **Random effects/mixed models**, 
    + **Marginal models**, 
    + **Transition models**

* **Random effects/Mixed Models**
    + "Random effects" are added to the regression model describing
    the outcomes for each individual.
    
    + These "random regression coefficients" are viewed as a sample from some distribution.
    
* **Marginal models**
    + Regression coefficients have a "population average" interpretation.

    + Only mean of $Y_{ij}$ and correlation structure of $(Y_{i1}, \ldots, Y_{in_{i}})$ are modeled.
    
    + Generalized estimating equations (GEEs) are often used for estimating model parameters.
    

* **Transition models**
    + Uses a probability model for the distribution of $Y_{ij}$ given the value of the outcome
    at the previous time point $Y_{ij-1}$.
    

## Mixed Models for Continuous Outcomes

* If each $Y_{ij}$ is a **continuous outcome** and we were to 
build a regression model without any random effects, we might assume something like:
\begin{equation}
Y_{ij} = \beta_{0} + \mathbf{x}_{ij}^{T}\boldsymbol{\beta} + e_{ij}
(\#eq:fixed-reg-model)
\end{equation}

* $\mathbf{x}_{ij} = (x_{i1}, \ldots, x_{ip})$ is the vector
of covariates for individual $i$ at time $j$.

* The vector $\mathbf{x}_{ij}$ could contain individual information such as smoking status or age. 
    + $\mathbf{x}_{ij}$ could also contain some of the actual time points: $t_{ij}, t_{ij-1}, ...$
or transformations of these time points.

---

* The regression model \@ref(eq:fixed-reg-model) assumes the same
mean function $\beta_{0} + \mathbf{x}_{ij}^{T}\boldsymbol{\beta}$ holds for all individuals in the study.

* It is often reasonable to assume that the regression coefficients across vary across individuals. 
    + This can often better account for heterogeneity across individuals.

* The figure below shows 3 different regression lines from the **sleepstudy** data.
    + Each regression line was estimated using only data from one individual.


```{r, echo=FALSE, fig.cap="Separately estimated regression lines for 3 subjects in the sleepstudy data."}
pc <- c(1,4,3,4,5,6,16)
cls <- c("black", "red", "black", "black","black","black", "blue")
plot(sleepstudy$Days, sleepstudy$Reaction, type="n",
     ylab="Reaction Time", xlab="Day of Study", las=1,
     main="Estimated Regression Lines for 3 Subjects")
for(k in c(1,2,7)) {
  ind <- sleepstudy$Subject == ids[k]
  points(sleepstudy$Days[ind], sleepstudy$Reaction[ind], pch=pc[k])
  tmp <- lm(Reaction ~ Days, data=sleepstudy[sleepstudy$Subject==ids[k],])
  abline(tmp$coef[1], tmp$coef[2], col=cls[k], lwd=2)
}
legend("topleft", legend=c("Subject 308", "Subject 309", "Subject 333"), pch=c(1,4,16),
       bty='n')
```

---

* Figure 1.1 suggests there is some heterogeneity in the **relationship**
between **study day** and **response time** across individuals.

* The response time of **Subject 309** changes very little over time.

* For **Subject 308**, there is a more clear positive association between
response time and day of study.

---

* For the **sleepstudy** data, a linear regression for **reaction time** vs. **study day**
which assumes that 
    1. Expected response time is a linear function of study day, 
    2. All individuals have the same regression coefficients, 

would have the form:
\begin{equation}
Y_{ij} = \beta_{0} + \beta_{1} t_{j} + e_{ij}
\end{equation}

* If we allowed each individual to have his/her **own intercept and slope**, we 
could instead consider the following model
\begin{equation}
Y_{ij} = \beta_{0} + \beta_{1} t_{j} + u_{i0} + u_{i1}t_{j} + e_{ij}
(\#eq:mixed-sleep)
\end{equation}

* $\beta_{0} + u_{i0}$ - intercept for individual $i$.
* $\beta_{1} + u_{i1}$ - intercept for individual $i$.

---

* If we assume $(u_{i0}, u_{i1})$ are sampled from some distribution, $u_{i0}$ and 
$u_{i1}$ are referred to as **random effects**.

* Typically, it is assumed that $(u_{i0}, u_{i1})$ are sampled from a multivariate normal distribution
with mean zero:
\begin{equation}
(u_{i0}, u_{i1}) \sim \textrm{Normal}( \mathbf{0}, \boldsymbol{\Sigma}_{\tau} )
\end{equation}

* Model \@ref(eq:mixed-sleep) is called a **mixed model** because 
it contains both **fixed effects** $(\beta_{0}, \beta_{1})$
and **random effects** $(u_{i0}, u_{i1})$.

---

* More generally, a **linear mixed model** (LMM) for longitudinal data will have the form:
\begin{equation}
Y_{ij} = \beta_{0} + \mathbf{x}_{ij}^{T}\boldsymbol{\beta} + \mathbf{z}_{ij}^{T}\mathbf{u}_{i} + e_{ij}
(\#eq:lmm-generalform)
\end{equation}
    + $\boldsymbol{\beta}$ - vector of fixed effects
    + $\mathbf{u}_{i}$ - vector of random effects

* If we stack the responses into a long vector $\mathbf{Y}$ and random effects into a long vector $\mathbf{u}$
    + $\mathbf{Y} = (Y_{11}, Y_{12}, ...., Y_{mn_{m}})$ - this vector has length $\sum_{k=1}^{m} n_{k}$
    + $\mathbf{u} = (u_{10}, u_{11}, ...., u_{mq})$ - this vector has length $m \times (q + 1)$.

* Then, we can write the general form \@ref(eq:lmm-generalform) of the LMM as
\begin{equation}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\mathbf{u} + \mathbf{e}
\end{equation}
    + $i^{th}$ row of $\mathbf{X}$ is $(1, \mathbf{x}_{ij}^{T})$.
    + $i^{th}$ row of $\mathbf{Z}$ is $\mathbf{z}_{ij}^{T}$.

* Constructing an LMM can be thought of as choosing the desired "$\mathbf{X}$" and "$\mathbf{Z}$" matrices.

## Advantages of using random effects

### Within-subject correlation

* Using an LMM automatically accounts for the "**within-subject**" correlation.
    + That is, the correlation between two observations from the same individual.

* This correlation arises because observations on the same individual "share" **common** random effects.

* The correlation between the $j^{th}$ and $k^{th}$ observation from individual $i$ is
\begin{equation}
\textrm{Corr}(Y_{ij}, Y_{ik}) = \frac{ \mathbf{z}_{ij}^{T}\boldsymbol{\Sigma}_{\tau}\mathbf{z}_{ik}  }{ \sqrt{\mathbf{z}_{ij}^{T}\boldsymbol{\Sigma}_{\tau}\mathbf{z}_{ij}}\sqrt{\mathbf{z}_{ik}^{T}\boldsymbol{\Sigma}_{\tau}\mathbf{z}_{ik}}}
\end{equation}

---

* 

#### Inference about Heterogeneity - Variance of Random Effects

* One of the goals of the data analysis may be to characterize
the **heterogeneity** in the relationship between the outcome
and some of the covariates across individuals.

* Looking at the estimates of the variance of the random effects
can help to address this goal. 

* An estimate of $\textrm{Var}( u_{ih} )$ "substantially greater than zero" 
is an indication that there is variability in the regression coefficient corresponding to $u_{ij}$
across individuals.

#### BLUPs

* BLUPs




## Generalized linear mixed models (GLMMs)

* Generalized linear models (GLMs) are used to handle "non-continuous" data
that can't be reasonably modeled with a Gaussian distribution.

* The most common scenarios where you would use GLMs in practice
are **binary**, **count**, and perhaps multinomial outcomes.

* With a generalized linear mixed model (GLMM), you assume that
a GLM holds conditional on the value of the random effects.

### GLMMs with Binary Outcomes

* Under the GLM framework, the usual approach for handling binary outcomes is **logistic regression**.

* The assumptions underying logistic regression are:
    + The outcomes are **independent**
    
    + Each outcome follows a **Bernoulli** distribution.
    
    + The **log-odds parameter** is assumed to be a linear combination of the covariates.

----
    
* With the GLMM version of logistic regression, we will make almost the same assumptions as the regular GLM
version of logistic regression. 
    + The main difference is that each assumption in the GLMM will be **conditional** on the values of the random effects.

* To be specific, for longitudinal binary outcomes $Y_{ij}$,
the  GLMM version of logistic regression assumes the following:
    1. **Conditional** on the vector of random effects $\mathbf{u}_{i}$
\begin{equation}
Y_{i1}, \ldots, Y_{in_{i}}|\mathbf{u}_{i}  \textrm{ are independent }
\end{equation}
    
    2. **Conditional** on $\mathbf{u}_{i}$, each $Y_{ij}$ has a Bernoulli distribution
\begin{equation}
Y_{ij}|\mathbf{u}_{i} \sim \textrm{Bernoulli}\big( p_{ij}(\mathbf{u}_{i}) \big)
\end{equation}
so that $p_{ij}( \mathbf{u}_{i} ) = P(Y_{ij} = 1| \mathbf{u}_{i})$.

    3. The "**conditional**" log-odds term $\log\{ p_{ij}(\mathbf{u}_{i})/[1 - p_{ij}(\mathbf{u}_{i})] \}$
is a linear combination of the covariates and the random effects vector $\mathbf{u}_{i}$:
\begin{equation}
\textrm{logit}\{ p_{ij}(\mathbf{u}_{i}) \} = \log\Big( \frac{ p_{ij}(\mathbf{u}_{i})}{ 1 - p_{ij}(\mathbf{u}_{i}) } \Big)
= \beta_{0} + \mathbf{x}_{ij}^{T}\boldsymbol{\beta} + \mathbf{z}_{ij}^{T}\mathbf{u}_{i}
\end{equation}

    4. As with a linear mixed model, we assume that the random-effects vector $\mathbf{u}_{i}$ has a **multivariate normal distribution** with mean zero and covariance matrix $\boldsymbol{\Sigma}_{\tau}$
    \begin{equation}
        \mathbf{u}_{i} \sim \textrm{Normal}( \mathbf{0}, \boldsymbol{\Sigma}_{\tau})
    \end{equation}
    
    
    
### GLMMs with Count Outcomes

* For **count** outcomes, responses are typically assumed to follow a **Poisson** or a **negative binomial** distribution - conditional on the values of the random effects model.

* If $Y_{ij}|\mathbf{u}_{i} \sim \textrm{Poisson}\{ \mu_{ij}( \mathbf{u}_{i} ) \}$,
\begin{equation}
E(Y_{ij}| \mathbf{u}_{i}) = \mu_{ij}(\mathbf{u}_{i})  \qquad 
\textrm{Var}( Y_{ij}| \mathbf{u}_{i} ) = \mu_{ij}(\mathbf{u}_{i})
\end{equation}

* The log of the conditional mean $\mu_{ij}(\mathbf{u}_{i})$ is modeled with a linear regression:
\begin{equation}
\log\{ \mu_{ij}(\mathbf{u}_{i}) \} = \beta_{0} + \mathbf{x}_{ij}^{T}\boldsymbol{\beta} + \mathbf{z}_{ij}^{T}\mathbf{u}_{i}
\end{equation}

## Fitting Linear Mixed Models (LMMs) and Generalized Linear Mixed models (GLMMs) in **R**

* The **lme4** package is probably the most general package
for fitting LMMs and GLMMs.

```{r, echo=TRUE, eval=FALSE}
library(lme4)
```

### Fitting LMMs with the sleepstudy data

* To start off, let's use the **sleepstudy** longitudinal data in **lme4**
and look at the data from the first two individuals in this data.

```{r}
data(sleepstudy) 
dim(sleepstudy) # 18 individuals, each with 10 observations
sleepstudy[1:20,] # Data from the subjects with ids: 308 and 309
```

* The **sleepstudy** data is an example of longitudinal data stored in **long format** 
(as opposed to "wide" format).
    + In **long format**, each row of the dataset corresponds to an observation from one individual at one time point.

---

* The **lmer** function in **lme4** fits linear mixed models. 
    + This has many of the same features as the **lm** function in **R**.

* To fit an LMM with **lmer**, the main thing to do is to specify
the "X" part of the model (i.e., the fixed effects) and the "Z" part of the 
model (i.e., the random effects).

* The "X" part of the model is done using 
the exact same "formula notation" used in the **lm** function.

* The "Z" part of the model is done using the following type of syntax:
```{r, echo=TRUE, eval=FALSE}
(formula | group_var)
```

* `group_var` is the "grouping variable" used for the random effects
    + This would be the variable telling you which 

#### LMM with a single, random intercept for each subject

* Let's fit an LMM where there is a fixed slope for time 
and only a random intercept for each `Subject`
\begin{equation}
Y_{ij} = \beta_{0} + \beta_{1}t_{j} + u_{i} + e_{ij}
(\#eq:lmm-intercept-sleep)
\end{equation}

* For the "X" part of this model, we use `Reaction ~ Days`.
   + This gives us a fixed intercept and a fixed slope for the `Days` variable.

* For the "Z" part of this model, we just add `(1|Subject)`.
   + This says that there is only a random intercept within the grouping variable `Subject`.
   
* Putting these two together, we can fit the LMM \@ref(eq:lmm-intercept-sleep) using the following code:
```{r, echo=TRUE}
lmm.sleep.intercept <- lmer(Reaction ~ Days + (1|Subject), data = sleepstudy)
```

---

* You can always use the `model.matrix` method on
the fitted `lmer` object to check that the "X" and "Z" matrices 
correspond to the model you want.

* Let's look at the first 5 rows of the "X" matrix from `lmm.sleep.intercept`
```{r, echo=TRUE}
x.mat <- model.matrix(lmm.sleep.intercept)
## This design matrix should have an intercept column
## and a column which stores the "Days" variable
x.mat[1:5,]
```

* Let's look at the first 20 rows of the "Z" matrix from `lmm.intercept`
```{r, echo=TRUE}
## Use argument type = "random" to get random-effects design matrix
z.mat <- model.matrix(lmm.sleep.intercept, type="random")
z.mat[1:20,] # The . values in zmat correspond to zeros
```

* The `.` values in `z.mat` are just zeros.

* Notice that each `Subject` has its own "intercept" column. 
   + This what we want - each `Subject` has its own intercept.
    
---

* Let's look at the **estimated parameters** from the LMM
with random intercepts using `summary`

```{r, echo=TRUE}
summary(lmm.sleep.intercept)
```

* The estimated fixed-effects intercept is $\hat{\beta}_{0} = 251.4$,
and the estimated fixed-effects slope is $\hat{\beta}_{1} = 10.5$.

* The estimated variance of the random intercept is $\hat{\tau}^{2} = 1378.2$
(standard deviation is $\hat{\tau} = 37.1$).
    + i.e., it is estimated that $u_{i} \sim \textrm{Normal}(0, 1378.2)$.

#### LMM with both a random intercept and slope for each subject

* Now, let's fit an LMM where there is a fixed slope for time 
and both a random intercept and slope for each `Subject`
\begin{equation}
Y_{ij} = \beta_{0} + \beta_{1}t_{j} + u_{i0} + u_{i1}t_{j} + e_{ij}
(\#eq:lmm-slope-sleep)
\end{equation}

* This is done with `lmer` using the following code:
```{r, echo=TRUE}
lmm.sleep.slope <- lmer(Reaction ~ Days + (Days|Subject), data = sleepstudy)
```

* Again, let's check the "X" and "Z" matrices from 
`lmm.sleep.slope` to double-check that everything makes sense
```{r, echo=TRUE}
x.mat2 <- model.matrix(lmm.sleep.slope)
## This design matrix should be the same as that from lmm.sleep.intercept
x.mat2[1:5,]
```

* First 20 rows of the "Z" matrix from `lmm.sleep.slope`:
```{r, echo=TRUE}
## Use argument type = "random" to get random-effects design matrix
z.mat2 <- model.matrix(lmm.sleep.slope, type="random")
z.mat2[1:20,] # The . values in zmat2 correspond to zeros
```

* Note that the two columns for each `Subject` in `z.mat2` 
are of the form $(1, t_{j})$, which is what we want.

---

* Let's look at the **estimated parameters** from `lmm.sleep.slope`
```{r, echo=TRUE}
summary(lmm.sleep.slope)
```

* The estimated fixed-effects coefficients are $\hat{\beta}_{0} = 251.4$,
and $\hat{\beta}_{1} = 10.5$ respectively.

* The estimated standard deviation and correlation of the random effects are
    + Estimated standard deviation of $u_{i0}$ is $24.7$.

    + Estimated standard deviation of $u_{i1}$ is $5.9$.
    
    + Estimated correlation between $u_{i0}$ and $u_{i1}$ is $0.07$.

---

* Rather than always printing out the entire summary, you can directly extract the estimates of the fixed effects with
```{r, echo=TRUE}
coef( summary(lmm.sleep.slope) )
```

* To directly extract the estimates of the variance (or standard deviation) of the
random effects, you can use:
```{r, echo=TRUE}
VarCorr( lmm.sleep.slope )

## using as.numeric( VarCorr( lmm.sleep.slope ) ) will give
## the value of the estimated variance

#as.numeric( VarCorr( lmm.sleep.slope ) )

#sqrt( as.numeric( VarCorr( lmm.sleep.slope ) ) )
```



---

### Fitting Binary GLMMs using the Ohio data

* To use the **ohio** data, we will first load the **geepack** R package:
```{r, echo=TRUE}
library(geepack)
```

* This dataset has 2148 observations from 537 individuals
```{r, echo=TRUE}
data(ohio)
head(ohio, 12) # look at first 12 rows of ohio
```

* The outcome of interest in **ohio** is "wheezing status": 1 - yes, 0 - no.
    + The **resp** variable contains wheezing status.

* The **id** variable contains the unique identifier for each individual.

* The **age** in the **ohio** dataset is the time variable.
    + The age variable is recorded as: (**age in years** - 9).
    
    + Each individual starts the study at 7 years of age.
    
* The **smoke** variable is an indicator of maternal smoking at the starting year of the study.

---

* In **lme4**, fitting a GLMM with binary responses can be done with the **glmer** function.

* The **glmer** function has the following syntax:
```{r, echo=TRUE, eval=FALSE}
glmer(formula, data, family)
```

* The `formula` argument uses the same syntax as `lmer`

* When handling binary outcomes, you need to specify the family argument as: `family = binomial`.

---

* Put table of proportions here.

#### A Random Intercept Model

* Let's use a GLMM to explore the relationship between wheezing status and the:
    + **age** of the child
    + maternal **smoking status** 
    
* A GLMM for wheezing status which has age and smoking status as fixed effects and 
random individual-specific intercepts can be expressed as

\begin{equation}
\textrm{logit}\{ p_{ij}(u_{i}) \}  = \beta_{0} + \beta_{age}\textrm{age}_{ij}
+ \beta_{smk}\textrm{smoke}_{i} + u_{i} 
(\#eq:ohio-intercept)
\end{equation}

* Model \@ref(eq:ohio-intercept) can be fit with the following code
```{r, echo=TRUE}
# id is the grouping variable
ohio.intercept <- glmer(resp ~ age + smoke + (1 | id), data = ohio, family = binomial)
```


```{r, echo=TRUE}
coef(summary(ohio.intercept))
```

```{r, echo=TRUE}
VarCorr(ohio.intercept)
```

---

* Plotting

```{r, echo=TRUE}
beta.hat <- coef(summary(ohio.intercept))[,1]
age.vec <- seq(-2, 1, length.out = 1000)
plot(0,0, type="n", xlim=c(-2,1), ylim=c(0,1), las=1,
     xlab = "Age in Years - 9 Years", ylab = "Wheezing Probability")
for(k in 1:50) {
  u.draw1 <- rnorm(1, mean=0, sd=2.34)
  u.draw2 <- rnorm(1, mean=0, sd=2.34)
  lines(age.vec, plogis(u.draw1 + beta.hat[1] + beta.hat[2]*age.vec + beta.hat[3]))
  lines(age.vec, plogis(u.draw2 + beta.hat[1] + beta.hat[2]*age.vec), col="red")
}
```


---